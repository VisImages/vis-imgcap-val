{"3101": [{"image_id": 0, "file_name": "3101_00.png", "page": 1, "dpi": 300, "bbox": [398, 669, 738, 936], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Contemporary interaction design tools increasingly enable smooth transitions and collaboration between design and development (top-left) and handoffs between designers and developers (bottom-left). In our experiences across projects, these transitions remain challenging for visualization designers (top-right) and teams (bottom-right). ", "caption_bbox": [392, 946, 744, 1015]}, {"image_id": 1, "file_name": "3101_01.png", "page": 2, "dpi": 300, "bbox": [39, 29, 758, 266], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: The \ufb01ve visualization design and development projects in which we ground our observations.", "caption_bbox": [145, 277, 648, 291]}, {"image_id": 2, "file_name": "3101_02.png", "page": 3, "dpi": 300, "bbox": [28, 29, 745, 280], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Stages of a data visualization development process and the dependencies between them. We focus on the data characterization, design, and development phases, highlighting the artifacts that bridge these phases and the challenges (C1-C6) that emerge within them. ", "caption_bbox": [28, 296, 744, 324]}, {"image_id": 3, "file_name": "3101_03.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 322], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: The initial design of the Energy Imports & Exports visualiza- tion (left) responded poorly to particular combinations of \ufb01lters (center) and ultimately required a revision (right). ", "caption_bbox": [392, 334, 744, 376]}, {"image_id": 4, "file_name": "3101_04.png", "page": 5, "dpi": 300, "bbox": [73, 1014, 707, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                     during implementation. In all cases, if the edge cases had been iden- Fig. 5: Changes to the dataset for this Energy Futures visualization ti\ufb01ed earlier in the design process, the entire design might have been altered category meanings, creating potentially misleading values.   conceived differently. ", "caption_bbox": [28, 978, 745, 1015]}, {"image_id": 5, "file_name": "3101_05.png", "page": 6, "dpi": 300, "bbox": [392, 29, 761, 438], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Two views of a parallel sets-style visualization of incidents on pipelines and pipeline facilities. Top: A single category is selected. Highlighted gray lines represent how the incidents in the selected category relate to incidents in the categories of the column to the right. Bottom: The design allows additional columns to be dragged into the visualization from the right. This single interaction introduces substantial complexity to the resulting view. Prototyping, testing, and communicating this resulting view in a data-accurate way is dif\ufb01cult using conventional graphic and interaction design tools. ", "caption_bbox": [404, 466, 756, 590]}, {"image_id": 6, "file_name": "3101_06.png", "page": 6, "dpi": 300, "bbox": [39, 29, 394, 252], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: A portion of a visualization showing a heat map of Canadian energy exports to United States Petroleum Administration Defense Dis- tricts (PADDs). Due to confusion surrounding the limitations of an ac- cessibility template, color mappings and the layouts of non-rectangular components in this visualization needed to be adapted to accommodate rectangular selection indicators. ", "caption_bbox": [40, 263, 392, 346]}, {"image_id": 7, "file_name": "3101_07.png", "page": 7, "dpi": 300, "bbox": [398, 859, 742, 964], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: The mapping between number of projects and the width of the bars in this horizontal list was unclear both in the design document and in follow-up conversations. ", "caption_bbox": [392, 974, 742, 1015]}], "3102": [], "3103": [{"image_id": 0, "file_name": "3103_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 308], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Traditional information visualization starts with the exploration of a data set to \ufb01nd inherent patterns. The represented phenomenon is interpreted based on what is found in the data, but the connection between data and phenomenon remains hidden. Autographic visualization starts with the phenomenon and explores the data generation process through material traces. A gap remains between the interpretation of traces and more complex forms of computational analysis. A combined model uses autographic principles of revealing traces to re-contextualize data with the phenomena they supposedly describe. ", "caption_bbox": [60, 329, 711, 396]}, {"image_id": 1, "file_name": "3103_01.png", "page": 2, "dpi": 300, "bbox": [38, 699, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. GISP2 ice core section showing annual layer structure (cropped), charts and graphs should be viewed as abstractions of natural phenom- illuminated from below. Source: ftp://ftp.ncdc.noaa.gov/pub/data/        ena based on shared organizational principles or as metaphorical refer- paleo/slidesets/icecore-polar/                                           ences will not be elaborated here. However, it is worth noting that both ", "caption_bbox": [40, 658, 757, 699]}, {"image_id": 2, "file_name": "3103_02.png", "page": 4, "dpi": 300, "bbox": [39, 29, 756, 390], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Autographic visualizations and their design operations (Table 2), top-left to bottom-right: (a) Cyanometer, a device for measuring the blueness of the sky, framing and encoding [14]; (b) Mercury-in-glass thermometer, constraining and encoding; (c) \ufb01lter for sampling airborne particulate matter, aggregating; (d) southern blot for DNA electrophoresis, separating, registering; (e) EJ Marey\u2019s smoke machine to visualize air\ufb02ow, coupling; (f) Campbell-Stokes sunshine recorder, registering, encoding; (g) Chladni \ufb01gure revealing sound waves, coupling, registering; (h) a planning diagram for neuro-surgery, annotating [81]; (i) pedocomparator for sampling and comparing soil samples, aggregating, encoding [37]; (j) reagent strips for ozone detection, registering, encoding. ", "caption_bbox": [39, 403, 755, 483]}, {"image_id": 3, "file_name": "3103_03.png", "page": 6, "dpi": 300, "bbox": [404, 594, 758, 959], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Autographic environment: gel with graduated antibiotic presence to observe microbial evolution towards antibiotic tolerance [4] ", "caption_bbox": [404, 972, 754, 999]}, {"image_id": 4, "file_name": "3103_04.png", "page": 7, "dpi": 300, "bbox": [411, 608, 733, 758], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Map of photo strips tarnished by H2S emanation on the \ufb01eld site, Public Lab. Registering, annotating [82] ", "caption_bbox": [392, 795, 743, 822]}, {"image_id": 5, "file_name": "3103_05.png", "page": 8, "dpi": 300, "bbox": [393, 29, 758, 977], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. D. Offenhuber, Staubmarke \u2013 reverse graf\ufb01ti washed into concrete, calling attention to air pollution, aggregating, encoding [50] ", "caption_bbox": [404, 990, 755, 1017]}, {"image_id": 6, "file_name": "3103_06.png", "page": 8, "dpi": 300, "bbox": [39, 29, 393, 975], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Amateur visual forensics, Institute for United Con\ufb02ict Analysts \u2013 framing, annotating (IUCA) [28] ", "caption_bbox": [40, 990, 391, 1017]}], "3104": [], "3105": [], "3107": [{"image_id": 0, "file_name": "3107_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 296], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. A de\ufb01nition of GenerativeMap. We design a general pipeline for visualization and exploration of the dynamic density map. The whole pipeline is composed of three modules in the de\ufb01nition, and the complete process is described in Section 3. ", "caption_bbox": [60, 298, 709, 325]}, {"image_id": 1, "file_name": "3107_01.png", "page": 2, "dpi": 300, "bbox": [60, 52, 757, 270], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                           In recent years, a large number of spatiotemporal data are produced and Fig. 2. Explanation of problems scenarios that may arise. The three                                                                            collected, particularly in \ufb01elds such as meteorology, oceanography, and parts are typical applications of density map evolution, they are the main motivation of our work. The details are introduced in Section 1.                                                                            transportation. As classical tasks, the extraction, dynamics and fusion ", "caption_bbox": [40, 269, 754, 316]}, {"image_id": 2, "file_name": "3107_02.png", "page": 3, "dpi": 300, "bbox": [28, 29, 745, 212], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The structure of our improved BiGAN model.(a) The framework of network; (b) The interpolation of images; (c) The structure of encoder; (d) The structure of decoder. ", "caption_bbox": [27, 225, 743, 252]}, {"image_id": 3, "file_name": "3107_03.png", "page": 4, "dpi": 300, "bbox": [420, 52, 738, 279], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The generated series shows the multiple types of results with our model. According to the change process of start-end frames, we simply classify the generative form as three types. ", "caption_bbox": [404, 282, 754, 322]}, {"image_id": 4, "file_name": "3107_04.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 134], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. The experiment shows the visual effect of blue noise sampling. (a) Colorful density map; (b) Gray density map; (c) Blue noise sampling result; (d) Field variation representation. ", "caption_bbox": [392, 135, 744, 175]}, {"image_id": 5, "file_name": "3107_05.png", "page": 6, "dpi": 300, "bbox": [403, 52, 757, 613], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Morphing visualization of arti\ufb01cial data; the start frame and the end frame are created by hand-drawn methods. (a) The series are generated by the diffusion model and (b) show the smooth change by our model. ", "caption_bbox": [404, 616, 754, 669]}, {"image_id": 6, "file_name": "3107_06.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 333], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. The density map shows the air quality in the nation, and we focus on North China, which is in\ufb02uenced by haze. The 1st row is the real data around Beijing, the 2nd row shows the interpolation change between inputs, and the 3rd row presents change trend with arrows using LBN. ", "caption_bbox": [392, 334, 742, 401]}, {"image_id": 7, "file_name": "3107_07.png", "page": 8, "dpi": 300, "bbox": [465, 311, 692, 435], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. A evaluation of interpolation ability. The visual effect is different when INT is different. ", "caption_bbox": [404, 273, 754, 300]}, {"image_id": 8, "file_name": "3107_08.png", "page": 8, "dpi": 300, "bbox": [391, 505, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. The evaluation of INT-TDS. The TDS increases as the INT increases, the slope of curve is slight, and the time cost is available for generating the number of interpolations. ", "caption_bbox": [404, 437, 754, 477]}, {"image_id": 9, "file_name": "3107_09.png", "page": 9, "dpi": 300, "bbox": [43, 251, 361, 583], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13. The evaluation of IGS-ICS-INT. The IGS of INT=8 is the highest in all groups, and the growth of ICS slowdown within more INT, compre- hensive conclude that INT=8 may be the best choice for most scenarios. ", "caption_bbox": [28, 197, 380, 237]}, {"image_id": 10, "file_name": "3107_10.png", "page": 9, "dpi": 300, "bbox": [391, 29, 745, 198], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15. The evaluation of the ILS-Frame. The generative ILS is higher than the diffusion ILS when the evaluated frames are close to the end frame. ", "caption_bbox": [392, 200, 742, 240]}], "3108": [{"image_id": 0, "file_name": "3108_00.png", "page": 1, "dpi": 300, "bbox": [70, 65, 728, 256], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Reproductions of average line and bar positions are systematically biased, such that people tend to underestimate average line positions (toward the bottom of the display) and overestimate average bar positions (toward the top of the display). This bias appears in displays containing one data series (e.g., two outermost images on each side: a visualization containing only one line or only one set of bars). When a display consists of two data series (e.g., middle image: a visualization containing two lines, two bars, or a combination of the two), an additional effect of \u201cperceptual pull\u201d occurs: perception of average line and bar positions are pulled toward each other. This pull is shown to diminish or exaggerate the effect of underestimation and/or overestimation depending on the spatial arrangement of the data series representations. ", "caption_bbox": [73, 271, 722, 364]}, {"image_id": 1, "file_name": "3108_01.png", "page": 2, "dpi": 300, "bbox": [28, 29, 745, 281], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Experimental procedure and design for Experiment 1, 2, and 3. Display times shown in grey.", "caption_bbox": [28, 297, 515, 311]}, {"image_id": 2, "file_name": "3108_02.png", "page": 3, "dpi": 300, "bbox": [43, 52, 753, 229], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Design space for Experiment 1, 2, and 3. Experiment 1 investigates position perception bias in simple line and bar graphs for a single line or a single set of bars, respectively, which could be noisy or uniform. Experiment 2 examines this bias in complex graphs with either two lines or two sets of bars present in the same display, to be compared against displays with a simple single line or a single set of bars. Experiment 3 examines combined data series with one line and one set of bars present in the same display, compared to displays with a simple single line or a single set of bars. Each possible display type was generated with a data series centered around the three means shown in Figure 4, for lines and for bars, respectively. ", "caption_bbox": [40, 244, 755, 324]}, {"image_id": 3, "file_name": "3108_03.png", "page": 3, "dpi": 300, "bbox": [40, 774, 757, 965], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Mean values generated for a line or a set of bars based on the     In Experiment 1, participants were cued before each trial with the pilot experiment. All stimulus displays were then created around these  task of either estimating the average position of a line or a set of bars to three set mean values (high, medium and low) for lines and bars located be shown on a subsequent stimulus display (see Figure 2). Depending in the top half or bottom half of the display.                          on that precue, participants then saw a stimulus display that contained ", "caption_bbox": [40, 964, 755, 1017]}, {"image_id": 4, "file_name": "3108_04.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 202], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Difference between the initial probe location in average line and bar estimation tasks. ", "caption_bbox": [392, 242, 742, 269]}, {"image_id": 5, "file_name": "3108_05.png", "page": 5, "dpi": 300, "bbox": [41, 53, 756, 588], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Results from Experiment 1 for noisy and uniform simple graphs. First and fourth columns: Example displays representing the \u2019medium\u2019 mean position for noisy and uniform displays, respectively. Second and third columns: Density curves for true average position distributions (solid black lines for each of the three means: high, medium, low; pixel values given), and density curves for the estimated average position distributions for each of the three means (line estimates: solid red lines; bar estimates: solid blue lines). The orientations of the solid red or blue lines (shown between the density curves for the estimated average position distributions) show the differences in position estimates between noisy and uniform conditions. ", "caption_bbox": [40, 605, 755, 685]}, {"image_id": 6, "file_name": "3108_06.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 612], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Results from Experiment 2 for compound stimulus displays (complex graphs; i.e., two lines or two sets of bars) and single stimulus displays (simple graphs; i.e., one line or one set of bars). There is still consistent underestimation of lines and overestimation of bars, but these effects are exaggerated or diminished by perceptual pull, depending on the location of the graphed data series. The orientations of the solid red or blue lines (shown between the density curves for the estimated average position distributions) show the differences in estimates between compound and single stimulus displays. ", "caption_bbox": [28, 626, 743, 693]}, {"image_id": 7, "file_name": "3108_07.png", "page": 7, "dpi": 300, "bbox": [46, 52, 748, 586], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Results from Experiment 3 for compound stimulus displays (i.e., one line and one set of bars) and single stimulus displays (i.e., one line or one set of bars). Perceptual pull does not depend on the characteristics of the other graphed data series. ", "caption_bbox": [40, 600, 755, 627]}, {"image_id": 8, "file_name": "3108_08.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 169], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Estimation error for target data series is not dependent on the irrelevant graphed data series. Left three columns: Comparing average estimated line positions when the non-target data series is a line (red) or a set of bars (blue). Right three columns: Comparing estimated bar positions when the non-target data series is a line (red) or a set of bars (blue). ", "caption_bbox": [392, 183, 742, 263]}], "3109": [{"image_id": 0, "file_name": "3109_00.png", "page": 1, "dpi": 300, "bbox": [83, 64, 714, 354], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Summary of tasks: participants are far more efficient in searching for a target relation (left), discriminating proportions of relations (center), and estimating the average relational magnitude (right) when values are encoded as deltas (bottom row) than when encoded as individual values (top row). ", "caption_bbox": [71, 367, 699, 407]}, {"image_id": 1, "file_name": "3109_01.png", "page": 2, "dpi": 300, "bbox": [27, 29, 746, 841], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. (A-B) Two ways of encoding the same data set. How many          perceiving the relation between two items is a serial \u2013 or close to serial students performed better on the second (red values) test? Is it easier \u2013 process. Similarly, another study found that search for T\u2019s among to judge improvements when the data is encoded by bars depicting        L\u2019s (different spatial relationships of the same two line segments) individual data points (2A) or differences (deltas) between data pairs  yielded steep search rates [14]. In fact, across a number of visual (2B). (C) Encoding Types. Six encodings were used as the stimuli in     search studies, search rates are far higher for spatial configurations of Experiments 1, 2, and 3, though absolute values varied. Encodings on    items than for simple items [23]. This serial processing may be due to the left half graphically represents two data values (individual value  the need to isolate each item within a relation individually with the encodings), while encodings on the right half represent the delta       attentional 'spotlight', in order to extract its location, independent of between those two values (delta value encodings). Each column shows     the other item [6]. Based on these results, one can imagine how poorly values encoded by one of three visual feature encodings (position,      this process would scale to large data sets with complex patterns. length, or slope). Encodings on the bottom row (increasing relations)      Importantly, to our knowledge, all of these studies ask viewers to depict the opposite relation as that on the top row (decreasing         find relations between qualitative visual identities (object X and object relations).                                                             Y), and no existing work has tested the efficiency of search for ", "caption_bbox": [27, 840, 743, 1013]}, {"image_id": 2, "file_name": "3109_02.png", "page": 4, "dpi": 300, "bbox": [21, 29, 750, 302], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Experiments 1, 2, and 3\u2019s designs. Stimuli shown here are not drawn to scale. Participants viewed a preview screen containing a target relation(s) to search for (Experiment 1) or judge (Experiments 2 and 3), followed by a blank screen, and then a test display. The test display remained until participants indicated which rectangle contained the target relation in Experiment 1, followed by an incorrect screen if they responded incorrectly. In Experiments 2 and 3, the test display was presented briefly, followed by a mask screen to prevent an after-image. In the final response screen, participants indicated which relation was more prevalent (Experiment 2) or moved the mouse to show the perceived average delta (Experiment 3). All trials concluded with a blank screen. Relations were located randomly, but always aligned to a bottom baseline. ", "caption_bbox": [27, 301, 742, 381]}, {"image_id": 3, "file_name": "3109_03.png", "page": 5, "dpi": 300, "bbox": [37, 812, 388, 927], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Summary of Expriments 1-3 Results. Delta value encodings lead to faster search rates (left) when searching for an opposing relation, higher accuracies (center) for distinguishing which relation there is more of, and lower error (right) when perceiving the average delta value in the ensemble task. Error bars indicate within-subject standard error of the mean. ", "caption_bbox": [41, 934, 378, 1014]}], "3110": [{"image_id": 0, "file_name": "3110_00.png", "page": 2, "dpi": 300, "bbox": [392, 29, 757, 182], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: The mental transformation required to correctly order the Y- axis of a \ufb01gure originally published in Padilla et al. [61]. ", "caption_bbox": [404, 197, 754, 225]}, {"image_id": 1, "file_name": "3110_01.png", "page": 6, "dpi": 300, "bbox": [40, 29, 758, 283], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Diagram depicting the experimental design sequence for the single-task (left) and dual-task (right) groups for                                                   an easy trial. ", "caption_bbox": [106, 294, 689, 322]}, {"image_id": 2, "file_name": "3110_02.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 350], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Each plot shows the effects of the experimental group and task dif\ufb01culty on accuracy (A), task completion time (B), and pupil dilation (C). Error bars show +/- 95% con\ufb01dence intervals. Error bars for pupilometry were created by taking the average pupil dilation per trial. ", "caption_bbox": [28, 354, 743, 382]}], "3111": [{"image_id": 0, "file_name": "3111_00.png", "page": 1, "dpi": 300, "bbox": [79, 65, 723, 460], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: VisTA: a visual analytics tool that allows UX practitioners to analyze recorded think-aloud sessions with the help of machine intelligence to detect usability problems. ", "caption_bbox": [73, 464, 722, 492]}, {"image_id": 1, "file_name": "3111_01.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 152], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: The feature timeline shows ML\u2019s main input features in a short time window around the current time and updates as the video plays. ", "caption_bbox": [392, 153, 742, 181]}, {"image_id": 2, "file_name": "3111_02.png", "page": 7, "dpi": 300, "bbox": [40, 54, 757, 250], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Typical playback behaviours (where x-axis indicates the study session time and y-axis the reviewed video time): (a) One-pass: No-Pause- Write; (b) One-pass: Pause-Write; (c) One-pass: Micro-Playback-Write; (d) Two-pass: Overview-then-Write; (e) Two-pass: Write-then-Check. ", "caption_bbox": [40, 249, 757, 277]}], "3112": [{"image_id": 0, "file_name": "3112_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 469], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Cropped screen captures from our experimental mobile application. Left: an instance of the animation condition (open this PDF in Acrobat Reader to view the animation). Right: an instance of the small multiples condition, in which the position of each item re\ufb02ects its values in the year 2000; a \u201ctrail\u201d encodes the item\u2019s previous positions from 1975 to 1999. ", "caption_bbox": [61, 482, 710, 522]}, {"image_id": 1, "file_name": "3112_01.png", "page": 2, "dpi": 300, "bbox": [38, 29, 758, 967], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Trend visualization designs considered in Robertson et al.\u2019s 2008 study [46]. The left and middle frames are examples from their animation condition, while the right frame is an example from their small multiples condition. We incorporate a subset of their data and tasks in our experiment, albeit with a smaller dataset of 16 nations (cf. Figure 1); we revisit the question of visualizing larger datasets on mobile displays in Section 5.2. ", "caption_bbox": [40, 977, 756, 1017]}, {"image_id": 2, "file_name": "3112_02.png", "page": 3, "dpi": 300, "bbox": [27, 521, 745, 973], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The \ufb01ve steps of a task (Task 5) in the Animation condition (top) and in the Multiples condition (bottom), comprised of a instruction reading phase (Steps 1\u20132) and a task completion phase (Steps 3\u20135). The alphabetical response interface was identical across the two conditions. ", "caption_bbox": [28, 990, 743, 1017]}, {"image_id": 3, "file_name": "3112_03.png", "page": 4, "dpi": 300, "bbox": [391, 29, 758, 580], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Diagrams of the nine tasks featured in our experiment. The start- ing and ending positions of target and distractor items are indicated by \ufb01lled and un\ufb01lled circles, respectively. Note that the number of distractor items is reduced for clarity of illustration, as the total number of (target + distractor) items was 16 in every task. Similarly, the items are shown with uniform size for clarity of illustration, whereas the size of items was mapped to the corresponding nation\u2019s population in each task. These diagrams are preceded by the instruction shown to participants. ", "caption_bbox": [403, 598, 756, 704]}, {"image_id": 4, "file_name": "3112_04.png", "page": 6, "dpi": 300, "bbox": [40, 29, 757, 967], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The 9 tasks, mean completion times, proportions of correct responses, and estimates of effect sizes. \u2013 \u2013 = Animation; \u2013 \u2013 = Multiples; \u2013 \u2013 = Effect size estimate. Thick error bars are 95% CIs [38], while thin error bars are conservative 99.4% CIs that account for multiple comparisons over the 9 tasks. Dotted blue lines in column 3 indicate 1x and 2x the length of a complete animation. In cases where a \u2013 \u2013 CI overlaps a dashed line (a Completion Time Ratio of 1 or a Proportional Correctness Difference of 0%), we interpret this as inconclusive evidence for an effect [9]. ", "caption_bbox": [40, 972, 756, 1025]}, {"image_id": 5, "file_name": "3112_05.png", "page": 8, "dpi": 300, "bbox": [39, 29, 758, 238], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                        data and emphasize that it changes over time. Following the completion Fig. 9. Task 1 asks participants to identify items where \u0394x > \u0394y. Left:                                                                         of the animation, a transition to a static small multiples scatterplot could the Animation condition (correct responses: H and O; open this PDF in Acrobat Reader to view the animation). Right: Multiples condition                                                                         then allow the viewer to make accurate judgments about trajectory (correct responses: C and K). Note that item labels and colors are      length or angle. Boyandin et al. [15] made a similar recommendation randomly assigned and are not consistent between these two instances.   following their qualitative study of animation and small multiples in ", "caption_bbox": [39, 237, 754, 310]}], "3113": [{"image_id": 0, "file_name": "3113_00.png", "page": 1, "dpi": 300, "bbox": [74, 65, 723, 347], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The three visualizations compared in our study. (a) Dorling cartograms as small multiples, (b) proportional symbols (circles) on maps as small multiples, and (c) proportional symbols (bar charts) on a single map. In this example, each map shows the values of two arti\ufb01cially-created variables over four years. In each case, both variables have an overall positive correlation (Pearson correlation coef\ufb01cient \u2265 0.75) and no monotonic evolution. ", "caption_bbox": [73, 351, 722, 406]}, {"image_id": 1, "file_name": "3113_01.png", "page": 5, "dpi": 300, "bbox": [40, 52, 357, 153], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Schematic illustration of all possible answers for tasks in Fig. 2. The three trial repetitions included combinations, such that each cor- relation type (positive, negative, no-correlation) appeared once. When temporal evolution was applicable, one of the positive/negative correla- tions was coupled with monotonic evolution, while the other was not. ", "caption_bbox": [40, 160, 392, 227]}, {"image_id": 2, "file_name": "3113_02.png", "page": 5, "dpi": 300, "bbox": [405, 52, 756, 223], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Web interface used to conduct the experiment. Visualization = GlyphSM; task performed on a T IME I NTERVAL, for A LL L OCATIONS. ", "caption_bbox": [405, 236, 755, 263]}, {"image_id": 3, "file_name": "3113_03.png", "page": 6, "dpi": 300, "bbox": [27, 725, 96, 849], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": " Fig. 7 shows the self-reported con\ufb01dence for each visual- DorlingSM) takes less time (less than 20sec) and causes less errors", "caption_bbox": [95, 780, 742, 798]}, {"image_id": 4, "file_name": "3113_04.png", "page": 6, "dpi": 300, "bbox": [27, 536, 98, 726], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6 shows error rates for all tasks collectively, with ings. For both tasks that considered S INGLE T IME, con\ufb01dence is high", "caption_bbox": [97, 681, 742, 696]}, {"image_id": 5, "file_name": "3113_05.png", "page": 6, "dpi": 300, "bbox": [25, 29, 394, 431], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5 shows completion times of all tasks col-", "caption_bbox": [132, 436, 379, 450]}, {"image_id": 6, "file_name": "3113_06.png", "page": 7, "dpi": 300, "bbox": [27, 48, 745, 419], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Results for Completion Time (sec) and Error Rate (in %) for each task in Fig. 2. In each cell (task), Mean values per visualization are seen on the left and means of pairwise differences on the right. Error bars represent 95% Bootstrap con\ufb01dence intervals. Gray rectangles indicate the direction of our hypotheses. Evidence of differences are marked with a (the further away from 0 and the tighter the CI, the stronger the evidence). ", "caption_bbox": [28, 425, 745, 465]}, {"image_id": 7, "file_name": "3113_07.png", "page": 8, "dpi": 300, "bbox": [28, 29, 394, 339], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Reported self-con\ufb01dence per task (in %).", "caption_bbox": [28, 339, 266, 353]}, {"image_id": 8, "file_name": "3113_08.png", "page": 9, "dpi": 300, "bbox": [90, 52, 758, 169], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Summary of our recommendations for the different tasks. For       analysis of data using a map of a known country could have led to bias tasks on subranges of space and time (middle cell) there is no clear       given preconceptions about the geographical distribution, we believe winner, but the table structure suggests small multiples work best for in- this to be unlikely given the extensive training, and the number of map creased spatial complexity, and a single map with bar charts for increased features and time steps involved.5 In summary, while we believe that temporal complexity (although the transition point is not known).          overall trends would persist across different maps, future work needs ", "caption_bbox": [40, 168, 755, 239]}], "3114": [{"image_id": 0, "file_name": "3114_00.png", "page": 5, "dpi": 300, "bbox": [28, 29, 393, 298], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 2: Inter-group2 Distance (\u03b8V ), Intra-group Similarity (\u03c4V ), and Grouping Parameters for the Study. (\u22a5 denotes perpendicular trajec- tories,  : value increases,  : value decreases) ", "caption_bbox": [392, 52, 742, 97]}, {"image_id": 1, "file_name": "3114_01.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 240], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Summary of the mean grouping strength for all visual variables examined in our main experiment. Error bars are 95% CIs. The left column shows the mean grouping strength per variable, for all comparisons it was involved in. The fact that the CIs of DP and DS fall to the right of the plots shows that they have stronger grouping strength than any other tested variables. Each cell in the rest of the table shows the mean grouping strength of the corresponding visual variable in the row (orange) vs. that of the visual variable in the column (blue), for that task only. ", "caption_bbox": [28, 253, 743, 308]}, {"image_id": 2, "file_name": "3114_02.png", "page": 8, "dpi": 300, "bbox": [51, 57, 748, 224], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Grouping strength ranking results (numerical answers to Q35 ) for the three dynamic variables (DP, DS, and DL) in the context of a Gapminder-like animated scatterplot visualization (left) and an animated thematic map of U.S. Presidential election data (right). ", "caption_bbox": [40, 223, 755, 253]}, {"image_id": 3, "file_name": "3114_03.png", "page": 9, "dpi": 300, "bbox": [391, 29, 745, 195], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Combinations of dynamic behavior and initial/\ufb01nal conditions that might emerge in realistic visualization scenarios. ", "caption_bbox": [392, 208, 742, 236]}], "3115": [{"image_id": 0, "file_name": "3115_00.png", "page": 4, "dpi": 300, "bbox": [39, 52, 392, 154], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: A relationship between variant dimensions and the incremental PCA results. (a) shows the data shape. While n stored data points have d = D (the gray area), m new data points have l dimensions (the red area). (b) shows the relationships between the data and its lower-dimensional representation generated by the incremental PCA. Gray and red points represent the stored and new data points, respectively. The PCA can be applied to only the grey area or the area within the orange border in (a). Thus, we need to apply a prediction or projection to obtain the lower-dimensional representation for all (n + m) data points where d = D. ", "caption_bbox": [39, 153, 391, 273]}, {"image_id": 1, "file_name": "3115_01.png", "page": 4, "dpi": 300, "bbox": [405, 52, 757, 296], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Visualizations with the position estimation method. The bus transportation dataset from [59] is used for this visualization. Each point represents one bus and each color represents a bus group de\ufb01ned by \u201cBlock ID\u201d in [59]. The time duration between each bus stop is used as a value for each dimension. Each bus passes through \ufb01ve stops and has four time durations (D = 4). The position transitions of two new buses are shown from (a) to (f). In (a), the PCA result of d = D are shown without plotting the new buses. From (b) to (e), the two buses are plotted with the position estimation method where d = 1 to d = D. Then, the PCA result is updated in (f). The outer-ring color represents the uncertainty as described in Sect. 3.4. From (c) to (e), paths of the two buses are also visualized with the corresponding uncertainty colors. We can see that a bus with higher uncertainty in the top-left moves farther away from the \ufb01nal result in (f). ", "caption_bbox": [403, 295, 754, 480]}, {"image_id": 2, "file_name": "3115_02.png", "page": 6, "dpi": 300, "bbox": [391, 810, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: A prototype system consisting of three views: (a) the DR view, (b) the parallel-coordinates view, and (c) the scatterplot-matrix view. ", "caption_bbox": [404, 769, 754, 796]}, {"image_id": 3, "file_name": "3115_03.png", "page": 6, "dpi": 300, "bbox": [430, 52, 730, 325], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1: Completion time (in milliseconds) of each process in Fig. 6. Graphical results are also available in [1]. ", "caption_bbox": [404, 397, 756, 424]}, {"image_id": 4, "file_name": "3115_04.png", "page": 6, "dpi": 300, "bbox": [39, 52, 392, 168], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: A \ufb02owchart of the streaming data visualization using incremental PCA. The red, green, blue, and yellow are process blocks that correspond to the methods for dealing with the computational cost, the viewer\u2019s mental map, and the non-uniform number of dimensions, the uncertainty visualization, respectively. ", "caption_bbox": [404, 324, 754, 391]}, {"image_id": 5, "file_name": "3115_05.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 233], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: An example of visual detection of an anomaly node. In (a), one node moves far away from other nodes and goes out of view from the current visualized range. This suspicious behavior indicates that this node could be an anomaly. Thus, we follow the node with our automatic detection in (b). To see more details about this node, we visualize it in the PCP view. We also show the scatterplots of the current work station (\u201cWS 03\u201d) and three immediate work stations (\u201cWS 04\u201d, \u201cWS 05\u201d, and \u201cWS 06\u201d). (c) shows the PCP and DR views after obtaining the entire set of values of the node. ", "caption_bbox": [27, 232, 744, 285]}, {"image_id": 6, "file_name": "3115_06.png", "page": 8, "dpi": 300, "bbox": [39, 254, 757, 378], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: An example of visual detection of a new forming cluster which has delays in the MTA bus trips. (a) shows two new nodes moved away from the originally formed cluster. (b) shows a distinct cluster (mainly consisted of orange nodes) formed with more arrived data points. (c) shows the PCP view of the two clusters. We can see that the values of the node (the purple polylines) show a clear distinction to the values of the new nodes (the orange polylines). In (d), we show the PCP view with the original scales of the values. ", "caption_bbox": [39, 377, 754, 430]}, {"image_id": 7, "file_name": "3115_07.png", "page": 8, "dpi": 300, "bbox": [92, 52, 703, 197], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: An example of a visual prediction of an error based on previous errors. In (a), we can see that one node, indicated with the green arrow, comes close to the error nodes colored with orange. From (a), we select this node and the error nodes with the lasso selection and visualized them as the parallel coordinates in (b). We can see that the values of the node (the purple polyline) follow closely to the values of the other nodes (the orange polylines). At two work stations after the state of (a) and (b), the node causes an error, as shown in (c). ", "caption_bbox": [40, 196, 756, 249]}], "3116": [{"image_id": 0, "file_name": "3116_00.png", "page": 1, "dpi": 300, "bbox": [391, 65, 757, 591], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Two children working with the Construct-A-Vis tool.", "caption_bbox": [405, 600, 690, 614]}, {"image_id": 1, "file_name": "3116_01.png", "page": 3, "dpi": 300, "bbox": [404, 52, 757, 178], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Creating (1) and modifying (2) tokens in Construct-A-Vis.", "caption_bbox": [405, 183, 719, 197]}, {"image_id": 2, "file_name": "3116_02.png", "page": 4, "dpi": 300, "bbox": [27, 29, 394, 178], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Linking coloured tokens to data points for feedback.", "caption_bbox": [28, 183, 316, 197]}, {"image_id": 3, "file_name": "3116_03.png", "page": 4, "dpi": 300, "bbox": [27, 205, 381, 847], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Tokens are shared by transferring across color-coded lines.", "caption_bbox": [28, 852, 354, 866]}, {"image_id": 4, "file_name": "3116_04.png", "page": 5, "dpi": 300, "bbox": [421, 53, 738, 212], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Child participants by age.", "caption_bbox": [405, 211, 566, 225]}, {"image_id": 5, "file_name": "3116_05.png", "page": 6, "dpi": 300, "bbox": [26, 362, 391, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. An overview of visualization types created in Task 1.", "caption_bbox": [28, 336, 319, 350]}, {"image_id": 6, "file_name": "3116_06.png", "page": 7, "dpi": 300, "bbox": [40, 557, 392, 1005], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Joined visualizations created by pairs in Task 3.", "caption_bbox": [40, 1004, 309, 1018]}], "3117": [{"image_id": 0, "file_name": "3117_00.png", "page": 5, "dpi": 300, "bbox": [40, 808, 757, 985], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Data binding via dropzones in Lyra (left), via the binding icon in Data Illustrator (middle), and via either approach in Charticulator (right).", "caption_bbox": [40, 1002, 754, 1016]}], "3118": [{"image_id": 0, "file_name": "3118_00.png", "page": 2, "dpi": 300, "bbox": [391, 29, 758, 793], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Users at the Plankton Populations exhibit in the Living Systems Gallery of the Exploratorium. ", "caption_bbox": [404, 792, 754, 820]}, {"image_id": 1, "file_name": "3118_01.png", "page": 3, "dpi": 300, "bbox": [27, 107, 379, 307], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Screenshot of the Plankton Populations visualization.", "caption_bbox": [28, 306, 325, 320]}, {"image_id": 2, "file_name": "3118_02.png", "page": 5, "dpi": 300, "bbox": [392, 402, 744, 556], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Screenshots of low resolution overhead video collected for naturalistic observation. ", "caption_bbox": [392, 555, 742, 583]}, {"image_id": 3, "file_name": "3118_03.png", "page": 6, "dpi": 300, "bbox": [404, 455, 757, 944], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Boxplot comparing the time the think-aloud participants took to arrive at their first (correct) data interpretation and the total time uncued visitors stayed at Plankton Populations in naturalistic observations. ", "caption_bbox": [405, 948, 755, 1002]}, {"image_id": 4, "file_name": "3118_04.png", "page": 6, "dpi": 300, "bbox": [40, 455, 392, 774], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Timeline of decoding comments indicating when an encoding is mentioned for the first time. ", "caption_bbox": [40, 777, 389, 805]}, {"image_id": 5, "file_name": "3118_05.png", "page": 6, "dpi": 300, "bbox": [40, 29, 758, 411], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Timeline of decoding and data interpretation comments in study participants\u2019 think-alouds. More saturated squares indicate multiple statements occurring close to one another. ", "caption_bbox": [40, 415, 755, 443]}], "3119": [{"image_id": 0, "file_name": "3119_00.png", "page": 4, "dpi": 300, "bbox": [39, 52, 757, 273], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. This \ufb01gure shows our study platform and supported interactions for different visualizations: bar chart (left) and scatterplot (right). Users can (1) resize, (2) reposition, and (3) recolor bars and points directly. ", "caption_bbox": [39, 286, 754, 313]}, {"image_id": 1, "file_name": "3119_01.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 387], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Each row is one of the 15 operations participants performed during the study, and each column is one of the 48 strategies we identi\ufb01ed. Each cell shows the number of times participants used the strategy in column to perform the operation in row. The higher the value in a cell, the darker the background of the cell. Strategies are grouped based on the main encoding(s) involved in employing that strategy (second row in the table). For each strategy we color code the high-level approaches: exempli\ufb01cation, declaration, instrumentation and selection (fourth row in the table), detailed in the Discussion section. We provide detailed description of each strategy in Figure 3. ", "caption_bbox": [28, 405, 743, 472]}, {"image_id": 2, "file_name": "3119_02.png", "page": 5, "dpi": 300, "bbox": [27, 490, 743, 767], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Description of each of the 48 strategies participants used in our study.", "caption_bbox": [28, 783, 406, 797]}, {"image_id": 3, "file_name": "3119_03.png", "page": 6, "dpi": 300, "bbox": [406, 725, 755, 810], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Four strategies to adjust the value of a point in a scatterplot and of a bar in a bar chart. ", "caption_bbox": [404, 828, 754, 855]}, {"image_id": 4, "file_name": "3119_04.png", "page": 6, "dpi": 300, "bbox": [406, 487, 747, 654], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Strategies to navigate a data point over time.", "caption_bbox": [404, 673, 659, 687]}, {"image_id": 5, "file_name": "3119_05.png", "page": 6, "dpi": 300, "bbox": [406, 56, 755, 231], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Prominent strategies to assign a data attribute to an axis (\ufb01rst row); and to the size or color of points in the scatterplot, and of bars in the bar chart (second row). ", "caption_bbox": [404, 251, 754, 291]}, {"image_id": 6, "file_name": "3119_06.png", "page": 6, "dpi": 300, "bbox": [407, 329, 727, 419], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Strategies to switch from scatterplot to bar chart.", "caption_bbox": [404, 433, 678, 447]}, {"image_id": 7, "file_name": "3119_07.png", "page": 7, "dpi": 300, "bbox": [392, 599, 742, 695], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Strategies to expand the range of a bin.", "caption_bbox": [392, 711, 628, 725]}, {"image_id": 8, "file_name": "3119_08.png", "page": 7, "dpi": 300, "bbox": [392, 352, 736, 526], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Strategies to change the size or color of all points in a scatterplot, and the color of all bars in a barchart. ", "caption_bbox": [392, 544, 743, 571]}, {"image_id": 9, "file_name": "3119_09.png", "page": 7, "dpi": 300, "bbox": [393, 207, 744, 293], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Three strategies to sort a bar chart.", "caption_bbox": [392, 311, 602, 325]}, {"image_id": 10, "file_name": "3119_10.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 143], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Three strategies to group two bars into one bar.", "caption_bbox": [392, 161, 661, 175]}], "3120": [{"image_id": 0, "file_name": "3120_00.png", "page": 1, "dpi": 300, "bbox": [40, 29, 757, 316], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. We explore how visualization can enhance data use in the \ufb01eld for domains such as earth science and emergency response. Working with \ufb01eld analysts, we elicit recommendations for how mobile and immersive technologies might bridge spatial and temporal gaps in data collection and analysis. We use these recommendations to develop FieldView, an extensible prototype data collection and visualization system that uses mobile overviews and situated AR visualizations to communicate data about on-going operations. ", "caption_bbox": [73, 332, 724, 385]}, {"image_id": 1, "file_name": "3120_01.png", "page": 4, "dpi": 300, "bbox": [391, 29, 747, 238], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. The design probe\u2019s mobile application provides (a) a \ufb01eld notebook-style data collection interface, (b) a corresponding list of local data points collected in the \ufb01eld, and (c & d) interactive mobile overview visualizations based on remote server data. ", "caption_bbox": [392, 250, 742, 303]}, {"image_id": 2, "file_name": "3120_02.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 163], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. We synthesized design recommendations for how data collection and visualization might enhance data use in \ufb01eld operations. These insights fall into four primary categories that can guide designers to scenarios and design considerations for \ufb01eld analysis tools. ", "caption_bbox": [28, 178, 742, 205]}, {"image_id": 3, "file_name": "3120_03.png", "page": 7, "dpi": 300, "bbox": [391, 29, 757, 296], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Use Case 1: Team coordinators can assess coarse-grain cov- erage at a glance on the mobile device (left). They can survey team responsibilities and validate collected data (redder data points indicate greater scorch percentage) using a contextualized scatterplot (top right) and compare new data to archival data (bottom right). ", "caption_bbox": [405, 309, 757, 376]}, {"image_id": 4, "file_name": "3120_04.png", "page": 8, "dpi": 300, "bbox": [27, 29, 745, 190], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                          reference Fig. 9. Use Case 2: Scorch data is visualized in a scatterplot embedded within the \ufb01eld sampling grid with one point per grid to encode scorch rates, mitigating occlusion of the surrounding environment. Blue regions  Fig. 10.  within the grid allow analysts to rapidly identify gaps in data coverage. encoded ", "caption_bbox": [27, 189, 437, 247]}], "3121": [{"image_id": 0, "file_name": "3121_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 397], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Our immersive space-time cube (STC) aims to lower the known steep learning curve of this three-dimensional spatio-temporal representation. The base map is coupled to a virtual reproduction of the analyst\u2019s own real desk and all data manipulations and queries are implemented through intuitive gestures and tangible controls, increasing usability and comfort, while decreasing mental workload. ", "caption_bbox": [61, 410, 712, 450]}, {"image_id": 1, "file_name": "3121_01.png", "page": 4, "dpi": 300, "bbox": [39, 29, 758, 229], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. In our immersive Space-Time Cube environment, all actions are implemented through intuitive mid-air gestures, such as grabbing (left), stretching (center left), and tapping (center right), or by tangible interaction with controls on the desk\u2019s surface (right). Since time advances downwards, the trajectories show the movement history up to the point currently crossing the map. Blue hand contours added for clarity. ", "caption_bbox": [40, 242, 756, 282]}, {"image_id": 2, "file_name": "3121_02.png", "page": 5, "dpi": 300, "bbox": [27, 29, 393, 254], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1. While interaction commands in Immersive are mostly intuitive and body-related, Desktop mimics a typical Rotate-Pan-Dolly paradigm. ", "caption_bbox": [392, 223, 744, 250]}, {"image_id": 3, "file_name": "3121_03.png", "page": 6, "dpi": 300, "bbox": [39, 29, 758, 229], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 2. Evaluated tasks target different movement features, with different levels of dif\ufb01culty and required interaction. Using Amini et al.\u2019s approach [1], the three movement task components are classi\ufb01ed as Known or Unknown, and Singular or Plural. ", "caption_bbox": [39, 430, 755, 457]}, {"image_id": 4, "file_name": "3121_04.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 233], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Times were similar in both conditions, with the exception of the simplest task. Success rates were generally similar across scenarios and conditions, except for T5, which became more dif\ufb01cult for Dense data in Desktop, but not in Immersive. Error bars indicate standard deviation. ", "caption_bbox": [28, 247, 743, 274]}, {"image_id": 5, "file_name": "3121_05.png", "page": 8, "dpi": 300, "bbox": [404, 307, 756, 446], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Likert-scale agreements to different assertions ranging from strongly disagree (dark red) to strongly agree (dark green). Participants found it signi\ufb01cantly easier to \ufb01nd information and interact in Immersive (IM) than in Desktop (D), and also considered it more comfortable. ", "caption_bbox": [404, 459, 754, 512]}, {"image_id": 6, "file_name": "3121_06.png", "page": 8, "dpi": 300, "bbox": [40, 29, 756, 227], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Distribution of time across the different interaction features, for tasks in both conditions and scenarios. Users in Desktop performed many more map rotations, to bene\ufb01t from structure-from-motion, and inspections, to be able to accurately determine times. In Immersive, users intuitively performed much more data translations and scalings, and constantly moved their heads to change their point of view. ", "caption_bbox": [40, 241, 755, 281]}], "3122": [{"image_id": 0, "file_name": "3122_00.png", "page": 1, "dpi": 300, "bbox": [78, 65, 720, 290], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. A cluster identi\ufb01cation task was performed and evaluated in four different visualization design spaces. Two screen-based methods, namely a scatterplot matrix (a) and a 3D scatterplot in a cube (b), and two visualizations in a VR environment: a 3D scatterplot on a virtual table (c) and a room-sized scatterplot (d). Gray lines emphasize transitions between visualization design spaces. ", "caption_bbox": [73, 307, 721, 347]}, {"image_id": 1, "file_name": "3122_01.png", "page": 3, "dpi": 300, "bbox": [40, 52, 757, 251], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Representation of one exemplary dataset in all four investigated visualization design spaces. Except for the scatterplot matrix (a), all visualization design spaces had some kind of navigation available to inspect the visualization from different perspectives. ", "caption_bbox": [40, 264, 755, 291]}, {"image_id": 2, "file_name": "3122_02.png", "page": 3, "dpi": 300, "bbox": [47, 788, 758, 978], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. PCA projection of data displayed in Figure 2. The dataset         the previous design spaces. Removing the restriction (VRRoom) may contains six clusters (highlighted on the right). Two clusters are hardly lead to an even more increased level of immersion as the user enters recognizable in the PCA projection (orange Y and green S).                the visualization itself and is fully enclosed by it. ", "caption_bbox": [40, 977, 755, 1017]}, {"image_id": 3, "file_name": "3122_03.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 237], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Measures of Immersion \u2013 Left: single measure question of subjectively perceived immersion. Right: multiple measure questionnaire on immersion. ", "caption_bbox": [392, 250, 742, 290]}, {"image_id": 4, "file_name": "3122_04.png", "page": 4, "dpi": 300, "bbox": [392, 765, 744, 926], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Interview \u2013 Median user ratings for the design spaces with regard to abstractness (left) and presence (center). Participants were asked to rate the abstractness and presence of each design space on a \ufb01ve-point Likert scale from 1 = not abstract/not present to 5 = very abstract/very present. Right: Combined median of the abstractness and presence scores used for statistical evaluation and as a measure of immersion. ", "caption_bbox": [392, 939, 742, 1019]}, {"image_id": 5, "file_name": "3122_05.png", "page": 7, "dpi": 300, "bbox": [404, 57, 759, 977], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Subjective preference: ranks (1 = easy to 4 = hard) assigned to the four visualization design spaces by the participants. ", "caption_bbox": [405, 990, 755, 1017]}, {"image_id": 6, "file_name": "3122_06.png", "page": 7, "dpi": 300, "bbox": [44, 743, 391, 937], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Average error rate and completion time as a function of visual- ization design space. Bars indicate the 95% CI of the mean, asterisks signi\ufb01cant differences between design spaces (*** p \u2264 .001). Note that for statistical analysis task completion times were log transformed because of skewed distributions, while in this \ufb01gure original data is displayed. ", "caption_bbox": [40, 950, 392, 1017]}], "3123": [{"image_id": 0, "file_name": "3123_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 409], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Macroeconomics analysis in the ImAxes immersive analytics tool [11]. (Photo by Samuel Zeller on Unsplash.)", "caption_bbox": [89, 419, 680, 433]}, {"image_id": 1, "file_name": "3123_01.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 251], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Process, timeline, re\ufb01nements, and observations for our study. DR4 shown in grey, as it was replaced based on user feedback. ", "caption_bbox": [392, 262, 742, 290]}, {"image_id": 2, "file_name": "3123_02.png", "page": 4, "dpi": 300, "bbox": [39, 29, 394, 703], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Tooltip providing details-on-demand for data items.", "caption_bbox": [66, 713, 362, 727]}, {"image_id": 3, "file_name": "3123_03.png", "page": 5, "dpi": 300, "bbox": [27, 29, 394, 236], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 3: Datasets used by summative participants.", "caption_bbox": [441, 736, 692, 750]}, {"image_id": 4, "file_name": "3123_04.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 297], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: Histogram showing the vertical distance of participant inter- actions with axes relative to their eye level. Eye level is at 0, and the approximate chest level is represented by the red line. ", "caption_bbox": [392, 313, 742, 354]}, {"image_id": 5, "file_name": "3123_05.png", "page": 7, "dpi": 300, "bbox": [28, 692, 744, 919], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Heatmaps of axis interaction in S2 (top-down). Participant coordinate plots. However, it is worth noting that during the presenta- position and view direction is represented by a direction arrow.   tion stage, only Participant 3 used a parallel coordinate plot. ", "caption_bbox": [28, 918, 742, 946]}, {"image_id": 6, "file_name": "3123_06.png", "page": 8, "dpi": 300, "bbox": [391, 29, 758, 252], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Subjective ratings from exit survey. Subfactors include: con- troller ease of use [CNTRLR], adjustment to environment [ADJUST], perceived immersion [IMMERS], user engagement [ENGAGE], and avoidance of sensory discord [DISCRD]. ", "caption_bbox": [404, 262, 754, 317]}], "3124": [{"image_id": 0, "file_name": "3124_00.png", "page": 1, "dpi": 300, "bbox": [68, 65, 730, 460], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Scalable Insets applied to a genome interaction matrix [57], gigapixel image [63], and geographic map from Mapbox [43] and OpenStreetMaps [49] (left to right). Various annotated patterns are highlighted in red. ", "caption_bbox": [73, 469, 722, 496]}, {"image_id": 1, "file_name": "3124_01.png", "page": 3, "dpi": 300, "bbox": [40, 871, 758, 978], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Three approaches exemplifying naive optimization of (1) L OCAL -    cities, mountains and other important geographical information needed ITY, (2) C ONTEXT, and (3) D ETAIL only. The red rectangle in (C) indicates for localization. ", "caption_bbox": [40, 977, 755, 1006]}, {"image_id": 2, "file_name": "3124_02.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 532], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Demonstration of the Scalable Insets approach on a gigapixel image of Rio de Janeiro [63] by The Rio\u2014Hong Kong Connection [64] and ski areas around Utah and Colorado shown on a map from Mapbox [43]. The screenshots illustrate how Scalable Insets enables pattern-driven exploration and navigation at scale; details are explained in Sect. 3.1. See Supplementary Figures S1 and S2 for scaled-up screenshots. ", "caption_bbox": [28, 538, 742, 579]}, {"image_id": 3, "file_name": "3124_03.png", "page": 5, "dpi": 300, "bbox": [42, 51, 755, 266], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Schematic design principals of Scalable Insets. (1) Inset design and information encoding. (2) Visual representation of aggregated insets. (3) Leader line styles. (4) The inset placement mechanism and stability considerations of Scalable Insets. (5) Aggregation procedure and stability considerations. (6) Interaction between insets applied in Scalable Insets. ", "caption_bbox": [40, 279, 757, 319]}, {"image_id": 4, "file_name": "3124_04.png", "page": 7, "dpi": 300, "bbox": [404, 281, 758, 944], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Mean completion time in seconds (lower is better) and mean accuracy in percent (higher is better) across tasks and techniques. Error bars indicate the standard error. Note, due to non-normal distribution of completion time we report signi\ufb01cance on the median time using Kruskal-Wallis and Holm-Bonferroni-corrected Mann-Whitney U tests. ", "caption_bbox": [405, 950, 755, 1017]}, {"image_id": 5, "file_name": "3124_05.png", "page": 7, "dpi": 300, "bbox": [56, 52, 738, 205], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Screenshots from the \ufb01rst user study showing Rio de Janeiro [64] with examples of each task and technique. (1) Comparing the frequency of annotated patterns in two distinct regions with BB OX. (2) Finding a speci\ufb01c pattern that shows a Brazilian \ufb02ag with I NSIDE. (3) Comparing the global frequency of patterns showing a \u201cplayer or sports \ufb01eld\u201d against \u201cBrazilian \ufb02ag\u201d with O UTSIDE. ", "caption_bbox": [40, 211, 754, 251]}, {"image_id": 6, "file_name": "3124_06.png", "page": 8, "dpi": 300, "bbox": [392, 906, 730, 969], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Results of the closing questionnaire. Mean values are indicated by a black bar. Questions marked with an asterisk have low absolute votes and are inconclusive. For details see Supplementary Table S6. ", "caption_bbox": [392, 977, 742, 1017]}, {"image_id": 7, "file_name": "3124_07.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 184], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Notes from the second user study. (1) Detailed inspection of an unexpected pattern through scale-up. (2) Zoom into the original location of a clustered pile of insets until it disperses. (3) Upon zoom-out, a new pattern appeared as an inset (see red arrow) and was recognized immediately. (4) Manual inspection of the context around the pile\u2019s origin (end of the blue line). (5) Focus on a pile of two insets due to their location. ", "caption_bbox": [27, 183, 744, 223]}], "3125": [{"image_id": 0, "file_name": "3125_00.png", "page": 1, "dpi": 300, "bbox": [73, 65, 723, 450], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Generative modeling of layouts for the Les Mis\u00e9rables character co-occurrence network [55]. We train our generative model to construct a 2D latent space by learning from a collection of example layouts (training samples). From the grid of generated samples, we can see the smooth transitions between the different layouts. This shows that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training samples. Users can use this sample grid of the latent space as a WYSIWYG interface to generate a layout they want, without either blindly tweaking parameters of layout methods or requiring expert knowledge of layout methods. The color mapping of the latent space represents the shape-based metric [23] of the generated samples. Throughout the paper, unless otherwise specified, the node color represents the hierarchical community structure of the graph [68, 79], so readers can easily compare node locations in different layouts. An interactive demo is available in the supplementary material [1]. Abstract\u2014Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics. Index Terms\u2014Graph, network, visualization, layout, machine learning, deep learning, neural network, generative model, autoencoder ", "caption_bbox": [73, 450, 724, 731]}, {"image_id": 1, "file_name": "3125_01.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 160], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Our encoder-decoder architecture that learns a generative model from a collection of example layouts. We describe it in Sect. 3", "caption_bbox": [28, 160, 687, 176]}, {"image_id": 2, "file_name": "3125_02.png", "page": 7, "dpi": 300, "bbox": [404, 488, 758, 926], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Computation time. The left table shows the layout computation times (D3, FA2, FM3 , and sfdp) for collecting the training data and training the model (Epoch). The layout computation times are the mean seconds for computing one layout per method per graph. The training computation times are the mean seconds for training one epoch (16K samples) per graph. The right chart shows that the average training loss is updated every batch to demonstrate that our models converge quickly. ", "caption_bbox": [405, 925, 755, 1020]}, {"image_id": 3, "file_name": "3125_03.png", "page": 7, "dpi": 300, "bbox": [40, 53, 757, 426], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Qualitative results of the lesmis graph using the four different models. The leftmost column shows the test input layouts that the models did not see in their training session. The test input layouts are computed with randomly assigned parameter values as described in Sect. 4.1. The other columns show the reconstructed layouts of the test inputs using the four models. The nodes with the same color, except gray, are structurally equivalent to each other. The nodes in gray are structurally unique. The results are discussed in detail in Sect. 4.6. ", "caption_bbox": [40, 425, 755, 480]}, {"image_id": 4, "file_name": "3125_04.png", "page": 8, "dpi": 300, "bbox": [27, 29, 745, 425], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Qualitative results of the five different graphs. The GIN-MLP model is used for the football graph and the GIN-MLP+GW models are used for the other graphs. For each pair of layouts, the left is the test input and the right is the reconstructed layout. The first three rows show the different styles of layouts for each graph, and the bottom two rows shows the reconstruction results of hairball layouts. The results are discussed in Sect. 4.6. ", "caption_bbox": [28, 424, 744, 466]}, {"image_id": 5, "file_name": "3125_05.png", "page": 9, "dpi": 300, "bbox": [41, 52, 757, 380], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Visualization of the latent spaces of can96 and rajat11. The GIN-MLP model is used for can96 and the GIN-MLP+GW model is used for rajat11. The grids of layouts are the generated samples by the decoding of a 8 \u00d7 8 grid in [\u22121, 1]2 . Also, Fig. 1 shows the sample grid of lesmis. The smooth transitions between the generated layouts show that the capability of generalization of our models. Novices can directly use this as a WYSIWYG interface to generate a layout they want. The rightmost column shows the heatmaps of the four layout metrics of 540 \u00d7 540 generated sample layouts in [\u22121, 1]2 of rajat11. Experts can use the heatmaps to see complex patterns of layout metrics on diverse layouts. The results are discussed in detail in Sect. 4.6 and Sect. 5. The results of other graphs and models are available in the supplementary material [1]. ", "caption_bbox": [40, 382, 757, 463]}], "3126": [{"image_id": 0, "file_name": "3126_00.png", "page": 3, "dpi": 300, "bbox": [28, 29, 394, 281], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The work\ufb02ow of graph drawing algorithms: (a) traditional graph drawing algorithms, (b) the proposed deep learning based approach. ", "caption_bbox": [28, 282, 378, 309]}, {"image_id": 1, "file_name": "3126_01.png", "page": 4, "dpi": 300, "bbox": [391, 29, 757, 335], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. An illustration of the proposed graph-LSTM-based model archi- tecture: (a) an example graph input, (b) the proposed model to process the input graph. The graph nodes are sorted using BFS, with each node represented by an adjacency vector encoding its connections with pre- decessor nodes. The dotted yellow arrows (\u201cfake\u201d edges) propagate the prior nodes\u2019 overall in\ufb02uence on the drawing of subsequent nodes, and the curved green arrows (real edges of graphs) explicitly re\ufb02ect the actual graph structure, enhancing the graph drawing details. The information of both forward and backward rounds is considered for generating the \ufb01nal 2D node layouts. ", "caption_bbox": [404, 336, 756, 468]}, {"image_id": 2, "file_name": "3126_02.png", "page": 4, "dpi": 300, "bbox": [404, 487, 758, 963], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The same graph drawing under transformations may look different: (a) the original graph drawing, (b) the same graph drawing that has been translated, rotated by 180 degrees and further scaled. ", "caption_bbox": [404, 977, 756, 1017]}, {"image_id": 3, "file_name": "3126_03.png", "page": 6, "dpi": 300, "bbox": [391, 71, 758, 543], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Qualitative evaluation results on grid graphs: the ground truth and the graph drawing generated by our approach are compared side by side. Each row shows the results of a speci\ufb01c drawing style (i.e., perfect grid layout, ForceAtlas2 and PivotMDS) and the number of graph nodes is shown in the bottom. ", "caption_bbox": [404, 548, 754, 615]}, {"image_id": 4, "file_name": "3126_04.png", "page": 6, "dpi": 300, "bbox": [411, 635, 736, 884], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Qualitative evaluation results on star graphs: the ground truth and the graph drawing generated by our approach are compared side by side. Each row shows the results of a speci\ufb01c drawing style (i.e., perfect star layout, ForceAtlas2 and PivotMDS) and the number of graph nodes is shown in the bottom. ", "caption_bbox": [404, 890, 754, 957]}, {"image_id": 5, "file_name": "3126_05.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 429], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Qualitative evaluation on general graphs drawn by ForceAtlas2. For the same graph, the ground truth drawing, the drawing by the baseline model and the drawing by our approach are compared in each row. Different colors indicate different communities. ", "caption_bbox": [392, 434, 744, 487]}, {"image_id": 6, "file_name": "3126_06.png", "page": 8, "dpi": 300, "bbox": [490, 286, 665, 420], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. A comparison of the average running time (second) for drawing each graph using different methods. (a) The average running time on CPU, where the error bars are 95% con\ufb01dence intervals and the graph drawing techniques with signi\ufb01cant difference are marked with a line between them (\u2217: p < 0.05), (b) the average running time of the baseline method and our approach on GPU. ", "caption_bbox": [404, 425, 754, 505]}, {"image_id": 7, "file_name": "3126_07.png", "page": 8, "dpi": 300, "bbox": [391, 29, 757, 206], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. The results of Procrustes Statistic-based similarity. The base- line method and our approach are evaluated on both ForceAtlas2 and PivotMDS graph drawing datasets. The error bars are 95% con\ufb01dence intervals and signi\ufb01cant differences are marked with a line between them (\u2217: p < 0.05). ", "caption_bbox": [404, 213, 756, 281]}, {"image_id": 8, "file_name": "3126_08.png", "page": 8, "dpi": 300, "bbox": [39, 562, 758, 767], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 3. The RMSEs of aesthetic metrics-based similarity evaluated on on CPU. Speci\ufb01cally, the time cost of our approach is only 29% of the drawing datasets visualized by both ForceAtlas2 and PivotMDS.     ForceAtlas2 (0.189 second) and 82% of PivotMDS (0.067 second) ", "caption_bbox": [39, 767, 754, 794]}, {"image_id": 9, "file_name": "3126_09.png", "page": 9, "dpi": 300, "bbox": [391, 29, 745, 190], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. The loss curves of training the baseline and our model on ForceAtalas2 and PivotMDS drawing datasets. The training of our model on PivotMDS drawings is stopped at 330 epochs due to its convergence. ", "caption_bbox": [392, 195, 744, 235]}], "3127": [{"image_id": 0, "file_name": "3127_00.png", "page": 2, "dpi": 300, "bbox": [27, 29, 745, 282], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Transplanting the sub-layout selected by the black box in (a) to the corresponding region of the unbundled layout in (b). (c) Directly replacing the selected regions results in jagged boundaries; (d) smoothing the layout in (c) results in some bundled structures missed; (e) result produced by our method using the constraints of structural preservation and smoothness; and (f) result produced by our method further using the readability constraints that relieve visual ambiguity, where the upper and bottom boxes highlight the result changes before and after applying this constraint. ", "caption_bbox": [28, 297, 743, 350]}, {"image_id": 1, "file_name": "3127_01.png", "page": 3, "dpi": 300, "bbox": [87, 52, 342, 147], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Illustration of the edge point sets: U for whole graph; S for user-selected sub-region; and C for connecting edges from U \\ S to S. ", "caption_bbox": [40, 153, 390, 181]}, {"image_id": 2, "file_name": "3127_02.png", "page": 3, "dpi": 300, "bbox": [441, 52, 717, 252], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. (a) Blending the edge segments selected by the box (dashed lines) from source to corresponding region in the destination layout. (b) avoiding the occlusion between the blue edge and the pink convex hull by re-routing the blue edges using the readability constraint. ", "caption_bbox": [405, 258, 755, 311]}, {"image_id": 3, "file_name": "3127_03.png", "page": 4, "dpi": 300, "bbox": [391, 29, 746, 273], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Convergence of our method. (a) Input layout, and (b)-(d) results optimized after 5, 10, and 30 iterations, respectively. ", "caption_bbox": [392, 286, 742, 313]}, {"image_id": 4, "file_name": "3127_04.png", "page": 5, "dpi": 300, "bbox": [406, 854, 753, 931], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Path bundling. (a) Input unbundled layout (red boxes are the user-marked origins and destinations); (b) our path bundling result; and (c) our result further enhanced by applying the readability constraints to alleviate the node-edge occlusion, edge congestion and small edge crossing angle issues. ", "caption_bbox": [405, 937, 755, 1004]}, {"image_id": 5, "file_name": "3127_05.png", "page": 5, "dpi": 300, "bbox": [405, 563, 758, 766], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. (a) The unbundled input; (b) the result generated by global bundling, where the two edge sets are mixed together; (c) the result generated by only bundling the segments inside the selected region (note the discontinuity); and (d) our optimized local bundling result. ", "caption_bbox": [405, 771, 755, 824]}, {"image_id": 6, "file_name": "3127_06.png", "page": 5, "dpi": 300, "bbox": [41, 194, 758, 515], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                           user-selected region. Fig. 7 shows a complex example, where the area Fig. 7. Edge vector interpolation results in the user-selected region produced using different \u03c9: (a) \u03c9 = 0; (b) \u03c9 = 0.35; (c) \u03c9 = 0.75; and (d)                                                                            inside the dotted circle is bundled using different weights, increased \u03c9 = 1.                                                                     from (a) to (d). We can see that the bundles in the dotted circle are ", "caption_bbox": [40, 514, 755, 562]}, {"image_id": 7, "file_name": "3127_07.png", "page": 5, "dpi": 300, "bbox": [42, 52, 752, 157], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. These three results (a-c) are produced using the same \u03b1 and \u03b2 but different \u03b8 : 1, 20, and 400, respectively.", "caption_bbox": [40, 162, 602, 178]}, {"image_id": 8, "file_name": "3127_08.png", "page": 7, "dpi": 300, "bbox": [40, 52, 752, 362], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. These visualizations are created using different edge bundling methods and rendering styles for presenting recorded aircraft trajectories over France during one day. From left to right, the visual simpli\ufb01cation varies from no simpli\ufb01cation to a signi\ufb01cant one. The width of edge segments encodes the edge density, and the top row displays the edge segments with high densities on top so that the main bundles are more clearly shown, while the bottom row displays the edges in a reverse way. The last column shows a strong aggregation with high transparency, where the main air \ufb02ows are more clearly shown. From these various rendering and edge bundling methods, the user can decide which part of each visualization best shows the relevant information and combine them together by using our blending method. For example, the regions with labels \u201c2\u20135\u201d are selected to blend with the original layout with the label \u201c1\u201d together, formed the result shown in Fig. 13(b). ", "caption_bbox": [40, 367, 756, 460]}, {"image_id": 9, "file_name": "3127_09.png", "page": 8, "dpi": 300, "bbox": [392, 260, 746, 663], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15. (a) Input unbundled graph. (b) Cluster \ufb01sheye lens zoom-in on the red cluster with black border by (c) bundling the context area while keeping the focus area unbundled. Route the unrelated edges, which go through the focused cluster. (d) Multi-focus \ufb01sheye lens on the red and purple clusters, and bundle the associated context area. ", "caption_bbox": [392, 670, 742, 737]}, {"image_id": 10, "file_name": "3127_10.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 194], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14. This example shows the usage of the origin-destination feature. (a) The original US migration dataset with the links in yellow between the selected source and destination nodes, and the bounding box of these links in blue; (b) the bundled result generated by applying the KDEEB algorithm [17] to (a); and (c) further aggregated result by our method, where the ambiguity between edges shown in the black box is resolved. ", "caption_bbox": [28, 203, 743, 243]}, {"image_id": 11, "file_name": "3127_11.png", "page": 9, "dpi": 300, "bbox": [40, 602, 392, 808], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 16. This \ufb01gure shows one of our method limitation when blending too divergent graph layouts. (a) the original US Migration dataset; (b) the bundled version of this dataset; (c) our output; and (d) a magni\ufb01ed view of our blending method with strong edge-edge overlap. ", "caption_bbox": [40, 821, 390, 874]}], "3128": [{"image_id": 0, "file_name": "3128_00.png", "page": 1, "dpi": 300, "bbox": [73, 65, 714, 431], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. (a) The Le\u0301s Miserables graph is drawn using a Fruchterman-Reingold (F-R) force-directed layout [31]. Our approach provides two mechanisms for interacting with the force-directed layout using (e) the persistence barcode. (b) The \ufb01rst mechanism contracts nodes of the graph associated with features of low signi\ufb01cance or persistence. (c) The second mechanism partitions the graph using user-selected features and repulses the nodes in different partitions from one another. (d) When combined, this approach allows interactively controlling the layout to emphasize user-selected aspects of the graph using persistent homology. ", "caption_bbox": [73, 448, 722, 515]}, {"image_id": 1, "file_name": "3128_01.png", "page": 2, "dpi": 300, "bbox": [391, 29, 746, 868], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Example of extracting 0-dimensional PH of a graph. (a) Given an undirected graph G with edge weights w, we obtain a metric space representation by converting weights to distances by using d = 1/w and completing the metric space using shortest-path distance. (b) Conceptu- ally, components, or PH features, are formed around each point in the metric space. Balls grow around the points in the metric space to identify the diameter t at which components merge into larger components. (c) A \ufb01ltration is constructed from G by adding edges when two balls intersect. When two components merge into one, the bar associated with one of the component in the persistence barcode (bottom) terminates. ", "caption_bbox": [392, 885, 744, 1017]}, {"image_id": 2, "file_name": "3128_02.png", "page": 3, "dpi": 300, "bbox": [403, 56, 758, 959], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Example of information extracted from a spanning tree (left) for edge e3 from Fig. 2(c) (the purple bar). The right shows the clusters created when a selected edge is removed from the spanning tree. ", "caption_bbox": [405, 977, 755, 1017]}, {"image_id": 3, "file_name": "3128_03.png", "page": 4, "dpi": 300, "bbox": [28, 30, 209, 177], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Example MST with as- sociated subset ratios. Nodes toward the periphery have low ratios, whereas more central nodes have balanced ratios. ", "caption_bbox": [208, 111, 365, 178]}, {"image_id": 4, "file_name": "3128_04.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 351], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Illustration of forces applied to the graph in Fig. 5(a). (a, c, and e): A contracting force applied to the layout. (b, d, and f): A repulsive force is then added to the layout from (e). ", "caption_bbox": [392, 367, 744, 407]}, {"image_id": 5, "file_name": "3128_05.png", "page": 4, "dpi": 300, "bbox": [48, 719, 746, 978], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The force-directed layout (a) for the graph in Fig. 2 is constructed  below a user-selected threshold. This interaction is done by dragging a using (c) repulsive forces, (d) spring attractive forces, and (e) a centering \ufb01lter bar at the top of the barcode; see the \ufb01lter bar in red in Fig. 6(c). force. The interactive barcode (b) is used to manipulate the display.         As the threshold is dragged left to right, PH features with persistence ", "caption_bbox": [28, 977, 745, 1017]}, {"image_id": 6, "file_name": "3128_06.png", "page": 5, "dpi": 300, "bbox": [46, 679, 759, 991], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Two scenarios for the Le\u0301s Miserables graph are shown where        than 100. Instead, the graph nodes are surrounded by a halo, which is repulsion may be considered by a user (top) and the result of it (bottom). colored according to the set they belong to. ", "caption_bbox": [40, 990, 754, 1020]}, {"image_id": 7, "file_name": "3128_07.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 637], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Illustration of our approach on the synthetic graph examples: (a) a Fruchterman-Reingold (F-R) force-directed layout, hand-tuned; (b) a Neato layout [72] generated by Graphviz [29] using Jaccard index for edge weights; (c) our approach: only contraction is applied; (d) our approach: only repulsion is applied; and (e) our approach: both contraction and repulsion are applied. ", "caption_bbox": [28, 653, 743, 693]}, {"image_id": 8, "file_name": "3128_08.png", "page": 7, "dpi": 300, "bbox": [40, 54, 752, 614], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Illustration of our approach on a series of graph examples: (a) Fruchterman-Reingold force-directed layout, hand-tuned; (b-c) 2 examples of modularity hierarchical clustering with cluster number selected manually; (d-e) 2 examples using our approach. ", "caption_bbox": [40, 631, 755, 658]}, {"image_id": 9, "file_name": "3128_09.png", "page": 8, "dpi": 300, "bbox": [392, 429, 746, 957], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Examples from the Madrid Train Bombing dataset: (a) the conventional layout, as recreated from the original paper [79]; (b) the \ufb01nal visualization using our layout highlighting key players in the network. ", "caption_bbox": [392, 977, 744, 1017]}, {"image_id": 10, "file_name": "3128_10.png", "page": 9, "dpi": 300, "bbox": [58, 52, 759, 385], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Co- and anti-voting graphs for the US Senate in 2007 and 2008. the amount of interaction saved by such an approach would not be that Using a mixture of contracting and repulsive forces, these graphs show  signi\ufb01cant. In our experience, exploring the contraction and repulsion the role of major political \ufb01gures during these timeframes. Color are   options takes only a few minutes. Second and more importantly, the Democrats: blue; Republicans: red; and independents: purple.            interaction process provides intuition about the graphs. This intuition ", "caption_bbox": [40, 384, 755, 437]}], "3129": [{"image_id": 0, "file_name": "3129_00.png", "page": 1, "dpi": 300, "bbox": [86, 65, 706, 470], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Different sampling methods for presenting the four-class Person Activity data [8]. (a) The left shows the input scatterplots with 100K points and the right shows the four classes separately, where the patterns of each class are obscured in the main plot, e.g., the three sub-clusters in the purple class, due to overdraw. We re-sample the data into \u223c5000 points using (b) random sampling, (c) non-uniform sampling [4], (d) multi-class blue noise sampling [11], and (e) our method. The results show that our method better preserves major outliers (see the rounded boxes labeled with \u201c1\u201d), relative data densities (see the ellipse labeled with \u201c2\u201d to compare (c) with (d)), and the relative class densities (see the orange points shown in the squares labeled with \u201c3\u201d in (a)-(e)), without introducing obvious visual artifacts such as highlighted by the square in (d) labeled with \u201c4\u201d. Points for all results are rendered in random order. ", "caption_bbox": [73, 470, 723, 564]}, {"image_id": 1, "file_name": "3129_01.png", "page": 3, "dpi": 300, "bbox": [650, 30, 758, 683], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. (a) input s- catterplot and (b) multi-class densi- ty map over 2D grid cells. ", "caption_bbox": [666, 684, 757, 751]}, {"image_id": 2, "file_name": "3129_02.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 181], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Pipeline of our method: (a) the original two-class scatterplot; (b) a multi-class density map Di is created from (a); (c) a binary kd-tree is built based on the relative data density (black lines indicate the split axes); (d) we determine the class to be shown in each leaf-node region by ensuring class visibility and locally preserving relative class densities; (e) the \ufb01nal point samples are randomly selected based on the results in (c). ", "caption_bbox": [28, 185, 743, 226]}, {"image_id": 3, "file_name": "3129_03.png", "page": 4, "dpi": 300, "bbox": [98, 242, 305, 326], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. An example iteration when building the binary kd-tree shown in Fig. 3(c), lower right. The split along the thick red line in (a) leads to the two new red leaf nodes boxed in (b). ", "caption_bbox": [28, 330, 378, 370]}, {"image_id": 4, "file_name": "3129_04.png", "page": 5, "dpi": 300, "bbox": [408, 53, 754, 186], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Two straightforward strategies for kd-tree based multi-class sam- pling. (a) Input scatterplot (from Fig. 3(a)); (b) random sampling produces a point set where all orange points are missed; (c) giving high priority to rare class helps preserve them, but changes relative class densities. ", "caption_bbox": [405, 192, 757, 245]}, {"image_id": 5, "file_name": "3129_05.png", "page": 5, "dpi": 300, "bbox": [198, 873, 386, 1002], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Backtracking procedure.", "caption_bbox": [214, 1004, 369, 1018]}, {"image_id": 6, "file_name": "3129_06.png", "page": 5, "dpi": 300, "bbox": [391, 453, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Illustration of our three-step multi-class sampling. A sub-region was extracted from the tree in Fig. 3(c); (b) Backtracking from the leaf to an interior node in the sub-tree; (c) assigning a class label to each leaf so that all classes are covered; and (d) \ufb01nal sampling result. ", "caption_bbox": [404, 377, 754, 430]}, {"image_id": 7, "file_name": "3129_07.png", "page": 7, "dpi": 300, "bbox": [411, 582, 753, 817], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Boxplots summarizing the scores of four measures for four sampling methods over all tested datasets: (a) PDDr and (b) PCDr, where a higher score indicates better sampling for both measures; (c) ESRr and (d) ECSr, where a lower score indicates better sampling. ", "caption_bbox": [404, 825, 756, 878]}, {"image_id": 8, "file_name": "3129_08.png", "page": 7, "dpi": 300, "bbox": [46, 52, 754, 508], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Parameter analysis on the Person Activity data set. (a,b,c) Grid size in\ufb02uences the number of point samples. From left to right, the results have 5969, 2273, and 1217 points, respectively. (d,e,f) For a large \u03bb , many outliers become visible, but overdraw happens in dense areas, while a small \u03bb reduces overdraw but miss a few outliers. (g,e,h) A large \u03c4 shows too many outliers and regions of medium density are suppressed, while a small \u03c4 is more balanced but outliers are reduced. (i) When \u03bb and \u03c4 both are large, the overdraw issue becomes severe while showing many outliers. ", "caption_bbox": [40, 510, 757, 566]}, {"image_id": 9, "file_name": "3129_09.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 371], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13. Sampled result of the MNIST dataset: (a) original scatterplot with 70K data points; (b,c,d) sampled results generated by random sampling, non-uniform sampling and our method. ", "caption_bbox": [392, 376, 744, 416]}, {"image_id": 10, "file_name": "3129_10.png", "page": 8, "dpi": 300, "bbox": [28, 29, 394, 224], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. (a) Runtimes of the four sampling methods; (b) relationship between runtime and sampled points (logarithmic scale). ", "caption_bbox": [28, 227, 378, 254]}, {"image_id": 11, "file_name": "3129_11.png", "page": 9, "dpi": 300, "bbox": [411, 55, 752, 437], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15. Sampling of a multi-class scatterplot matrix. (a) Two input scat- terplots with 50K data samples de\ufb01ned by the selected three variables; (b) the corresponding two sampled scatterplots with 4K samples. ", "caption_bbox": [405, 443, 757, 483]}, {"image_id": 12, "file_name": "3129_12.png", "page": 9, "dpi": 300, "bbox": [44, 52, 387, 404], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14. Exploration of the power consumption dataset with 1,570K data points: (a) the original scatterplot; (b) the sampled result generated by our method. ", "caption_bbox": [40, 409, 390, 449]}], "3130": [{"image_id": 0, "file_name": "3130_00.png", "page": 1, "dpi": 300, "bbox": [40, 29, 757, 507], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Our method creates a sub-sampled point set that is optimized for different views of a SPLOM and per class. In contrast to blue noise scatterplot sampling (and other sampling methods), which create different point sets for different views, our method uses joint optimization to yield a single point set for multiple views. This way, it not only optimizes for multi-view and multi-class scatterplots simultaneously, but also presents results perceptually similar to the original data distributions while reducing overdraw. ", "caption_bbox": [73, 523, 722, 576]}, {"image_id": 1, "file_name": "3130_01.png", "page": 3, "dpi": 300, "bbox": [39, 286, 394, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Sampling based on Z-order: (a) input set of points; (b) quadtree subdivision and z-order curve of the point set, where points inside the same subset are connected with solid lines (here each subset consists of 3 points); (c) \ufb01nal sampling result, composed of only one point from each subset. ", "caption_bbox": [40, 194, 390, 261]}, {"image_id": 2, "file_name": "3130_02.png", "page": 3, "dpi": 300, "bbox": [391, 364, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Reduction of simultaneous point sampling for multiple views into a set cover problem. For each view in (a), the Z-order algorithm partitions the input point set into n subsets, where n = 3 in this example. From the partition, we can de\ufb01ne a set cover matrix (b), where an entry (i, j) is non-zero (not white) if subset i contains point j. Our goal then is to select a set of points (columns) so that all the subsets (rows) are covered without redundancy. Given that certain points cover the same subsets (columns with the same color in (b)), we can merge similar points (c) and simplify the set cover problem. In this example, we can \ufb01nd two different covers indicated by the two different colors (blue and yellow). ", "caption_bbox": [404, 201, 755, 333]}, {"image_id": 3, "file_name": "3130_03.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 210], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Sampling with outlier inclusion. Original scatterplot (a) with two classes (blue and red), where outliers are marked with green stars. The samplings (b) and (c) both cover each subset of the individual views (per-class and global view), but (c) selects the outliers of the subsets. ", "caption_bbox": [28, 225, 743, 252]}, {"image_id": 4, "file_name": "3130_04.png", "page": 5, "dpi": 300, "bbox": [437, 641, 724, 796], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Scalability of our method: number of sample points obtained and sampling time, while increasing the number of views in the dataset. ", "caption_bbox": [405, 817, 755, 844]}, {"image_id": 5, "file_name": "3130_05.png", "page": 5, "dpi": 300, "bbox": [40, 29, 757, 302], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1. Time required for processing selected datasets and \ufb01nal number of sampled points, along with other dataset statistics. All the datasets consist of 20K points and our goal is to sample 1K points. Note that, for datasets with a single class, #partitions = #views, while for datasets with more than one class, #partitions = #views \u00d7 2. ", "caption_bbox": [405, 396, 755, 464]}, {"image_id": 6, "file_name": "3130_06.png", "page": 6, "dpi": 300, "bbox": [422, 269, 714, 416], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Effect of view selection: number of sample points, sampling time, and KDE error, when increasing the number of discarded views. ", "caption_bbox": [392, 431, 743, 458]}, {"image_id": 7, "file_name": "3130_07.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 201], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Comparison of sampling results on view 3 (V3 ) of the sythetic dataset, where the sampling was computed for speci\ufb01c and all views. ", "caption_bbox": [392, 215, 742, 242]}, {"image_id": 8, "file_name": "3130_08.png", "page": 7, "dpi": 300, "bbox": [40, 29, 757, 577], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Comparison of different sampling methods for multi-class scatterplots on six selected datasets. For each dataset, we show on the left the scatterplot drawn with the full point set as reference, while on the right the boxplots present the distribution of KDE errors, indicating the median, minimum, maximum, \ufb01rst and third quartiles, and outliers. We show one boxplot for each of the four sampling methods, both when considering the global distribution of points (\u201cVglobal \u201d\u2019) and individual classes (\u201cVlocal \u201d\u2019). Note that the lowest errors are consistently provided by our sampling method that considers the local and global views (blue boxplots), while all the Z-order-based samplings provide better results than the blue noise method. ", "caption_bbox": [40, 591, 756, 658]}, {"image_id": 9, "file_name": "3130_09.png", "page": 8, "dpi": 300, "bbox": [393, 372, 743, 722], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Three example questions of T4 in the user study on multi- class scatterplots, where the users selected our sampling as being more perceptually similar to the original point distribution. Note how the results of blue noise sampling tend to be too uniformly distributed. ", "caption_bbox": [392, 738, 744, 791]}, {"image_id": 10, "file_name": "3130_10.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 259], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Results of the user study on multi-class scatterplots. The boxplots show the average (higher is better) and standard deviation of the scores obtained by users for each sampling method on tasks T1, T2 and T3, where C, B, and O represent conventional scatterplot, blue noise sampling, and our sampling, respectively. Note the higher scores obtained with our sampling on T1 and T3, and comparative result on T2. ", "caption_bbox": [392, 274, 744, 354]}, {"image_id": 11, "file_name": "3130_11.png", "page": 9, "dpi": 300, "bbox": [40, 29, 758, 321], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Results for the user study on multi-class scatterplot matrices.     Speci\ufb01cally, the use of our sampling can overcome problems such as We use the same visualization as Chen et al. [10], where the violin plots    continuous drawing of points of the same class in the animation, which show the density of ratings, the boxplots show the \ufb01rst and third quartiles, some users found confusing. ", "caption_bbox": [40, 320, 755, 361]}], "3131": [{"image_id": 0, "file_name": "3131_00.png", "page": 3, "dpi": 300, "bbox": [40, 52, 757, 531], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Data and image similarity measures: Mean-Squared Error (MSE), Structural Similarity Index (SSIM), and Multi-Scale SSIM (MS-SSIM). Leftmost images in each row are the references. Top: global deaths from natural disasters (Vega-lite gallery) and simulated perturbations. Middle: unemployment across industries (Vega-lite gallery) and simulated perturbations. Bottom: graphical models of passwords [48]. MSE is inversely proportional to similarity. MS-SSIM weights: [0.1, 0.1, 0.1, 0.2, 0.5]. ", "caption_bbox": [40, 547, 757, 602]}, {"image_id": 1, "file_name": "3131_01.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 331], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: The effect of grids on SSIM for scatterplots of the Iris dataset. (a-d): SSIM with grids. Top row: local SSIM values for these plots (brighter is more similar). (e-h): the same comparison without grids. Scores are relative to the leftmost plots in each row. ", "caption_bbox": [392, 345, 744, 400]}, {"image_id": 2, "file_name": "3131_02.png", "page": 5, "dpi": 300, "bbox": [41, 52, 392, 195], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1: Cluster quality measures for clusterings of 247 scatter plots based on MS-SSIM. The quality measures are relative to the clustering based on human similarity judgments reported by Pandey et al. [33]. Each row corresponds to a parameter set (w1 ..w5 ). The parameters in the \ufb01rst row were obtained through gradient descent. ", "caption_bbox": [404, 53, 756, 122]}, {"image_id": 3, "file_name": "3131_03.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 377], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 2: Kim and Heer\u2019s experiment was divided into four tasks. Q1 is a continuous variable. ", "caption_bbox": [392, 444, 742, 472]}, {"image_id": 4, "file_name": "3131_04.png", "page": 7, "dpi": 300, "bbox": [404, 355, 757, 933], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: Pairs of colored scatterplots (y x color) with y values swapped between two categories. a) and b) have 3 categories in total, while c) and d) have 30 categories. These pairs (a,b) and (c,d) are used to measure the visual discriminability of two categories (other categories \ufb01xed) along one variable. ", "caption_bbox": [405, 946, 755, 1015]}, {"image_id": 5, "file_name": "3131_05.png", "page": 7, "dpi": 300, "bbox": [405, 56, 757, 222], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Images generated for the global discriminability test. Left: Original plot used by Kim and Heer. Right: Plots depicting variations of the original data, resulting from sampling from statistical models \ufb01tted to Kim and Heer\u2019s data. Only the question variable Q1 (WSF5 in this example) is simulated. The simulated data is depicted using position encoding (y x color), and size encoding (size y x) for Q1 . ", "caption_bbox": [405, 233, 756, 317]}, {"image_id": 6, "file_name": "3131_06.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 382], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Global and Local discriminability scores computed with MS-SSIM (W = [1, 1, 1, 1, 1]), aggregated by data attributes. For each dimension, we computed the Pearson correlation coef\ufb01cient against the empirical accuracy measured by Kim and Heer [22], using the mean values for each encoding. The signi\ufb01cance codes correspond to the null hypothesis that correlation is 0. Signi\ufb01cance: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 ", "caption_bbox": [27, 393, 744, 437]}, {"image_id": 7, "file_name": "3131_07.png", "page": 9, "dpi": 300, "bbox": [65, 53, 739, 318], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Discriminability rankings of encodings (divided by data property) derived from discriminability scores.", "caption_bbox": [121, 333, 674, 347]}], "3132": [{"image_id": 0, "file_name": "3132_00.png", "page": 2, "dpi": 300, "bbox": [391, 29, 746, 870], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Pipeline for computing scagnostics, where the Outlying value is obtained after all outliers are removed, and then the other measures are computed. ", "caption_bbox": [392, 882, 742, 923]}, {"image_id": 1, "file_name": "3132_01.png", "page": 3, "dpi": 300, "bbox": [421, 454, 655, 598], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Two edges used for comput- ing the Clumpy measure. ", "caption_bbox": [666, 506, 757, 559]}, {"image_id": 2, "file_name": "3132_02.png", "page": 3, "dpi": 300, "bbox": [409, 30, 756, 203], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Differences between the de\ufb01nition of the Outlying measure in Scag-05 (a) and Scag-06 (b,c). (a) Only the point v1 is removed by the older version of outlying. In Scag06, v1 and v2 are removed in the \ufb01rst iteration (b) and v3 , v4 and v5 are further removed in the second iteration (c) , which results in a new long edge of the graph. ", "caption_bbox": [405, 213, 755, 280]}, {"image_id": 3, "file_name": "3132_03.png", "page": 3, "dpi": 300, "bbox": [41, 99, 388, 203], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Examples of four geometric graphs built after deleting outliers highlighted in gray. ", "caption_bbox": [40, 205, 390, 232]}, {"image_id": 4, "file_name": "3132_04.png", "page": 4, "dpi": 300, "bbox": [28, 29, 393, 315], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Examples of scatterplots with different cluster-speci\ufb01c charac- teristics generated by Binormal distributions: (a) a circular distribution; (b) a rotated elliptical distribution; (c) a circular distribution with varying densities; (d) a three-cluster distribution with varying cluster sizes; (e) a four-cluster distribution with varying cluster densities and shapes; (f) the two-cluster distribution with interior outliers (shown in red). ", "caption_bbox": [27, 329, 379, 409]}, {"image_id": 5, "file_name": "3132_05.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 200], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Relative changes of various measures in Scag-06 as a result of including or omitting the binning step. In case a result is out of the plot range (see Outlying and Clumpy ), we draw a dark transparent shadow to indicate the amount. ", "caption_bbox": [392, 212, 742, 267]}, {"image_id": 6, "file_name": "3132_06.png", "page": 4, "dpi": 300, "bbox": [403, 277, 734, 871], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Dot plot showing Spearman\u2019s rank correlation coef\ufb01cient \u03c1 of eight scagnostic measures in Scag-05, Scag-06, and our proposed RScag (see Section 6) obtained by applying random deletion operations to our scatterplot dataset. The regression trend lines of \u03c1 values except Outlying and Clumpy measures are shown in gray dashes. Values close to 1 indicate that the measure was relatively consistent across plots even after points were deleted. ", "caption_bbox": [392, 884, 742, 978]}, {"image_id": 7, "file_name": "3132_07.png", "page": 5, "dpi": 300, "bbox": [407, 196, 754, 324], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Illustrating the stability of MST. (a) MST of an input scatterplot with the two points to be deleted indicated by circles; (b) deleting one boundary point and its adjacent edge (in red) does not change the MST structure; (c) deleting an interior point and its adjacent edges introduces new edges in purple, resulting in a structural change of the MST. 25th and 75th percentiles of the sorted lengths of the MST edges in (a,b,c) ", "caption_bbox": [404, 338, 754, 418]}, {"image_id": 8, "file_name": "3132_08.png", "page": 6, "dpi": 300, "bbox": [28, 29, 746, 203], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. An example where the Outlying measure de\ufb01ned in Scag-06 is sensitive to perturbation, while the version in Scag-05 does not detect   5 U SER S TUDY any outliers. The scatterplot and its \ufb01nal MST are de\ufb01ned by solid edges, Pandey et al. [35] investigated perceived similarity in scatterplots by while the dotted and pink lines depict deleted and the newly inserted     comparing the results of user-driven groupings with the Euclidean edges during outlier deletion. (a,c) Original scatterplot and MST; (b)    distances of all 9 scagnostic measures. However, it is unclear how Scatterplot and MST generated by deleting the circled point in (a).       individual scagnostic measures align with human judgments. As we ", "caption_bbox": [27, 202, 742, 283]}, {"image_id": 9, "file_name": "3132_09.png", "page": 6, "dpi": 300, "bbox": [30, 286, 746, 439], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                      comparison between Scag-06 and human judgments; our results using Fig. 10. Scatterplots with varying densities and numbers of clusters  Scag-05 exhibit similar patterns and can be found in the supplementary and the two edges in each scatterplot used for computing Clumpy value materials. ", "caption_bbox": [28, 438, 742, 480]}, {"image_id": 10, "file_name": "3132_10.png", "page": 7, "dpi": 300, "bbox": [42, 52, 387, 175], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Example comparisons from tasks I and II.", "caption_bbox": [40, 181, 286, 195]}, {"image_id": 11, "file_name": "3132_11.png", "page": 7, "dpi": 300, "bbox": [417, 30, 757, 212], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13. Response times of the lab study with perturbation by deletion. We show mean values and deviation as 95% CIs of response times in terms of Outlying (a) and Clumpy (b). ", "caption_bbox": [404, 218, 756, 258]}, {"image_id": 12, "file_name": "3132_12.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 244], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14. Results of the lab study: Mean values and deviation for 95% CIs of error rates in terms of Outlying (a) and Clumpy (b,c) measures, which are de\ufb01ned in Scag-06 are shown in black (see Section 5), while our proposed measures in red of each study (see Section 6). ", "caption_bbox": [28, 249, 743, 277]}, {"image_id": 13, "file_name": "3132_13.png", "page": 8, "dpi": 300, "bbox": [27, 299, 746, 874], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15. The pipeline for computing RScag. After adaptive binning and of large sub-clusters (see Figs. 10 (a,b)). To address this issue, we building the cluster hierachy, all measures are computed except the   re-de\ufb01ne the Clumpy measure by incorporating the longest edge em of Monotonic measure.                                                    the larger sub-cluster and the number of points in each of them: ", "caption_bbox": [28, 873, 743, 915]}, {"image_id": 14, "file_name": "3132_14.png", "page": 9, "dpi": 300, "bbox": [470, 30, 757, 185], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1. Average runtime (in ms) for three versions of scagnostics for Binormal scatterplots with different numbers of points. ", "caption_bbox": [405, 529, 755, 556]}], "3133": [{"image_id": 0, "file_name": "3133_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 349], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. (Left) A common visualization of a multi-class scatterplot uses colors to associate the points to their clusters. The uncertainty is visually encoded by darkening the colors. (Right) Little wings attached to the points have a stronger descriptive power of association based on the Gestalt principles. The wings length expresses the associating uncertainty of a point. ", "caption_bbox": [60, 362, 709, 402]}, {"image_id": 1, "file_name": "3133_01.png", "page": 2, "dpi": 300, "bbox": [391, 29, 756, 329], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. An example showing how Winglets can enhance the grouping perception in a scatterplot, both with and without colors ", "caption_bbox": [404, 341, 754, 369]}, {"image_id": 2, "file_name": "3133_02.png", "page": 2, "dpi": 300, "bbox": [39, 29, 393, 380], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The left and right scatterplots use the exact same con\ufb01guration of points and coloring. The \ufb01gure emphasizes the strength of Winglets in shaping the perception of clusters, by forming horizontal clusters (left) and vertical clusters (right) on the exact same point con\ufb01guration. The two examples also show the interplay between coloring and Winglets. ", "caption_bbox": [40, 393, 391, 460]}, {"image_id": 3, "file_name": "3133_03.png", "page": 2, "dpi": 300, "bbox": [404, 387, 756, 515], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. The enclosure technique draws contours to visually encode hard association\u2019s boundaries. Winglets suggest the association in a local soft manner, with \ufb01ne-grain depiction on association, especially remov- ing the associating ambiguity of points in the joint region. ", "caption_bbox": [404, 528, 756, 581]}, {"image_id": 4, "file_name": "3133_04.png", "page": 4, "dpi": 300, "bbox": [404, 234, 756, 398], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Orienting Procedure: (a) Gaussian kernel density map is calcu- lated for the plot. (b) isocontours of sampled densities are extracted by Marching Squares algorithm. (c) a global reference contour is picked for the coherent perception of grouping, before splitting into multiple con- tour siblings (annotated in (b)). (d) contours are interpolated from out- side to inside. (e) points grow their wings along the orientation as their nearest points on the contours. ", "caption_bbox": [404, 412, 756, 505]}, {"image_id": 5, "file_name": "3133_05.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 547], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Orientation Choice of Winglets: there are two main types of wings\u2019 orientation according to whether the global form is open or closed. Compared with Contour, orienting towards Centroid, Line or Boundary Circle may fail in some cases (marked by dashed line boxes). ", "caption_bbox": [28, 566, 745, 594]}, {"image_id": 6, "file_name": "3133_06.png", "page": 6, "dpi": 300, "bbox": [404, 377, 756, 662], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Winglet joint use with other design tools in a scatterplot: (a) Winglets with color; (b) colorless Winglets; (c) Winglets with a density map to compensate for down-sampling; (d) Winglets with aggregation. ", "caption_bbox": [404, 676, 754, 717]}, {"image_id": 7, "file_name": "3133_07.png", "page": 8, "dpi": 300, "bbox": [40, 29, 758, 333], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Cluster and Overlap conditions: three options for the amount of clusters with and without Winglets; three levels of overlap with and without Winglets. Note that in the \ufb01gures the scatterplots are aligned. In the experiment, a random rotation was performed to vary each of the scatterplots. ", "caption_bbox": [40, 354, 757, 382]}, {"image_id": 8, "file_name": "3133_08.png", "page": 9, "dpi": 300, "bbox": [51, 384, 346, 469], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15. Error percentage with and without Winglets by task.", "caption_bbox": [28, 489, 323, 504]}, {"image_id": 9, "file_name": "3133_09.png", "page": 9, "dpi": 300, "bbox": [28, 29, 745, 309], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14. Time cost with and without Winglets averaged within the three options of cluster quantity (top) and the three levels of overlap (bottom) for the four tasks, with the error bars of standard deviation. ", "caption_bbox": [28, 322, 743, 350]}], "3134": [{"image_id": 0, "file_name": "3134_00.png", "page": 3, "dpi": 300, "bbox": [28, 29, 745, 335], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The work\ufb02ow of SmartCube. According to the original spatiotemporal data set, the algorithm builds the initial structure with the \ufb01nest cuboid, which contains the \ufb01nest aggregates. A machine learning based model can be added as an additional part to decide the cuboids that need to be created based on the statistic of the dimensions. The cuboids adding algorithm adds the cuboids to the data structure accordingly. SmartCube records user queries, and an updating decision making part decides whether a cuboid should be created or deleted according to the query records. ", "caption_bbox": [27, 338, 744, 391]}, {"image_id": 1, "file_name": "3134_01.png", "page": 4, "dpi": 300, "bbox": [40, 75, 756, 368], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The illustration of the updating algorithm. The structure on the leftmost only has Cuboid 1. This cuboid has all the possible levels, which are spatial levels S1 , S2 , and categorical level C, and temporal levels Year, month. The very right one is the tree with only cuboid 2 that aggregated on level S2 . In the middle are the virtual tree created during the process of creating Cuboid 2, and the tree with both Cuboid 1 and Cuboid 2 as well. Two cuboids share the nodes pointed by the dashed arrow line. ", "caption_bbox": [39, 368, 756, 421]}, {"image_id": 2, "file_name": "3134_02.png", "page": 6, "dpi": 300, "bbox": [39, 53, 755, 494], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. The pseudo-code of creating a new cuboid on the structure. In the left, the function AddCuboid builds the new cuboid based on the basic cuboid with the schema of these cuboids. In the right, the function AddCuboidChildren shares nodes or creates new nodes. ", "caption_bbox": [40, 496, 754, 523]}, {"image_id": 3, "file_name": "3134_03.png", "page": 8, "dpi": 300, "bbox": [404, 56, 758, 614], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. An example of visualization that requires the aggregations on several combination of the data sets. The constraints on time can be added by the right part. The query for spatial region is for time series, temporal pattern in hour of week days. ", "caption_bbox": [404, 613, 755, 666]}, {"image_id": 4, "file_name": "3134_04.png", "page": 9, "dpi": 300, "bbox": [392, 335, 744, 651], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. The heat map shows the weekday \u00d7 hour values according to the certain region on the map. The darker the square color is, the higher the value it represented is. ", "caption_bbox": [392, 653, 742, 694]}, {"image_id": 5, "file_name": "3134_05.png", "page": 9, "dpi": 300, "bbox": [27, 336, 379, 428], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. The performance of ML-initialize towards the cold start. It shows that ML reduces the query time before \ufb01nishing convergence. ", "caption_bbox": [28, 431, 378, 458]}, {"image_id": 6, "file_name": "3134_06.png", "page": 9, "dpi": 300, "bbox": [28, 29, 745, 269], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. (a) to (d) is the performance of Query Q1 to Q4 change over time in the BrightKite dataset; (a) for Q1, (b) for Q2, (c) for Q3, and (d) for Q4, respectively. (e) and (f) shows the query performance and memory change as the query pattern of different resolution. The query time convergence soon as the new cuboid are added to the data structure. ", "caption_bbox": [28, 272, 744, 312]}], "3136": [{"image_id": 0, "file_name": "3136_00.png", "page": 1, "dpi": 300, "bbox": [39, 313, 757, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The same data showing the relation between eating breakfast and GPA presented via text, bar graph, line graph or scatter plot. Which depiction makes eating breakfast causing higher GPA seem more plausible to you? ", "caption_bbox": [73, 273, 722, 300]}, {"image_id": 1, "file_name": "3136_01.png", "page": 2, "dpi": 300, "bbox": [399, 751, 738, 923], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Left: recreation of NPR article \"Money Buys Happiness,\" show- ing a correlation between GDP and life satisfaction [36]. Right: recre- ation of the Washington Post news article \ufb01gure, \"Researchers have debunked one of our most basic assumptions about how the world works,\" showing a correlation, but not causation, between income and SAT scores[14]. ", "caption_bbox": [392, 937, 742, 1017]}, {"image_id": 2, "file_name": "3136_02.png", "page": 3, "dpi": 300, "bbox": [40, 833, 394, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Pilot results. Grey numbers indicate the index of the 19 state- ments, details see supplementary. The line positions represent mean correlation and causation plausibility ratings. Red lines are the correla- tion and causation plausibility ratings for the selected contexts, intended to cover a range of plausibility. ", "caption_bbox": [40, 740, 390, 807]}, {"image_id": 3, "file_name": "3136_03.png", "page": 3, "dpi": 300, "bbox": [391, 944, 757, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Example of generative task (top) and judgment task (middle and bottom) in Experiment 1. The three questions were shown on separate pages in Qualtrics in the order from top to bottom. ", "caption_bbox": [405, 877, 755, 917]}, {"image_id": 4, "file_name": "3136_04.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 304], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. The bar graph stimulus in the four contexts.", "caption_bbox": [392, 319, 640, 333]}, {"image_id": 5, "file_name": "3136_05.png", "page": 5, "dpi": 300, "bbox": [39, 29, 758, 364], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Quantitative results from all three experiments showing participants\u2019 correlation and causation agreement ratings.", "caption_bbox": [40, 380, 630, 394]}, {"image_id": 6, "file_name": "3136_06.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 207], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Qualitative coding results of Experiment 1. Each bar represents the percentage of participants that mentioned the indicated dimension (e.g., third variable) for a certain visualization design. ", "caption_bbox": [28, 220, 743, 247]}, {"image_id": 7, "file_name": "3136_07.png", "page": 7, "dpi": 300, "bbox": [404, 301, 757, 422], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Main effect of aggregation levels (top) and visual encoding types (bottom) on correlation and causation ratings in Experiment 2. ", "caption_bbox": [405, 436, 755, 463]}, {"image_id": 8, "file_name": "3136_08.png", "page": 7, "dpi": 300, "bbox": [39, 29, 392, 333], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Three aggregation levels tested in Experiment 3 for bar, line and dot type encoding marks. ", "caption_bbox": [40, 346, 390, 373]}, {"image_id": 9, "file_name": "3136_09.png", "page": 7, "dpi": 300, "bbox": [405, 30, 758, 241], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Snapshots from Experiment 2 (left) and Experiment 3 (right).", "caption_bbox": [405, 257, 744, 271]}, {"image_id": 10, "file_name": "3136_10.png", "page": 8, "dpi": 300, "bbox": [410, 587, 726, 714], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13. Non-aggregated data visualized with bars, lines and dots.", "caption_bbox": [392, 728, 715, 742]}], "3137": [{"image_id": 0, "file_name": "3137_00.png", "page": 3, "dpi": 300, "bbox": [40, 327, 392, 447], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 2. Distribution of different visual element types", "caption_bbox": [451, 469, 709, 483]}, {"image_id": 1, "file_name": "3137_01.png", "page": 3, "dpi": 300, "bbox": [413, 160, 753, 302], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 2. Distributions of different presentation layouts across content structures. ", "caption_bbox": [405, 308, 755, 334]}, {"image_id": 2, "file_name": "3137_02.png", "page": 5, "dpi": 300, "bbox": [41, 53, 392, 150], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 4. A sample tabular dataset of tablet sales.", "caption_bbox": [95, 443, 336, 457]}, {"image_id": 3, "file_name": "3137_03.png", "page": 7, "dpi": 300, "bbox": [413, 420, 746, 646], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 5. The interface of DataShot. (A) is the control panel; (B) is the fact sheet presentation zone. After clicking the item listed in \u201cData Source\u201d, corresponding data sheet will be presented in the presentation zone \ufb01rst. Users can further interact with the system to generate different fact sheets with different styles. ", "caption_bbox": [405, 654, 755, 717]}, {"image_id": 4, "file_name": "3137_04.png", "page": 7, "dpi": 300, "bbox": [40, 53, 756, 365], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 4. Fact sheets generated by DataShot from three data tables, namely, SharkAttack, CarSales, and SummerOlympics. (A) is about the shark attack events happened in swimming activity; (B-C) are the sales status of sports cars and the manufacturer BMW; (D) shows the conditions for winning gold medals in the Summer Olympics from 1896 to 2012. ", "caption_bbox": [40, 377, 755, 416]}], "3138": [{"image_id": 0, "file_name": "3138_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 465], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 1. Examples created by Text-to-Viz. (a)-(d) are generated from the statement: \u201cMore than 20% of smartphone users are social network users.\u201d (e) and (f) are generated from the statement: \u201c40 percent of USA freshwater is for agriculture.\u201d (g) and (h) are generated from the statement: \u201c3 in 5 Chinese people live in rural areas.\u201d (i) and (j) are generated from the statement: \u201c65% of coffee is consumed at breakfast.\u201d (k)-(m) are generated from the statement: \u201cAmong all students, 49% like football, 32% like basketball, and 21% like baseball.\u201d (n) and (o) are generated from the statement: \u201cHumans made 51.5% of online traf\ufb01c, while good bots made 19.5% and bad bots made 29%.\u201d ", "caption_bbox": [61, 483, 710, 563]}, {"image_id": 1, "file_name": "3138_01.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 273], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1. Information categories of the collected infographics. Please note that the percentages are based on 983 infographic units, rather than 200 infographic sheets. Therefore, the numbers for non-statistics based infographics are exceptionally low, since they often take a larger space than statistics-based ones. ", "caption_bbox": [392, 538, 742, 605]}, {"image_id": 2, "file_name": "3138_02.png", "page": 4, "dpi": 300, "bbox": [40, 227, 391, 345], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 4. Exemplars of change-related infographics [57, 65]: (a) con- trast color + side-by-side comparison and (b) contrast color + arrows. ", "caption_bbox": [40, 360, 390, 387]}, {"image_id": 3, "file_name": "3138_03.png", "page": 4, "dpi": 300, "bbox": [391, 29, 758, 153], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 5. Exemplars of rank-related infographics [56, 70]: (a) highlighted keyword + star embellishment and (b) ordered small-multiples. ", "caption_bbox": [404, 169, 754, 196]}, {"image_id": 4, "file_name": "3138_04.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 223], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 6. Exemplars of infographics with multiple facts [50]: (a) side-by- side and sharing context, (b) sharing axes, (c) sharing center. ", "caption_bbox": [392, 239, 742, 266]}, {"image_id": 5, "file_name": "3138_05.png", "page": 6, "dpi": 300, "bbox": [391, 29, 756, 154], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 2. Training results for text analyzer model.", "caption_bbox": [461, 320, 698, 336]}, {"image_id": 6, "file_name": "3138_06.png", "page": 7, "dpi": 300, "bbox": [28, 29, 394, 158], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 8. (a) A layout blueprint example and (b) its realization.", "caption_bbox": [28, 175, 332, 189]}], "3139": [{"image_id": 0, "file_name": "3139_00.png", "page": 1, "dpi": 300, "bbox": [40, 29, 758, 360], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: An automated approach to extract an extensible timeline template from a bitmap image. a) Original bitmap image; b) Content understanding including global and local information of the timeline; c) Extensible template contains editable elements and their semantic roles; d) New timeline (with mock-up colors) automatically generated with updated data. ", "caption_bbox": [73, 361, 722, 402]}, {"image_id": 1, "file_name": "3139_01.png", "page": 3, "dpi": 300, "bbox": [409, 703, 753, 857], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Categories of elements in a timeline infographic. The event mark, annotation mark, and main body can be reused, while others need to be updated. ", "caption_bbox": [405, 856, 755, 898]}, {"image_id": 2, "file_name": "3139_02.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 227], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Example timelines from: a) a synthetic dataset D1 , which shows two different scales, and b) a real-world dataset D2 , which shows two different orientations. ", "caption_bbox": [28, 228, 743, 256]}, {"image_id": 3, "file_name": "3139_03.png", "page": 4, "dpi": 300, "bbox": [393, 540, 739, 649], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Initial architecture to parse the global information. After ex- tracting the feature map of an image, two FC layers are used to classify its type and orientation. ", "caption_bbox": [392, 654, 744, 695]}, {"image_id": 4, "file_name": "3139_04.png", "page": 5, "dpi": 300, "bbox": [660, 362, 757, 455], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 4: Average Precision of parsing local information.", "caption_bbox": [438, 621, 720, 635]}, {"image_id": 5, "file_name": "3139_05.png", "page": 6, "dpi": 300, "bbox": [415, 390, 727, 539], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: DL model \u201cinteracts\u201d with GrabCut by using the bbox and mask: a) the bbox and mask predicted by our model; b) the predicted mask is coarse; c) the re\ufb01ned result from DL GrabCut. ", "caption_bbox": [392, 538, 742, 579]}, {"image_id": 6, "file_name": "3139_06.png", "page": 7, "dpi": 300, "bbox": [48, 681, 749, 996], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13: Example results from D2 . We visualize the \ufb01nal predicted category, bbox, and mask of each element, following the color legend in Fig. 3. We use gray-scale images for a clear demonstration. The original images and extra results can be checked in the supplemental material. ", "caption_bbox": [40, 995, 757, 1023]}, {"image_id": 7, "file_name": "3139_07.png", "page": 7, "dpi": 300, "bbox": [405, 203, 757, 319], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12: The error from the imperfect label. a) an annotation mark and b) its manually labeled mask; c) the predicted mask of the annotation mark; d) the re\ufb01ned result from DL GrabCut. ", "caption_bbox": [405, 319, 755, 360]}, {"image_id": 8, "file_name": "3139_08.png", "page": 7, "dpi": 300, "bbox": [39, 29, 394, 236], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 5: Gains come from Reconstruction at IoU 0.5 and 0.75.", "caption_bbox": [423, 82, 737, 96]}, {"image_id": 9, "file_name": "3139_09.png", "page": 8, "dpi": 300, "bbox": [28, 510, 381, 937], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15: a) A timeline generated by using the template from Fig. 13b. b) The result of applying the representation from Fig. 13f to a). ", "caption_bbox": [28, 936, 374, 964]}], "3140": [{"image_id": 0, "file_name": "3140_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 378], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: CerebroVis is a novel network visualization for cerebral arteries. CerebroVis uses an abstract topology-preserving visual design which is put in spatial context by enforcing constraints on the network layout. Here we show the conversion of an almost symmetrical healthy human brain cerebral artery network from a 2D isosurface visualization (left) to CerebroVis (right). Each artery has the same categorical color in both views (see Sec. 3 for a legend). ", "caption_bbox": [61, 379, 710, 432]}, {"image_id": 1, "file_name": "3140_01.png", "page": 2, "dpi": 300, "bbox": [391, 223, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Existing cerebral artery visualizations: (A) raw image \u201cslices\u201d, (B) MIPS, and (C) 3D rendering. (D) Our CerebroVis visualization. ", "caption_bbox": [404, 163, 756, 190]}, {"image_id": 2, "file_name": "3140_02.png", "page": 4, "dpi": 300, "bbox": [40, 29, 757, 384], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Summary of domain goals with accompanying abstract graph and analytical tasks.", "caption_bbox": [176, 394, 617, 408]}, {"image_id": 3, "file_name": "3140_03.png", "page": 6, "dpi": 300, "bbox": [441, 483, 710, 627], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: At each artery bifurcation CerebroVis preserves the relative spatial context of each subtree by comparing average horizontal position. ", "caption_bbox": [404, 638, 754, 665]}, {"image_id": 4, "file_name": "3140_04.png", "page": 6, "dpi": 300, "bbox": [40, 29, 757, 164], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: The visual design evolution of CerebroVis from (left) 3D rendering to (right) \ufb01nal 2D network representation.", "caption_bbox": [109, 178, 684, 192]}, {"image_id": 5, "file_name": "3140_05.png", "page": 6, "dpi": 300, "bbox": [409, 220, 751, 407], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Left: Reconstructing of the Circle of Willis (CoW) cycle of the cerebral artery network. Right: How CerebroVis abstracts the carotid artery geometry to preserve a frame of reference. ", "caption_bbox": [404, 422, 754, 462]}, {"image_id": 6, "file_name": "3140_06.png", "page": 8, "dpi": 300, "bbox": [409, 325, 737, 499], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Expert accuracy at identifying simulated stenoses using the 3D visualization (3D) and 2D network layout CerebroVis (CV). One expert (#3) did not complete the 3D visualization condition due to early departure from the interview. ", "caption_bbox": [404, 519, 754, 572]}, {"image_id": 7, "file_name": "3140_07.png", "page": 8, "dpi": 300, "bbox": [46, 323, 390, 473], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: Left: Examples of an abnormally narrow artery (\u201cstenosis\u201d) and wide artery (\u201caneurysm\u201d), with the relevant branches colored red ", "caption_bbox": [40, 483, 391, 510]}, {"image_id": 8, "file_name": "3140_08.png", "page": 8, "dpi": 300, "bbox": [39, 29, 757, 245], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: The CerebroVis Dashboard with categorical coloring to differentiate arteries. Left: A cerebral artery scan with a stenosis in the MCA . Right: Users can click on an artery mark in CerebroVis and the corresponding mark is highlighted in the 2D projection. This feature allows users to validate the stenosis with the underlying geometry and plan for therapeutic surgery. ", "caption_bbox": [40, 255, 757, 295]}], "3141": [{"image_id": 0, "file_name": "3141_00.png", "page": 1, "dpi": 300, "bbox": [90, 65, 715, 446], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Results produced by our ShapeWordle: (left) using the abstracts of VAST, InfoVis, and SciVis papers of IEEE VIS 2018 to \ufb01ll an expressive shape of \u201cVIS\u201d and (right) words of the call for papers of IEEE VIS 2019 \ufb01lled in the shape of the Canadian Maple Leaf. ", "caption_bbox": [73, 446, 721, 473]}, {"image_id": 1, "file_name": "3141_01.png", "page": 2, "dpi": 300, "bbox": [28, 29, 745, 232], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Comparing word clouds produced by different algorithms: generated by WordArt [41] by (a) using the input word frequency and by (b) further \u201ctweaking\u201d the actual word frequency to \ufb01ll the space, without respecting the data \ufb01delity; (c) generated by a traditional Wordle layout algorithm [35] but taking the shape as the boundary constraint; (d) generated by ShapeWordle, our algorithm; and (e) all words enlarged at the same rate for a dense \ufb01lling using ShapeWordle (note that such operation is not supported by WordArt). Also, note that the colors do not encode any information; we manually assign consistent colors for the words to allow more convenient comparisons across the results. ", "caption_bbox": [26, 236, 742, 303]}, {"image_id": 2, "file_name": "3141_02.png", "page": 3, "dpi": 300, "bbox": [50, 52, 757, 212], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                        Wordle algorithm adjusts the size of each word in proportion to its Fig. 3. (a) Along the spiral started from the orange dot, search for a  weight and then represents the boundary of each word using a spline- position to place the next word (blue rectangle at the white dot), such                                                                         based contour. To arrange the words in a compact and non-overlapping ", "caption_bbox": [40, 211, 755, 252]}, {"image_id": 3, "file_name": "3141_03.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 227], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Shape-aware Archimedean spirals. (a) Distance \ufb01eld created from a Christmas tree contour, with the spiral starting from the orange dot and currently stopping at the white dot; the zoomed view (b) shows the circle of curvature at the point A, where R is the local curvature radius at A and the red point marks the corresponding circle center. We approximate the movement distance from A to B (denoted as ds) by the arc length Rd\u03b7, where d\u03b7 is a user-speci\ufb01ed parameter for angular speed. (c) Also, we approximate the length of ds by another arc rd\u03b8 . (d) The distance from B to C is computed along the normal direction (in red) with length md\u03b8 (based on Eq. (4)). (e) Our generated shape-aware spiral for the Christmas tree. ", "caption_bbox": [27, 226, 743, 293]}, {"image_id": 4, "file_name": "3141_04.png", "page": 4, "dpi": 300, "bbox": [402, 311, 738, 444], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Examples of shape-aware Archimedean spirals generated on the same shape using different parameters m, d\u03b7, and number of iterations (nIter). ", "caption_bbox": [392, 447, 742, 487]}, {"image_id": 5, "file_name": "3141_05.png", "page": 5, "dpi": 300, "bbox": [404, 613, 757, 759], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Re\ufb01ning a seg- mentation by brushing. ", "caption_bbox": [641, 758, 756, 785]}, {"image_id": 6, "file_name": "3141_06.png", "page": 5, "dpi": 300, "bbox": [413, 437, 759, 541], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Illustration of the uniform constraint: (a) central forces (grey arrows) towards the virtual rigid body in the middle, where the forces are applied to surrounding word boxes in the region marked by the red dashed box; and (b) updated layout, where the position of surrounding word boxes are re-adjusted. ", "caption_bbox": [404, 544, 754, 611]}, {"image_id": 7, "file_name": "3141_07.png", "page": 5, "dpi": 300, "bbox": [413, 52, 759, 366], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Multi-centric layouts created by using multiple shape-aware Archimedean spirals: (a) tracing one spiral per component without shape segmentation; (b) the four parts segmented in the pigeon; (c) result after tracing one spiral per segmented part; and (d) result by tracing one spiral per part using the traditional Archimedean spirals instead. ", "caption_bbox": [405, 367, 755, 434]}, {"image_id": 8, "file_name": "3141_08.png", "page": 5, "dpi": 300, "bbox": [260, 132, 392, 256], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. A shape with three components. ", "caption_bbox": [277, 257, 390, 284]}, {"image_id": 9, "file_name": "3141_09.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 176], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Pipeline of our approach. (a) The input consists of a set of words with weight values and a shape; (b) the shape is segmented into different parts; (c) an initial Wordle \ufb01lled up the given shape; (d) the layout of the important words in the editing mode; (e) the result generated after manipulating individual important words; (f) the result generated by \ufb01lling the marginal words into the layout in (e) and if the further editing is required, user enters the editing model and the layout becomes the one shown in (e). ", "caption_bbox": [28, 180, 744, 233]}, {"image_id": 10, "file_name": "3141_10.png", "page": 7, "dpi": 300, "bbox": [408, 276, 751, 567], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13. Typical results produced from our ShapeWordle (left) and Wor- dArt (right) for single-centric (top) and multi-centric (bottom) input shapes. ", "caption_bbox": [405, 571, 757, 598]}, {"image_id": 11, "file_name": "3141_11.png", "page": 7, "dpi": 300, "bbox": [409, 54, 749, 206], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Boxplots summarize the scores of LC, LU, and SS of our ShapeWordle (green ones) and WordArt (orange ones). The outliers with the dark halos are out of the plot range. A larger LC value and smaller LU and SS values indicate a better result. ", "caption_bbox": [405, 211, 755, 264]}, {"image_id": 12, "file_name": "3141_12.png", "page": 7, "dpi": 300, "bbox": [42, 52, 387, 190], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Illustration: computing (a) layout uniformity using the distance \ufb01eld in input word cloud and (b) shape similarity over boundary pixel pi . ", "caption_bbox": [40, 191, 390, 219]}, {"image_id": 13, "file_name": "3141_13.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 575], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14. Results designed by the participants in our case studies. (a) the Wikipedia page of Google Chrome [40]; (b) The speech \u201cTowards a Strategy of Peace\u201d by the former U.S. President John F. Kennedy; (c) the speech \u201cWe shall \ufb01ght on the beaches\u201d by the British politician Winston Churchill; (d) the novel \u201cAlice\u2019s Adventures in Wonderland\u201d; and (e) prince, rose, and fox from the novel \u201cLittle Prince.\u201d ", "caption_bbox": [28, 574, 743, 614]}, {"image_id": 14, "file_name": "3141_14.png", "page": 9, "dpi": 300, "bbox": [419, 54, 751, 555], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15. The human life cycle from teenager to elder: (a) the three key frames created by our method; and (b) presented in Morphable word clouds [12]. Note that we assigned the same color to the same word in (a) and (b) to facilitate word-by-word comparison. ", "caption_bbox": [405, 555, 755, 608]}], "3142": [{"image_id": 0, "file_name": "3142_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 336], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: What\u2019s visual in visual comparisons, such as \ufb01nding the larger mean value? We identify mark arrangements that allow for better performance across comparison tasks. Combining previous results with the results of two new tasks fails to produce a clean ranking of arrangement effectiveness across tasks. We argue that to explain these complex patterns of performance, we \ufb01rst need a perceptual explanation of how visual comparison actually unfolds. Viewers likely perform these mathematical comparison operations with perceptual proxies. We propose and evaluate a candidate set of proxies for two visual comparison tasks. ", "caption_bbox": [73, 384, 696, 450]}, {"image_id": 1, "file_name": "3142_01.png", "page": 2, "dpi": 300, "bbox": [39, 29, 757, 337], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Visual comparison depends not on a single dimension of mark,  that visual comparisons can often be classi\ufb01ed as being made between arrangement, or task, but of the interactions between them. These     isolated parts (i.e., a bar distinct from other bars in its set) or whole sets interactions can be represented as a cube. Our present goal is not to (i.e., all the bars). Frequently, these correspond to analytic tasks for examine the full space of the cube, but rather to understand how a    which a goal is identi\ufb01cation or comparison of items, or of sets, in data. viewer uses visual features to serve analytic task goals depending on We propose that these analytic task goals correspond to proxies that the marks and arrangements they see.                                  determine the visual features that a viewer uses for visual comparison. ", "caption_bbox": [40, 336, 754, 419]}, {"image_id": 2, "file_name": "3142_02.png", "page": 4, "dpi": 300, "bbox": [39, 29, 757, 187], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: In the staircase procedure, a correct response produces a smaller difference in the subsequent trial.", "caption_bbox": [133, 227, 661, 241]}, {"image_id": 3, "file_name": "3142_03.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 273], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Means of averaged \ufb01nal titer values across participants performing the M AX M EAN and M AX R ANGE tasks. Smaller titers correspond to more precise differences between means (range widths). The precision of both the M AX M EAN and M AX R ANGE tasks was affected by chart arrangements. Also presented are titer values from previously published empirical evaluations of the precision of other comparison tasks. Note that different chart arrangements support different visual comparisons. Gray bars represent 95% con\ufb01dence intervals. ", "caption_bbox": [28, 284, 743, 339]}, {"image_id": 4, "file_name": "3142_04.png", "page": 6, "dpi": 300, "bbox": [39, 29, 758, 825], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: A set of candidate perceptual proxies that might be used in visual comparison of means and ranges (and possibly other tasks). The proxies are arranged by their correspondence with hypothesized distinctions between global and local visual scopes. ", "caption_bbox": [39, 839, 754, 867]}, {"image_id": 5, "file_name": "3142_05.png", "page": 8, "dpi": 300, "bbox": [39, 29, 757, 498], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Results of the two analyses of visual feature performance, split by task: M AX M EAN and M AX R ANGE. The x-axis is the percentage of trials for which the visual proxy was predictive, for human behavior (vertical bars), and for true answer for the comparison (colored dots). The small dots show individual subjects, and light gray around the black lines shows 95% con\ufb01dence interval. True answer dots are color-coded to show whether we informally coded them as a global proxy feature (blue) or focal proxy feature (orange). The true answer dots indicates that some features are more useful than others for a given visual comparison. ", "caption_bbox": [39, 517, 754, 586]}], "3143": [{"image_id": 0, "file_name": "3143_00.png", "page": 1, "dpi": 300, "bbox": [408, 694, 725, 762], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. A sequence of three trees, visualized using classical node- link drawings (left) and then stacked as three BCTw rows. By aligning matching nodes, the differences in the structures become apparent. ", "caption_bbox": [392, 773, 744, 813]}, {"image_id": 1, "file_name": "3143_01.png", "page": 1, "dpi": 300, "bbox": [391, 29, 745, 593], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. BarcodeTrees (BCTs) linearize hierarchical data and map the nodes to rectangles. The BCTw encodes the depth of each node with the Width of each rectangle, leaving height and color available for en- coding node attributes. The BCTh encodes depth with the Height of the rectangles, leaving color available for encoding node attribute values. ", "caption_bbox": [392, 606, 744, 673]}, {"image_id": 2, "file_name": "3143_02.png", "page": 3, "dpi": 300, "bbox": [391, 285, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. \u201cStructural cues\u201d highlight certain nodes in response to the cursor hovering over a node, to make topological relationships more apparent. Horizontal underlining appears under ancestors to the left of the cursor, and also under descendants to the right of the cursor. Vertical tick marks appear under siblings at the same level as the node under the cursor. Both marks appear under the node under the cursor, resulting in a cross- shaped glyph. Above we see two variants of these structural cues, with an ellipse with dotted border indicating their differences: (a) Descendants are shown with a single horizontal stroke; (b) Each subtree is shown with a separate stroke. ", "caption_bbox": [392, 144, 744, 276]}, {"image_id": 3, "file_name": "3143_03.png", "page": 4, "dpi": 300, "bbox": [39, 57, 758, 138], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Diagonal stripe glyphs. (a) Selecting the root nodes of two        in the union BCT. The union BCT is composed of BCTs as a kind subtrees of interest; (b) Replacing each contiguous uninteresting region   of superset of the nodes. Constructing the union BCT is a top-down with diagonal stripe glyphs. The density of the diagonal stripe texture in recursive process. Each iteration merges all matched children (and ", "caption_bbox": [39, 137, 754, 181]}, {"image_id": 4, "file_name": "3143_04.png", "page": 4, "dpi": 300, "bbox": [48, 215, 382, 284], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Collapse/Expand subtrees. The \ufb01gure above shows the process that users re\ufb01ne their focus of interest. Darker icons correspond to branches with more nodes. Taller icons correspond to deeper branches, and wider icons correspond to a higher average branching factor. As users change the focus of the layout (i.e., click on the node under the cursor in (a)), more details in (b) are revealed. (c) Changing the focus of layout to a deeper branch by clicking on the node under the cursor in (b). ", "caption_bbox": [39, 298, 391, 391]}, {"image_id": 5, "file_name": "3143_05.png", "page": 5, "dpi": 300, "bbox": [411, 251, 724, 350], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Competing techniques for experimental evaluation.", "caption_bbox": [392, 368, 677, 382]}, {"image_id": 6, "file_name": "3143_06.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 184], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Node alignment for comparison. (a) Juxtapose the original BCTs vertically; (b) Align the nodes in the subtrees of interest; (c) Align the nodes to the second level. The black line at the bottom of the BCTs indicates the aligned parts, and the gray dotted line indicates the unaligned parts; ", "caption_bbox": [28, 196, 743, 223]}, {"image_id": 7, "file_name": "3143_07.png", "page": 6, "dpi": 300, "bbox": [435, 52, 724, 302], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Example tasks in the controlled experiment. Nodes highlighted at the start of the trial are red, and cursors above show where the user had to click. (Top) Task1: Click the second child of the second child of the highlighted node. (Middle) Task2: Click the nearest common ancestor of the highlighted nodes. (Bottom) Task3: Click the tree most similar to the reference tree (tree-0) indicated by the box. ", "caption_bbox": [404, 318, 754, 398]}, {"image_id": 8, "file_name": "3143_08.png", "page": 7, "dpi": 300, "bbox": [28, 29, 746, 170], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Experiment results are broken down by the techniques and tasks.   ICICLE for Task1 and Task2. H2: the BCT techniques require signi\ufb01- Vertical error bars indicate standard errors. Horizontal red line segments cantly less time than ICICLE for Task3 and Task4. H3: the BCT tech- indicate pairs that are signi\ufb01cantly different (p < 0.05).                 niques yield error rates not signi\ufb01cantly worse than ICICLE for Task3 ", "caption_bbox": [27, 169, 744, 212]}, {"image_id": 9, "file_name": "3143_09.png", "page": 8, "dpi": 300, "bbox": [442, 474, 717, 683], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Changes in books borrowed over time. Within the overview histogram along the top, the user has selected intervals (a) and (b), which are similar periods in different semesters. The two intervals are visualized below (in the left and right panes, respectively) of the main view. Both panes focus on the Language subtree, replacing other sub- trees with diagonal stripe textures. Matching nodes are aligned. Three categories under Language are labelled c1 , c2 , and c3 . In both intervals, c2 (English and Old English) shows similar continuous activity. In interval (a) and interval (b), c1 (Linguistics) exhibits different periodic patterns. Speci\ufb01cally, students borrowing book nearly every weekday in interval (a) and on every Monday in interval (b). ", "caption_bbox": [404, 696, 756, 841]}, {"image_id": 10, "file_name": "3143_10.png", "page": 8, "dpi": 300, "bbox": [128, 56, 645, 343], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. The histogram along the top shows the number of borrowed books over the entire two years. The user \ufb01rstly selects interval (a), yielding the view in the main view (shown as a background layer in the \ufb01gure) of just over 100 days. In other words, just over 100 trees (with hundreds of nodes per tree) are visible simultaneously. The different columns correspond to categories of books, including columns (as , at ), where subscripts indicate the category: s for Social Science, t for Technology. Later, the user selects intervals (b) and (c), resulting in the data shown in columns (bs , bt , cs , ct ), shown in the \ufb01gure as cropped layers superimposed on the original main view for comparison. Throughout all these views, the user has set the ratio \ufb01lter in (d) to the range 0.35 to 0.95 to show only the categories that account for the most commonly borrowed books, and not including the root whose ratio is 1.0. In interval (b), the activity in Technology (bt ) is clearly different. ", "caption_bbox": [39, 356, 755, 450]}], "3144": [{"image_id": 0, "file_name": "3144_00.png", "page": 3, "dpi": 300, "bbox": [80, 57, 703, 272], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: 2x12-hours and 24-hours radial charts showing the number of traf\ufb01c accidents in Staten Island, New York, in March.", "caption_bbox": [87, 285, 707, 299]}, {"image_id": 1, "file_name": "3144_01.png", "page": 3, "dpi": 300, "bbox": [444, 329, 715, 488], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: 2x12-hours linear bar charts (12l).", "caption_bbox": [475, 498, 684, 512]}, {"image_id": 2, "file_name": "3144_02.png", "page": 3, "dpi": 300, "bbox": [444, 541, 719, 612], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: 24-hours linear bar charts (24l).", "caption_bbox": [482, 625, 678, 639]}, {"image_id": 3, "file_name": "3144_03.png", "page": 5, "dpi": 300, "bbox": [404, 57, 758, 679], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Mean number of observations with 95% con\ufb01dence intervals in task 1 for radial (R) and linear (L). ", "caption_bbox": [405, 698, 755, 726]}, {"image_id": 4, "file_name": "3144_04.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 190], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: Percentage of correct responses in tasks 2 \u2014 5 for radial (R) and linear (L). ", "caption_bbox": [392, 206, 743, 234]}, {"image_id": 5, "file_name": "3144_05.png", "page": 7, "dpi": 300, "bbox": [404, 281, 757, 507], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: Mean ratings issued on a \ufb01ve-point Likert scale with 95% con\ufb01dence intervals. ", "caption_bbox": [405, 525, 756, 553]}, {"image_id": 6, "file_name": "3144_06.png", "page": 7, "dpi": 300, "bbox": [75, 277, 357, 411], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Example for incorrectly read value with 12r (correct value is 25, user input was 7), which could be caused by an assumed grid line interval of \ufb01ve or by reading the value of the bar at 12-1 a.m. ", "caption_bbox": [40, 422, 390, 463]}, {"image_id": 7, "file_name": "3144_07.png", "page": 7, "dpi": 300, "bbox": [87, 56, 707, 206], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Mean completion times with 95% con\ufb01dence intervals in task 2 (locate time), task 3 (read value), task 4 (locate maximum), and task 5 (compare a.m./p.m. interval) for radial (R) and linear (L). ", "caption_bbox": [40, 223, 755, 251]}], "3145": [{"image_id": 0, "file_name": "3145_00.png", "page": 1, "dpi": 300, "bbox": [391, 65, 738, 514], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. A conceptual sketch of post-hoc model diagnostics. Scien- tists need to diagnose which models have high or low \ufb01delity. Fidelity is de\ufb01ned by statistical metrics that score model outputs based on how closely they match observation data. ", "caption_bbox": [405, 516, 757, 569]}, {"image_id": 1, "file_name": "3145_01.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 461], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. A classi\ufb01cation scheme for deriving comparative visual cues that address the tasks (T1, T2, T3, T4, T5, T6) for climate model \ufb01delity analysis. The visual cues, by leveraging the perceptual principles of visual encoding, help minimize comparison complexity by letting scientists readily spot patterns of disagreement and stability across many combinations of models, metrics, and output variables ", "caption_bbox": [28, 460, 743, 500]}, {"image_id": 2, "file_name": "3145_02.png", "page": 5, "dpi": 300, "bbox": [404, 52, 757, 284], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Con\ufb01gurable magnitude plots. Experts can either choose quartiles or rank based bins for understanding the ordering of models computed by their weighted average of \ufb01delity scores. The dot plots complement the bar charts by providing \ufb02exibility to select models from any range and also the ability to see changes among multiple models readily when weights are adjusted. Scattered dots provide an immediate cue about high sensitivity of models to the weight changes. ", "caption_bbox": [405, 287, 755, 380]}, {"image_id": 3, "file_name": "3145_03.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 508], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. MyriadCues comprises: a) a set of controls for con\ufb01guring different visualizations by selecting axes, data normalization and ranking strategies, and adjusting the parameters for different views; b) a set of \ufb01lters for subsetting across different elements; c) Magnitude Plots for showing magnitude differences across models; d) Slope Plots for showing dissimilarities across models and variables; and e) legends and guides for navigating the visualizations. In this view, bayes f actor is the reference metric, which means that models are color-coded based on their ranks with respect to bayes f actor. These colors are used to link models in other small multiples, where their respective ranks are shown. ", "caption_bbox": [28, 509, 743, 576]}, {"image_id": 4, "file_name": "3145_04.png", "page": 7, "dpi": 300, "bbox": [405, 52, 757, 361], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Case study for assessing consistency among \ufb01delity met- rics. The different stages include: understanding what causes disagree- ment among models (a), and inspecting cases where metrics disagree about the \ufb01delity levels across multiple output variables (b, c). ", "caption_bbox": [405, 364, 757, 417]}, {"image_id": 5, "file_name": "3145_05.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 272], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Case study for assessing robustness of model rankings. The different stages include: spotting of model outputs with high variabil- ity (a) (T3) that resulted in the adjustment of variable weights (b), followed by comparison across multiple metrics to look at disagreement about \ufb01delity values across different variables (c,d,e,f) (T4, T6). ", "caption_bbox": [28, 274, 744, 314]}], "3146": [{"image_id": 0, "file_name": "3146_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 350], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Design Study timeline (log scale). The top contains a mark for each collected artifact. Connections to identi\ufb01ed goals, sub-goals, and tasks are marked when direct evidence for them has been identi\ufb01ed. Artifacts from meetings presenting major design changes and notes from the evaluation sessions of Section 7.2 are indicated with color. The bottom shows the timing of various deployments with users. This rich collection of over 150 artifacts mitigated issues in designing around shifting data and concerns. ", "caption_bbox": [61, 370, 711, 423]}, {"image_id": 1, "file_name": "3146_01.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 262], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. A goal-to-task lattice, showing the relationships between high-level umbrella concerns (U1 - U3); more speci\ufb01c goals (G1 - G6) and sub-goals (S1 - S6); and low-level tasks (T1 - T6) that directly inform the design of a visualization interface. The comparison sub-goal and task were added as data and concerns evolved. G5 was identi\ufb01ed as a future goal based on project priorities in collecting and analyzing the data. ", "caption_bbox": [27, 279, 742, 319]}, {"image_id": 2, "file_name": "3146_02.png", "page": 6, "dpi": 300, "bbox": [299, 527, 394, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. The expression tree of transx \u00b7 (pred \u2212 y \u2212 x).", "caption_bbox": [40, 621, 300, 637]}, {"image_id": 3, "file_name": "3146_03.png", "page": 6, "dpi": 300, "bbox": [39, 52, 757, 424], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The design of Atria. The main view represents the expression tree contained in the execution graph. (A) Triangles represent collapsed subtrees. (B) Elided links are shown on hover. (C) Fill color and border style encode time and execution mode respectively. (D) Users can toggle between showing inclusive and exclusive time. (E) Tooltips provide details on hover. (F) Code view with linked line of code highlighting. (G) Primitives listed by execution time. (H) If multiple runs are available, comparative mode may be enabled. ", "caption_bbox": [40, 439, 755, 492]}, {"image_id": 4, "file_name": "3146_04.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 213], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Comparison between two runs of the same application with different policies. Pink-outlined nodes indicate a difference in execution mode between two runs. The orange node ran slower after the policy change, but the net affect on the parents was positive. ", "caption_bbox": [392, 226, 742, 279]}], "3147": [{"image_id": 0, "file_name": "3147_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 370], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: OntoPlot interface. Classes with associations are highlighted in colour based on the key on the right-hand side. A panel on the left (not shown) selects different types of non-hierarchy associations found within the ontology (denoted by the OWL object properties involved). The panel on the right provides search and displays additional information for selected classes (not shown). ", "caption_bbox": [61, 383, 709, 424]}, {"image_id": 1, "file_name": "3147_01.png", "page": 5, "dpi": 300, "bbox": [27, 388, 380, 703], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Examples of OntoPlot visual compression.", "caption_bbox": [77, 715, 328, 729]}, {"image_id": 2, "file_name": "3147_02.png", "page": 6, "dpi": 300, "bbox": [456, 674, 704, 880], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: The manual approach used by domain experts to show associa- tions within an ontology: association numbers written next to branches of the hierarchy on a screenshot of Prote\u0301ge\u0301 indented list view (from [25], used with permission). ", "caption_bbox": [404, 890, 756, 945]}, {"image_id": 3, "file_name": "3147_03.png", "page": 6, "dpi": 300, "bbox": [38, 362, 393, 842], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: OntoPlot showing the same ontology as in Figure 1 but after the user has selected focus mode for class \u2018AE severity G2\u2019 to concentrate on the associations only for this class. ", "caption_bbox": [40, 852, 390, 893]}, {"image_id": 4, "file_name": "3147_04.png", "page": 9, "dpi": 300, "bbox": [27, 29, 393, 449], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Participants\u2019 rating of the two tools in the expert user study.", "caption_bbox": [31, 461, 373, 475]}], "3148": [{"image_id": 0, "file_name": "3148_00.png", "page": 3, "dpi": 300, "bbox": [67, 579, 759, 864], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Conventional pipeline (a) versus progressive pipeline (b) for data visualization. The convention pipeline requires users to wait for analysis                                                                            3.3   Application Programming Interface and visualization operations to be completed on the whole dataset before   P5 provides an intuitive API for specifying and controlling the progres- interaction, while the progressive pipeline processes data in chunks       sive work\ufb02ow. The syntax and high-level operations of P5\u2019s API are and provides incrementally re\ufb01ning visualization of results at interactive shown in the left of Fig. 2. Following the convention used in P4\u2019s speed.                                                                     syntax, progressive operations are speci\ufb01ed as a pipeline in P5. The ", "caption_bbox": [40, 868, 757, 948]}, {"image_id": 1, "file_name": "3148_01.png", "page": 3, "dpi": 300, "bbox": [404, 57, 758, 537], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. System architecture and components of P5 for enabling progres- sive parallel processing. ", "caption_bbox": [405, 551, 757, 578]}, {"image_id": 2, "file_name": "3148_02.png", "page": 4, "dpi": 300, "bbox": [401, 156, 727, 256], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. P5\u2019s extended syntax of data aggregation for allowing multiple attributes and measures to be included in the results. ", "caption_bbox": [392, 269, 742, 296]}, {"image_id": 3, "file_name": "3148_03.png", "page": 4, "dpi": 300, "bbox": [393, 837, 743, 975], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. P5\u2019s visualizations for supporting progressive data analysis and visualization with different types of data. ", "caption_bbox": [392, 990, 742, 1017]}, {"image_id": 4, "file_name": "3148_04.png", "page": 5, "dpi": 300, "bbox": [412, 55, 755, 248], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Interaction speci\ufb01cation (a) for connecting the visualization views (b) based on the selection of the user. P5 automatically generates the data cubes (c) for supporting interactive visualization during progressive processing. Summed-Area Table (d) can also be computed from the data cubes to support interactive performance for higher resolutions. ", "caption_bbox": [405, 265, 755, 332]}, {"image_id": 5, "file_name": "3148_05.png", "page": 5, "dpi": 300, "bbox": [42, 54, 387, 214], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Declarative speci\ufb01cation for visualizing multivariate time-series data in faceted views showing the top 3 user-de\ufb01ned variables ordered by their variances. ", "caption_bbox": [40, 231, 390, 271]}, {"image_id": 6, "file_name": "3148_06.png", "page": 6, "dpi": 300, "bbox": [27, 29, 745, 305], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. A progressive analytic application implemented by P5 for analyzing large multivariate time-series data using faceted views and interactive visualizations. P5 allows custom callback functions to be de\ufb01ned in the user interaction for exporting the results of progressive processing to create customized visualizations, such as the circular chart on the right. ", "caption_bbox": [27, 319, 742, 359]}, {"image_id": 7, "file_name": "3148_07.png", "page": 7, "dpi": 300, "bbox": [404, 376, 757, 561], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. P5 Play allows users to progressively analyze multidimensional data using declarative visualization grammars. P5 Play\u2019s user interface consists of a side panel (a) for selecting data \ufb01les and listing the data attributes, a control panel (b) for starting and pausing progressive op- erations, a text editor (c) for editing declarative speci\ufb01cations, and a dashboard (d) for displaying the output visualizations. ", "caption_bbox": [405, 588, 757, 668]}, {"image_id": 8, "file_name": "3148_08.png", "page": 7, "dpi": 300, "bbox": [41, 53, 756, 296], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. We use P5 to reproduce the dashboard used by imMens for visual exploration of the Brightkite dataset. The speci\ufb01cations of the data and view (a), batch operations (b), visualization of the results (c) in P5\u2019s declarative grammar are used to generate a set of coordinated and linked views (d) with the brushing-and-linking interaction speci\ufb01ed in (e). ", "caption_bbox": [40, 311, 755, 351]}, {"image_id": 9, "file_name": "3148_09.png", "page": 8, "dpi": 300, "bbox": [424, 304, 718, 451], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13. Average frame rate for interactive visual querying of the Brightkite dataset with P5 and imMens. ", "caption_bbox": [392, 467, 742, 494]}], "3149": [{"image_id": 0, "file_name": "3149_00.png", "page": 1, "dpi": 300, "bbox": [53, 65, 744, 421], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 1. RSATree facilitates fast answering of aggregate queries in large-scale tabular datasets while allowing \ufb02exible binning strategies. (a) A case built on a social network check-in dataset with 4.5 million records. A brushing and linking operation is performed by brushing workdays (Monday \u2013 Friday) and 13 hours of each day (9am \u2013 9pm) for \ufb01ltering. (b) Binned scatterplot created from an airline on-time performance dataset with 180 million records. The bin width can be freely modi\ufb01ed and instant previews are shown. (c) Capability of RSATree to generate a histogram with varied bin widths by using the same dataset as (b). As shown on the left side of (c), the distribution of \u201cLateAircraftDelay\u201d is relatively unbalanced. Application of log-scale binning produces a recognizable histogram (right side). Conventional approaches cannot simultaneously achieve low response time and \ufb02exible binning strategy in the three cases. Abstract\u2014 Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating prede\ufb01ned data cubes. However, the queries are limited to the prede\ufb01ned binning schema of preprocessed data cubes. Such limitation hinders analysts\u2019 \ufb02exible adjustment of visual speci\ufb01cations to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and \ufb02exible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual speci\ufb01cation, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the ef\ufb01ciency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity. ", "caption_bbox": [73, 422, 724, 665]}, {"image_id": 1, "file_name": "3149_01.png", "page": 5, "dpi": 300, "bbox": [71, 56, 720, 268], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 3. Overview of RSATree operation. The construction of RSATree consists of three consecutive stages, namely, (a) partitioning of the data space into subspaces on the basis of data distribution using R-tree; (b) computation of IHs as the approximation of the original data for each subspace; (c) and storing and indexing of IHs by LSH, thereby preserving the spatial coherence of subspaces. (d) Possible subspaces that intersect with the speci\ufb01ed range are fetched from LSH buckets when RSATree is used to execute an aggregate query. After validation, involved IHs are merged and used to estimate the actual distribution of the requested values. ", "caption_bbox": [40, 267, 755, 334]}, {"image_id": 2, "file_name": "3149_02.png", "page": 5, "dpi": 300, "bbox": [63, 497, 758, 586], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 4. (a) By using an SAT, the sum of values inside area can be                                                                          thereby making it relatively suitable for large-scale datasets. However, calculated as D+A\u2212B\u2212C = (x+y+z+{)+x\u2212(x+y)\u2212(x+z) =                        new coming points that cannot \ufb01t in any existing subspace of the staged {. (b) Misaligned computational grids and SAT cells. (c) Errors occured  partition may exist during the formulation of a complete partition. The due to the mismatching of computational grids (blue rectangles) and data resulting expansion of the existing subspaces may cause the original cells (the red region).                                                  partition to be distorted to some extent. These IHs cover a large space ", "caption_bbox": [40, 585, 756, 657]}, {"image_id": 3, "file_name": "3149_03.png", "page": 7, "dpi": 300, "bbox": [422, 878, 740, 994], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 7. Comparisons of (a) construction time and (b) storage consump- tion to Nanocubes. The x-axis is ordered by the record number of each dataset. ", "caption_bbox": [405, 993, 757, 1033]}, {"image_id": 4, "file_name": "3149_04.png", "page": 7, "dpi": 300, "bbox": [404, 588, 758, 828], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 6. The growth of storage consumption and construction time when inserting records into RSATree for (a) Urban-POI, (b) Brightkite, and (c) Flight datasets. ", "caption_bbox": [405, 830, 755, 870]}, {"image_id": 5, "file_name": "3149_05.png", "page": 7, "dpi": 300, "bbox": [404, 57, 758, 526], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 5. Number of records and (a and d) storage consumption, (b and e) construction time, and (c and f) response time when creating an RSATree using different SPLOM datasets. The dashed lines show the number of points sampled for the progressive construction. ", "caption_bbox": [405, 527, 755, 580]}, {"image_id": 6, "file_name": "3149_06.png", "page": 9, "dpi": 300, "bbox": [76, 52, 721, 307], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 8. Applying RSATree in querying a heatmap of Urban-POI distribution in three regions. From left to right: Global region, Region 1 with high density, and Region 2 with low density. (a) Map view, (b) storage consumption, (c) Response time, and (d) ARE rates when using R-tree with different heights and LSH, as well as two raw SATs. The scale alignment scheme is used in (a-d). (e) ARE and (f) coincident rates of querying in an RSATree with and without the scale alignment scheme. ", "caption_bbox": [40, 306, 755, 359]}], "3150": [{"image_id": 0, "file_name": "3150_00.png", "page": 1, "dpi": 300, "bbox": [81, 65, 715, 373], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Our approach uses design mining and unsupervised clustering techniques to produce automatically generated color ramps that capture designer practices. The four choropleth maps shown above utilize color ramps generated from our approach. Developers select a single guiding seed color, shown in the squares below each map, to generate a ramp. We then \ufb01t curves capturing structural patterns in designer practices in CIELAB (bottom) to these seed colors to generate ramps (middle, seed colors indicated by a black dot). We embody this technique in Color Crafter, a web-based tool that enables designers of all ability levels to generate high-quality custom color ramps. ", "caption_bbox": [73, 396, 722, 476]}, {"image_id": 1, "file_name": "3150_01.png", "page": 3, "dpi": 300, "bbox": [53, 69, 746, 187], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. An overview of our algorithm for generating effective color ramps. (1) We collected a corpus of designer-crafted color ramps. (2) We then \ufb01t an interpolating cubic B-spline curve to the colors in each ramp in CIELAB, resample each curve to a uniform number of control point colors, and (3) cluster them based on structural patterns in the curves. (4) We then construct a representative model curve for each cluster and (5) use an input seed color (either given by a user or programmatically selected) to anchor the model curve in color space. ", "caption_bbox": [40, 215, 755, 268]}, {"image_id": 2, "file_name": "3150_02.png", "page": 5, "dpi": 300, "bbox": [432, 52, 739, 174], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. We construct diverging color ramps by appending two sequential model curves together, translating the joining color to a neutral value, and rotating the sequential arms of the color ramp within the range of angles represented in our designer-crafted diverging ramps from our corpus. An example diverging ramp crafted using this method is shown above along with the associated curve. ", "caption_bbox": [405, 189, 756, 269]}, {"image_id": 3, "file_name": "3150_03.png", "page": 5, "dpi": 300, "bbox": [421, 290, 740, 630], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. We embody our approach in Color Crafter, a web-based tool for ramp generation and editing. (a) Users can specify target colors and models and (b) to tune these models using af\ufb01ne transforms applied through sliders. These transforms enable users to rapidly re\ufb01ne and explore different encodings while retaining desirable structural properties. ", "caption_bbox": [405, 642, 757, 709]}, {"image_id": 4, "file_name": "3150_04.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 191], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Examples of the study interface used in our empirical evaluation showing each visualization type: scatterplot (left), heatmap (middle), choropleth map (right). Participants clicked on the mark they felt matched a target value and reported how pleasant they found the visualization to be. ", "caption_bbox": [28, 204, 745, 231]}, {"image_id": 5, "file_name": "3150_05.png", "page": 7, "dpi": 300, "bbox": [391, 333, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Mean aesthetic rating for each color ramp type. Error bars indicate 95% con\ufb01dence intervals. Our automatically generated ramps had slightly higher aesthetic ratings than designer-crafted color ramps (though the difference was not signi\ufb01cant) and signi\ufb01cantly outperformed color ramps generated using linear interpolation. Signi\ufb01cant differences are denoted with asterisks (*: p < .05, **: p < .01, ***: p < .001) ", "caption_bbox": [405, 223, 755, 304]}, {"image_id": 6, "file_name": "3150_06.png", "page": 7, "dpi": 300, "bbox": [64, 57, 734, 204], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Mean error (\u2223target value \u2212 selected value\u2223) for each color ramp Fig. 9. type. Error bars indicate 95% con\ufb01dence intervals. We found that our     indicate participants were signi\ufb01cantly more accurate with our ramps than with    had slig linear ramps and slightly more accurate than with designer ramps, though (though the difference was not signi\ufb01cant. Signi\ufb01cant differences are denoted    color ra with asterisks (*: p < .05, **: p < .01, ***: p < .001).                 are den ", "caption_bbox": [40, 223, 444, 305]}, {"image_id": 7, "file_name": "3150_07.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 236], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Color ramps generated using \u201cugly colors\u201d from the ColourLovers designer community. Seed colors used are shown above and highlighted with a black dot within the ramp. Despite the negative aesthetic of the individual colors, our method generates reasonable encodings even using default structures, suggesting the robustness of this approach. ", "caption_bbox": [392, 249, 742, 316]}], "3151": [{"image_id": 0, "file_name": "3151_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 394], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 1. We constructed models that estimate human color-concept associations using color distributions extracted from images of relevant concepts. We compared methods for extracting color distributions by de\ufb01ning different kinds of color tolerance regions (white outlines) around each target color (regularly spaced large dots) in CIELAB space. Subplots show a planar view of CIELAB space at L* = 50, with color tolerance regions de\ufb01ned as balls (left column; radius \u0394r ), cylindrical sectors (middle columns; radius \u0394r and hue angle \u0394h), and category boundaries around each target color (right column; Red, Orange, Yellow, Green, Blue, Purple, Pink, Brown, Gray; white and black not shown). Each target color is counted as \u201cpresent\u201d in the image each time any color in its tolerance region is observed. This has a smoothing effect, which enables the inclusion of colors that are not present in the image but similar to colors that are. A model that includes two sector features and a category feature best approximated human color-concept associations for unseen concepts and images (see text for details). ", "caption_bbox": [61, 399, 711, 518]}, {"image_id": 1, "file_name": "3151_01.png", "page": 4, "dpi": 300, "bbox": [65, 52, 728, 288], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 3. Illustration of our pipeline for automatically extracting color distributions from images. The bottom \ufb02ow (concepts to color ratings to human associations) describes the slow yet reliable direct approach using human experiments to determine ground-truth associations. The top \ufb02ow involves querying Google Images, extracting colors using a variety of different methods (features), then weighting those features appropriately to obtain estimated associations. Deciding which features to use and how to weight them is learned from human association data using sparse regression and cross-validation. Once the model is trained, color-concept associations can be quickly estimated for new concepts without additional human data. ", "caption_bbox": [40, 295, 755, 362]}, {"image_id": 2, "file_name": "3151_02.png", "page": 4, "dpi": 300, "bbox": [82, 376, 758, 597], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                     The concept is queried in Google Images, the k chosen features are Figure 4. UW-58 colors used in Experiment 1 and 2, plotted in CIELAB extracted from the images for the desired colors, and the trained model color space. Exact coordinates are given in Supplementary Table S.1. weights are applied to the features to obtain the estimates. ", "caption_bbox": [40, 596, 755, 632]}, {"image_id": 3, "file_name": "3151_03.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 190], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1. Top 4 features selected using sparse regression as more fea- tures were made available in Experiments 1A to 1C. Ball features were not selected when sector or category features became available. ", "caption_bbox": [392, 371, 744, 411]}, {"image_id": 4, "file_name": "3151_04.png", "page": 6, "dpi": 300, "bbox": [421, 317, 732, 655], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 7. Scatter plots showing relationships between model estimates and human ratings for lemon, lime, and blueberry using models from Experiments 1A\u20131C. Marks represent each of the UW-58 colors, dashed line represent best-\ufb01t regression lines, and r values indicate correla- tions within each plot. Adding more perceptually relevant (sector) and cognitively relevant (category) features improved \ufb01t for fruits where ball features performed poorly. ", "caption_bbox": [404, 660, 756, 753]}, {"image_id": 5, "file_name": "3151_05.png", "page": 6, "dpi": 300, "bbox": [40, 52, 753, 258], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 6. Correlations between model estimations and human ratings across each of the UW-58 colors for each fruit from the best 4-feature model in Experiment 1A (Ball model), Experiment 1B (Sector model), and Experiment 1C (Sector+Category model). The Sector+Category model performed best, followed by the Sector model, then the Ball model, see Table 2 and text for statistics. ", "caption_bbox": [40, 263, 755, 303]}, {"image_id": 6, "file_name": "3151_06.png", "page": 7, "dpi": 300, "bbox": [418, 759, 717, 919], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 8. Correlations for top 50 images, photo images, and cartoon images using the Ball, Sector, and Sector+Category models. The Sec- tor+Category model performed best and was similar across all image types. The Ball model was worst for top 50 and photo images, but less poor for cartoon images. Estimates based on Lin et al. [23] were strongly correlated with human ratings, but less so than the Sector+Category model (see text for statistics and explanation). ", "caption_bbox": [392, 924, 744, 1017]}, {"image_id": 7, "file_name": "3151_07.png", "page": 8, "dpi": 300, "bbox": [430, 53, 728, 208], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 9. Correlations between human ratings and estimated ratings across all colors for each recycling-related concept using the Sec- tor+Category model. The range of correlations is similar to fruits (Fig. 6.) ", "caption_bbox": [404, 210, 756, 250]}], "3152": [{"image_id": 0, "file_name": "3152_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 369], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Our search interface lets users query for D3 visualizations based on the data, the marks and the encodings that describe how visual attributes of the marks represent the data. Our query syntax is based on the Vega-Lite chart speci\ufb01cation grammar [40]. Here, the user searches for visualizations that use circles as data-encoding marks and contain at least three encodings. The query speci\ufb01es that two of the encodings must map quantitative data \ufb01elds to the x-position and the y-position of the circle marks respectively, while a third encoding must map a nominal data \ufb01eld to the \ufb01ll-color of the marks. The query results include a variety of scatterplot and bubble chart styles that can help developers explore the design space of such visualizations. ", "caption_bbox": [60, 373, 710, 453]}, {"image_id": 1, "file_name": "3152_01.png", "page": 2, "dpi": 300, "bbox": [40, 52, 754, 158], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Our visualization search engine is comprised of three main components (green boxes): a visualization crawler, a visualization deconstructor and a search interface. The crawler \ufb01nds webpages containing D3 charts, the deconstructor extracts the visual style and structure of the charts (e.g. the data, the marks and the encodings) and the search interface lets users query for charts that include speci\ufb01c aspects of style and structure. ", "caption_bbox": [40, 158, 757, 198]}, {"image_id": 2, "file_name": "3152_02.png", "page": 3, "dpi": 300, "bbox": [28, 29, 745, 353], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. A D3 bar chart (a). The data, marks, encodings, axes and non-data-encoding attributes of marks as extracted by our deconstructor (b). In this case, our deconstructor extracts two data \ufb01elds named \u201crace\u201d and \u201cpercent\u201d of data types \u201cnominal\u201d and \u201cquantitative\u201d respectively. It also extracts data-encoding marks of type \u201crect\u201d and a set of 4 encodings that map data \ufb01elds to visual attributes of the rect marks (e.g., the data \ufb01eld \u201crace\u201d is mapped to the \ufb01ll-color of \u201crect\u201d). Finally, it extracts the axes, including the data \ufb01eld they represent and their orientation and non-data-encoding attributes of the marks (e.g. typeface, background color). A Vega-Lite speci\ufb01cation for the bar chart is similar to our deconstructed representation and explicitly describes the relevant encodings (c). ", "caption_bbox": [28, 357, 743, 437]}, {"image_id": 3, "file_name": "3152_03.png", "page": 4, "dpi": 300, "bbox": [40, 45, 756, 296], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Query for charts containing one quantitative data \ufb01eld using the y-position and another data \ufb01eld of either quantitative or nominal data type using the x-position of the bars. The search results are presented in list view showing the data table and encodings between data \ufb01elds and mark attributes. The URL for the webpage containing the chart is given above the data table and a thumbnail shows the visual form of the chart and this query returns a set of vertical bar charts. As in Harper and Agrawala [26, 27] each encoding is represented as d\ufb01eld \u2192 mattr and the background color indicates the data type: green for nominal data and purple for quantitative data. Users can click on \u2018Show more\u2019 button to expand the view so that all the data tables and encodings are visible. ", "caption_bbox": [40, 304, 755, 384]}, {"image_id": 4, "file_name": "3152_04.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 307], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Query for charts containing at least one encoding of nominal data \ufb01elds and at least two encodings of quantitative data \ufb01elds. Our search engine returns stacked and grouped bar charts, bubble charts and other variations with the desired encodings. ", "caption_bbox": [28, 311, 743, 338]}, {"image_id": 5, "file_name": "3152_05.png", "page": 6, "dpi": 300, "bbox": [40, 381, 756, 648], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Query for visualizations that contain at least 1500 data points of a quantitative data \ufb01eld encoded using some visual attribute of circle marks. The results include scatterplots, bubble charts and other more exotic chart types. ", "caption_bbox": [39, 654, 756, 681]}, {"image_id": 6, "file_name": "3152_06.png", "page": 6, "dpi": 300, "bbox": [40, 52, 756, 334], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Query for charts containing a quantitative data \ufb01eld with both negative and positive data values mapped to the x- or y-position attribute of bar marks. Our search engine mostly returns bar charts with bars extending in both directions from a baseline at zero. The user could \ufb01lter out this result by for example including another condition in the query checking for axes. ", "caption_bbox": [40, 338, 755, 378]}, {"image_id": 7, "file_name": "3152_07.png", "page": 7, "dpi": 300, "bbox": [28, 380, 744, 531], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Query for charts containing dark backgrounds similar in color to \u201cslategray\u201d and that encode data using the \ufb01ll-color of foreground marks. The results include a variety of color palettes for the foreground marks. ", "caption_bbox": [28, 532, 743, 559]}, {"image_id": 8, "file_name": "3152_08.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 324], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. In query by example users select a chart returned by an earlier query (shown at top of query textbox) and our search engine \ufb01nds charts with similar encodings. Here, the example query is a vertical bar chart that contains an additional encoding that maps a nominal data \ufb01eld to the \ufb01ll-color of the bars. Our search engine automatically converts the example into our query syntax keeping only the encodings (shown at bottom of query textbox). The query returns a variety of vertical bar charts that match as many of the encodings as possible. ", "caption_bbox": [28, 324, 743, 377]}, {"image_id": 9, "file_name": "3152_09.png", "page": 8, "dpi": 300, "bbox": [45, 48, 745, 212], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Number of charts using each mark type for encoding data (a). The percentages are with respect to the total number of charts in each collection (7860 whole collection, 457 New York Times subcollection). Note that some charts contain multiple data encoding marks while others do not contain any encodings. Number of times each mark attribute is used to encode quantitative or nominal data (b). ", "caption_bbox": [40, 211, 755, 251]}, {"image_id": 10, "file_name": "3152_10.png", "page": 9, "dpi": 300, "bbox": [28, 29, 394, 299], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. User study responses to post-task questionnaires.", "caption_bbox": [28, 298, 316, 312]}], "3153": [{"image_id": 0, "file_name": "3153_00.png", "page": 4, "dpi": 300, "bbox": [40, 363, 758, 567], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1: Baseline Experiment       Observed Values ", "caption_bbox": [504, 657, 654, 687]}, {"image_id": 1, "file_name": "3153_01.png", "page": 4, "dpi": 300, "bbox": [40, 29, 758, 321], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                       We use Pearson\u2019s Chi-Squared test to assess whether users in different Fig. 2: Diagram showing how latency is assigned to tiles, denoted as   latency conditions tend to \ufb01nd the low-latency target \ufb01rst. We ob- individual squares. The purple square is the user\u2019s current location   served no statistically-signi\ufb01cant relationship between these variables, U. Tiles along a straight path from the user\u2019s current location to the \u03c7 2 (4, N = 101) = 2.373, p = 0.6675. We therefore conclude that the ", "caption_bbox": [40, 566, 756, 623]}, {"image_id": 2, "file_name": "3153_02.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 392], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Results for a logistic regression analysis of the results for Exper- iment 3.2, with latency as the independent variable and the probability of \ufb01nding the low-latency target \ufb01rst as the dependent variable. This relationship is signi\ufb01cant only at the level of p = 0.1. When we view these results graphically, we observe the same weak relationship. In this diagram, blue tickmarks along the top of the \ufb01gure represent trials in which the low-latency target was located \ufb01rst, and black tickmarks along the bottom of the \ufb01gure represent trials in which the high-latency target was located \ufb01rst. The grey band around the logistic regression line represents the 95% con\ufb01dence interval. ", "caption_bbox": [392, 395, 744, 533]}, {"image_id": 3, "file_name": "3153_03.png", "page": 6, "dpi": 300, "bbox": [391, 29, 758, 995], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: The organization of background images for Experiment 4.", "caption_bbox": [415, 1002, 743, 1016]}, {"image_id": 4, "file_name": "3153_04.png", "page": 6, "dpi": 300, "bbox": [39, 466, 393, 930], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: The resulting decision tree for Experiment 3.2. An initial split on latency > 3103ms indicates that latency below this threshold had no discernible effect, classifying just 8 of 15 observations correctly. At higher levels of latency, additional factors such as screen size and target location provide greater predictive capacity. The confusion matrix indicates that this model achieves 76.9% accuracy on the test set. ", "caption_bbox": [40, 933, 390, 1016]}, {"image_id": 5, "file_name": "3153_05.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 309], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: The resulting decision tree for Experiment 4, with maximum latencies sampled uniformly from the range [0ms, 14,000ms). An initial split on totalInteractions >= 50 indicates that in this modi\ufb01ed experimental setup, latency was not the most important predictor of which target was located \ufb01rst. However, the confusion matrix indicates that this model achieves just 60% accuracy on the reserved test set. ", "caption_bbox": [392, 312, 742, 395]}, {"image_id": 6, "file_name": "3153_06.png", "page": 8, "dpi": 300, "bbox": [404, 646, 758, 955], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Distribution of strategies across Experiments 1-4. The ob- served instances of each strategy appear in the corresponding bubble beneath each experiment. Bubbles are scaled by total observations, and color encodes how latency was treated (categorical or continuous). ", "caption_bbox": [404, 960, 756, 1015]}, {"image_id": 7, "file_name": "3153_07.png", "page": 8, "dpi": 300, "bbox": [406, 267, 729, 589], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: Logistic regression analysis for Experiment 4, with latency as the independent variable and the probability of strategy switching as the dependent variable. This model was \ufb01t using maximum likelihood. ", "caption_bbox": [404, 593, 754, 634]}, {"image_id": 8, "file_name": "3153_08.png", "page": 8, "dpi": 300, "bbox": [42, 267, 365, 591], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Logistic regression analysis for Experiment 3.2, with latency as the independent variable and the probability of strategy switching as the dependent variable. Because the incidence is small (12 out of 88 cases), we employ Firth\u2019s method [11] rather than maximum likelihood. ", "caption_bbox": [39, 594, 391, 649]}, {"image_id": 9, "file_name": "3153_09.png", "page": 8, "dpi": 300, "bbox": [40, 29, 758, 208], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Four high-level search strategies emerged during initial clustering of search path diagrams. The red dot in each diagram represents the participant\u2019s starting location in the collage. The yellow dot denotes the position of the fast (low-latency) target; the black dot the slow (high-latency) target. Arrows indicate the direction and length of each panning interaction. ", "caption_bbox": [39, 212, 754, 254]}], "3154": [{"image_id": 0, "file_name": "3154_00.png", "page": 2, "dpi": 300, "bbox": [391, 29, 746, 337], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Work\ufb02ow of our approach. Ensemble simulations are conducted with different simulation parameters on supercomputers, and visualiza- tion images are generated in situ for different visual mapping and view parameters. The generated images and the parameters are collected into an image database. A deep image synthesis model (i.e., InSituNet) is then trained of\ufb02ine based on the collected data, which is later used for parameter space exploration through an interactive visual interface. ", "caption_bbox": [392, 343, 744, 436]}, {"image_id": 1, "file_name": "3154_01.png", "page": 2, "dpi": 300, "bbox": [392, 442, 746, 804], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Our in situ training data collection pipeline. Simulation data, gen- erated with different simulation parameters, are visualized in situ with different visual mapping and view parameters. The in situ visualization generates a large number of images, which are collected along with the corresponding parameters for the training of InSituNet of\ufb02ine. ", "caption_bbox": [392, 808, 744, 875]}, {"image_id": 2, "file_name": "3154_02.png", "page": 3, "dpi": 300, "bbox": [404, 209, 757, 418], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Architecture of R\u03c9 , which encodes input parameters into a latent vector with fully connected layers and maps the latent vector into an output image with residual blocks. The size of R\u03c9 is de\ufb01ned by k, which controls the number of convolutional kernels in the intermediate layers. ", "caption_bbox": [405, 422, 755, 475]}, {"image_id": 3, "file_name": "3154_03.png", "page": 4, "dpi": 300, "bbox": [27, 393, 746, 872], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                        an adversarial loss Ladv R into the loss function. Unlike the feature Fig. 6. Architecture of F (i.e., VGG-19 network), where each layer is   reconstruction loss, which measures the difference between each pair labeled with its name. Feature maps are extracted through convolutional of images, the adversarial loss focuses on identifying and minimizing layers (e.g., relu1 2) for feature-level comparisons.                   the divergence between two image distributions following the adver- ", "caption_bbox": [28, 871, 743, 919]}, {"image_id": 4, "file_name": "3154_04.png", "page": 4, "dpi": 300, "bbox": [27, 29, 746, 316], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Architecture of D\u03c5 . Input parameters and the predicted/ground    lier works [31, 32, 36], however, the average pixel wise distance often truth image are transformed into latent vectors with fully connected lay- produces over-smoothed images, lacking high-frequency features. ers and residual blocks, respectively. The latent vectors are then incor-    In this work, we de\ufb01ne L by combining two advanced loss func- porated by using the projection-based method [42] to predict how likely   tions: a feature reconstruction loss [32] LF,l f eat and an adversarial the image is a ground truth image conditioning on the given parameters.   loss [25] Ladv R , namely ", "caption_bbox": [28, 315, 743, 392]}, {"image_id": 5, "file_name": "3154_05.png", "page": 5, "dpi": 300, "bbox": [404, 642, 755, 814], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Visual interface for parameter space exploration. (a) The three groups of parameters: simulation, visual mapping, and view parameters. (b) The predicted visualization image and the sensitivity analysis result. ", "caption_bbox": [405, 820, 757, 860]}, {"image_id": 6, "file_name": "3154_06.png", "page": 7, "dpi": 300, "bbox": [449, 454, 722, 533], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Images generated by InSituNet trained with L f eat that uses dif- ferent layers after the \ufb01rst pooling layer of VGG-19: (a) relu2 1 and (b) relu3 1. Checkerboard artifacts are introduced. ", "caption_bbox": [405, 536, 757, 578]}, {"image_id": 7, "file_name": "3154_07.png", "page": 8, "dpi": 300, "bbox": [391, 441, 746, 706], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Parameter space exploration with the Nyx simulation. For the selected parameter values, the sensitivity of different parameters is es- timated and visualized as line charts on the left, whereas the predicted image is visualized on the right. ", "caption_bbox": [392, 711, 744, 764]}, {"image_id": 8, "file_name": "3154_08.png", "page": 8, "dpi": 300, "bbox": [28, 29, 394, 159], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 5. Evaluation of the number of ensemble runs used for training.", "caption_bbox": [33, 417, 373, 431]}, {"image_id": 9, "file_name": "3154_09.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 226], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Comparison of the images generated using interpolation, GAN-VR, and InSituNet with the ground truth images. ", "caption_bbox": [392, 225, 743, 252]}, {"image_id": 10, "file_name": "3154_10.png", "page": 9, "dpi": 300, "bbox": [40, 384, 375, 579], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13. Comparison of the visual appearance of the predicted images using different h values to see the effect of this simulation parameter. \ufb01rst regularly sample the simulation parameters along the curve (128 samples are drawn) and then generate the visualization images with respect to the sampled simulation parameters. The L1 norm of the generated images is then computed and used to compute the sensitive curve using the central difference method. The result is shown as the orange curve in Figure 13. We can see that the sensitivity curves gener- ated with the two methods are similar. From the guidance provided by the line chart in Figure 13, we see that parameter h is more sensitive in the \ufb01rst half of its range (i.e., the left side of the dashed line). The three images generated using h values from this range demonstrate a bigger variance compared with the two images shown on the right (which are images generated by using h values from the second half of its range). ", "caption_bbox": [40, 179, 392, 375]}], "3155": [{"image_id": 0, "file_name": "3155_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 765, 321], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Tracking of ligaments and junctions in an aluminum open-cell foam during incremental compressive loading from 0.0 to 2.0 mm. Red boxes highlight the changes in connectivity that we detect when ligaments fracture. The yellow boxes highlight part of the material drifting outside the \ufb01eld of view of during CT scans. Colors are mapped to the ids of ligaments and junctions. ", "caption_bbox": [61, 341, 712, 381]}, {"image_id": 1, "file_name": "3155_01.png", "page": 2, "dpi": 300, "bbox": [40, 29, 758, 185], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Left: an as-manufactured investment-cast aluminum foam. Right: by their importance on the performance of the foam. This knowledge chamber of Varian BIR 150/130 X-ray CT imaging system with mechani-    will be crucial to enable design optimization of the foams to meet cal load frame in place and mechanical load frame (Images reproduced   speci\ufb01c performance requirements. The \ufb01rst step in enabling a data- with permission from Matheson et al. [37]).                            driven design paradigm is collecting and analyzing large amounts of ", "caption_bbox": [39, 184, 756, 239]}, {"image_id": 2, "file_name": "3155_02.png", "page": 4, "dpi": 300, "bbox": [406, 410, 753, 515], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: A T-junction of tubes with radius r is rasterized onto a volumetric grid. As the search radius R increases (indicated with a yellow sphere, left to right), the contours of the geodesic density function \u03c1   become more localized in the vicinity of the junction. The colormap on each slice is rescaled. ", "caption_bbox": [404, 525, 755, 592]}, {"image_id": 3, "file_name": "3155_03.png", "page": 4, "dpi": 300, "bbox": [391, 29, 758, 238], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: The ratio of the volume of a junction to the volume of a tube is plotted in red as a function of the ratio of the radius of the tube r to the search radius R. The dotted line represents the portion where the closed-form equation V junction is not valid, and an approximation is used. The ratio of the volume of a disk-like feature with thickness a to a tube with radius a/2 is plotted in green as a function of a/R. The tube, junction, and disk all have identical values in the distance function, but they can be separated by the geodesic density function \u03c1   . As the search radius R increases, the ratios r/R and a/R decrease, and the curves increase. For instance, if R = 2r, then the intersection points at junctions are expected to have 1.28x the value in \u03c1   as points along the center of a ligament of the same radius. ", "caption_bbox": [404, 248, 756, 407]}, {"image_id": 4, "file_name": "3155_04.png", "page": 4, "dpi": 300, "bbox": [38, 29, 194, 1064], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Sphere (dark blue) transected", "caption_bbox": [193, 223, 386, 237]}, {"image_id": 5, "file_name": "3155_05.png", "page": 5, "dpi": 300, "bbox": [453, 346, 684, 575], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: When structures touch in the last time step, a user determines if they are separable by a simple intensity thresholding. In the CT scan on the left, the structures are separable, and the blue represents user- selected background, and the green represents user-selected material. The CT scan on the right is not able to separate merged structures, and therefore a 3d distance \ufb01eld is computed from the material interface, and a user selects background/material in this image. ", "caption_bbox": [392, 584, 744, 677]}, {"image_id": 6, "file_name": "3155_06.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 264], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: Work\ufb02ow for extracting and tracking features from foam image data over time, with numbers ordering the steps. Red numbers indicate steps where manual interaction is required and black indicates automated computation. The user \ufb01rst estimates (1-3) thresholds to correctly segment the junctions using the geodesic density metric (4-5). The junctions segmented can guide the extraction of the Morse-Smale complex (MSC) representation of the signed distance \ufb01eld of the input data (6) to generate a 1 \u2212 skeleton containing a superset of the ligaments (7). Finally, time tracking of junctions and ligaments (9-11) reports foam attributes over time. ", "caption_bbox": [27, 273, 743, 340]}, {"image_id": 7, "file_name": "3155_07.png", "page": 6, "dpi": 300, "bbox": [391, 29, 758, 182], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: Using Morse-Smale complex (MSC) representation to compute a 1 \u2212 skeleton from the signed distance \ufb01eld ", "caption_bbox": [404, 193, 754, 221]}, {"image_id": 8, "file_name": "3155_08.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 177], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12: We show the result of closest point matching (a) between junctions in consecutive time steps. For visual veri\ufb01cation, we also produce closest point matching between tracked ligaments (b). The ligaments and their matches are colored according to feature id. ", "caption_bbox": [392, 188, 742, 241]}, {"image_id": 9, "file_name": "3155_09.png", "page": 7, "dpi": 300, "bbox": [28, 29, 393, 167], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: The 1-skeleton generated by the Morse-Smale complex places a maximum (red spheres) in each junction (pink blobs) (a), with arcs (yellow tubes) connecting them. This graph structure is used to derive the connectivity of junctions. The locations of the 1-skeleton within disk-like regions is not stable. In (b) the skeleton in consecutive time steps (red for gray time step, blue for purple) shifts from one side of the disk to the other, highlighting the need for stable junction extraction. ", "caption_bbox": [27, 178, 377, 271]}, {"image_id": 10, "file_name": "3155_10.png", "page": 8, "dpi": 300, "bbox": [391, 29, 758, 459], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1: Number of junctions and ligaments extracted from four experi- mental foams (see corresponding foams in Figure 14). All datasets have images of resolution 1024x1024 and number of slices as listed. ", "caption_bbox": [404, 571, 756, 611]}, {"image_id": 11, "file_name": "3155_11.png", "page": 9, "dpi": 300, "bbox": [28, 438, 744, 513], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15: Some of the metrics extracted for the ligaments. On the left, a   There is a high demand for means to understand and correlate the design binary \ufb01eld indicates when a point along the ligament is inside a junction space of material properties to the material performance metrics with or not, in the middle, the color of the tubes indicates the length of each respect to attributes of their features for open cell metallic foams. Prior ligament; and on the right, the color indicates the orientation respect to to our work, the \ufb01ne grain characterization of their features was accom- the Z axis of the ligament.                                                plished via manual segmentation or with skeletonization approaches ", "caption_bbox": [28, 522, 745, 591]}, {"image_id": 12, "file_name": "3155_12.png", "page": 9, "dpi": 300, "bbox": [28, 29, 745, 236], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 2: Computation time of the analysis were performed using a  from the MSC 1-skeleton. Finally, computing the geodesic density synthetic dataset of size 300x300x300 and an experimental dataset function is expensive, practically limiting the search radius used, and of size 1043x1024x1024.                                           discouraging experimentation with the speed function thresholds. We ", "caption_bbox": [27, 396, 742, 436]}], "3156": [{"image_id": 0, "file_name": "3156_00.png", "page": 1, "dpi": 300, "bbox": [66, 65, 731, 355], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The Persistence diagrams of three members (a-c) of the Isabel ensemble (wind velocity) concisely and visually encode the number, data range and salience of the features of interest found in the data (eyewall and region of high speed wind, blue and red in (a)). In these diagrams, features with a persistence smaller than 10% of the function range or on the boundary are shown in transparent white. The pointwise mean for these three members (d) exhibits three salient interior features (due to distinct eyewall locations, blue, green and red), although the diagrams of the input members only report two salient interior features at most, located at drastically different data ranges (the red feature is further down the diagonal in (a) and (b)). The Wasserstein barycenter of these three diagrams (e) provides a more representative view of the features found in this ensemble, as it reports a feature number, range and salience that better matches the input diagrams (a-c). Our work introduces a progressive approximation algorithm for such barycenters, with fast practical convergence. Our framework supports computation time constraints (e) which enables the approximation of Wasserstein barycenters within interactive times. We present an application to the clustering of ensemble members based on their persistence diagrams ((f), lifting: \u03b1 = 0.2), which enables the visual exploration of the main trends of features of interest found in the ensemble. ", "caption_bbox": [73, 363, 723, 510]}, {"image_id": 1, "file_name": "3156_01.png", "page": 2, "dpi": 300, "bbox": [391, 29, 745, 137], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Critical points (spheres, light brown: minima, light green: maxima, other: saddles) and persistence diagrams of a clean (a) and noisy (b) 2D scalar \ufb01eld. From left to right : 2D data, 3D terrain visualization, persis- tence diagram. In both cases, the two main hills are clearly represented by salient persistence pairs in the diagrams. In the noisy diagram (b), small pairs near the diagonal correspond to noisy features in the data. ", "caption_bbox": [392, 139, 744, 219]}, {"image_id": 2, "file_name": "3156_02.png", "page": 2, "dpi": 300, "bbox": [399, 227, 734, 352], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Synthetic ensemble of a pattern with 2 gaussians and additive noise (a). The persistence diagram of the pointwise mean (b) contains 8 highly persistent features although each of the input ensemble members contain only 2 features. The Wasserstein barycenter (c) provides a diagram which is representative of the set, with a feature number, range and salience which better describes the input ensemble (2 large features). ", "caption_bbox": [392, 354, 744, 434]}, {"image_id": 3, "file_name": "3156_03.png", "page": 4, "dpi": 300, "bbox": [27, 29, 745, 197], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Illustration of the Auction algorithm [12] on a 2D example for the optimal assignment of 2 point sets (light and dark green). Boxes and columns represent auction rounds and auction iterations respectively. Each matrix reports the value va\u2192b currently estimated by the bidder a for the purchase of the object b. Assignments are shown with black arrows. After the \ufb01rst round (left box), a sub-optimal assignment is achieved, which becomes optimum at the second round (right box). Note that bidders can steal objects from each other within one auction round (iteration 3, second round). ", "caption_bbox": [28, 207, 742, 260]}, {"image_id": 4, "file_name": "3156_04.png", "page": 6, "dpi": 300, "bbox": [27, 29, 745, 193], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1. Comparison of running times (in seconds, 1 thread) for the estimation of Wasserstein barycenters of Persistence diagrams. N and  3.5 Parallelism #D( fi ) respectively stand for the number of members in the ensemble Our progressive framework can be trivially parallelized as the most and the average size of the input persistence diagrams.               computationally demanding task, the Assignment step (Alg. 2), is in ", "caption_bbox": [27, 231, 742, 284]}, {"image_id": 5, "file_name": "3156_05.png", "page": 7, "dpi": 300, "bbox": [406, 54, 754, 233], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Comparison of the evolution of the Fre\u0301chet energy (log scale, Sea Surface Height, maximum diagrams), for the Auction barycenter method [94]+[51](red) and 3 variants of our approach: without (blue) and with (orange) persistence progressivity, and with time constraints (green). Persistence-driven progressivity drastically accelerates convergence. ", "caption_bbox": [404, 234, 756, 301]}, {"image_id": 6, "file_name": "3156_06.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 363], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Clusters automatically identi\ufb01ed by our topological clustering (tmax : 10 seconds). From top to bottom: pointwise mean of each cluster. Left: Centroids computed by our interruptible clustering algorithm. Right: Wasserstein barycenters of the clusters, computed by our progressive al- gorithm run until convergence. Differences are visually indistinguishable. Barycenter extrema are scaled in the domain by persistence (spheres). ", "caption_bbox": [392, 374, 744, 454]}, {"image_id": 7, "file_name": "3156_07.png", "page": 8, "dpi": 300, "bbox": [27, 209, 380, 293], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Clustering the Gaussians ensemble. From left to right, pointwise mean and Wasserstein barycenter for each of the identi\ufb01ed clusters (tmax : 10s.) with geometrical lifting (\u03b1 = 0.65). ", "caption_bbox": [27, 303, 377, 345]}, {"image_id": 8, "file_name": "3156_08.png", "page": 8, "dpi": 300, "bbox": [27, 29, 394, 146], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Interrupted Wasserstein barycenters for one cluster of the Sea Surface Height ensemble with different computation time constraints. From left to right : 0.1 s., 1 s., 10 s., and full convergence (21 s.). ", "caption_bbox": [28, 156, 380, 196]}, {"image_id": 9, "file_name": "3156_09.png", "page": 9, "dpi": 300, "bbox": [404, 52, 757, 269], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Clusters automatically identi\ufb01ed by our topological clustering (tmax : 10s.). From left to right, top to bottom: pointwise mean of each cluster with barycenter extrema scaled by persistence (spheres). Left insets (a, c, e, g): Centroids computed by our interruptible clustering algorithm. Right insets (b, d, f, h): Wasserstein barycenters of the clusters, computed by our progressive algorithm run until convergence. ", "caption_bbox": [405, 279, 755, 359]}, {"image_id": 10, "file_name": "3156_10.png", "page": 9, "dpi": 300, "bbox": [40, 52, 392, 201], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Clusters automatically identi\ufb01ed by our topological clustering (tmax : 10s.). Left insets (a, c): Centroids computed by our interruptible clustering algorithm. Right insets (b, d): Wasserstein barycenters of the clusters, computed by our progressive algorithm run until convergence. Differences are visually indistinguishable. Top: pointwise mean of each cluster, with barycenter extrema scaled by persistence (spheres). ", "caption_bbox": [40, 211, 392, 291]}], "3157": [{"image_id": 0, "file_name": "3157_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 371], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Topological segmentation of counter-rotating vortex sampled using different methodologies discussed in the paper and by \ufb01ltering the contour tree for segments that resemble vortex-like structures. The number, shape, and boundaries of the segments are different for the three techniques. ", "caption_bbox": [61, 382, 710, 422]}, {"image_id": 1, "file_name": "3157_01.png", "page": 4, "dpi": 300, "bbox": [404, 254, 758, 593], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Persistence curves of the sampled data (at three resolutions) as compared to the original function solution. ", "caption_bbox": [404, 609, 754, 636]}, {"image_id": 2, "file_name": "3157_02.png", "page": 4, "dpi": 300, "bbox": [38, 57, 393, 928], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: The function f (x, y), simulation mesh, and the projected data.", "caption_bbox": [47, 931, 382, 947]}, {"image_id": 3, "file_name": "3157_03.png", "page": 4, "dpi": 300, "bbox": [404, 52, 753, 202], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: The persistence curve, the persistence diagram, and the segmen- tation of the original function f (x, y) (Figure 2a). ", "caption_bbox": [404, 207, 756, 235]}, {"image_id": 4, "file_name": "3157_04.png", "page": 5, "dpi": 300, "bbox": [27, 200, 377, 359], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Visualizations of the persistence diagrams ((a), (b), and (c)) and segments ((d), (e), and (f)) of the sampled data at different resolutions ", "caption_bbox": [28, 363, 378, 390]}, {"image_id": 5, "file_name": "3157_05.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 190], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Persistence curves of the subdivided data (at three resolutions) compared to the original function solution. ", "caption_bbox": [392, 207, 742, 234]}, {"image_id": 6, "file_name": "3157_06.png", "page": 6, "dpi": 300, "bbox": [404, 52, 757, 357], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Visualizations of the persistence diagrams ((a), (b), and (c)) and segments ((d), (e), and (f)) of the L-SIAC data at different resolutions ", "caption_bbox": [404, 362, 754, 389]}, {"image_id": 7, "file_name": "3157_07.png", "page": 6, "dpi": 300, "bbox": [39, 390, 757, 605], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                        but here we analyze only a single snapshot (in time) topologically. Fig. 8: Persistence curves of the L-SIAC data (at three resolutions) as                                                                         The mesh used for this simulation is shown in Figure 10a, which compared to the original function solution. ", "caption_bbox": [40, 604, 757, 638]}, {"image_id": 8, "file_name": "3157_08.png", "page": 6, "dpi": 300, "bbox": [404, 639, 758, 842], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: (a) Simulation mesh of the \ufb02ow over a 2D cylinder. (b) Vorticity calculated using element derivatives in the highlighted region of the simulation mesh. ", "caption_bbox": [404, 847, 754, 887]}, {"image_id": 9, "file_name": "3157_09.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 192], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13: The persistence diagrams of the vorticity for the \ufb02uid \ufb02ow past a circular cylinder described in Section 4.1. Thick gray lines are used to show the persistence pairs above the threshold 2.0, and the lines in orange are used to indicate the persistence pairs below the threshold. ", "caption_bbox": [392, 196, 742, 249]}, {"image_id": 10, "file_name": "3157_10.png", "page": 7, "dpi": 300, "bbox": [27, 306, 380, 435], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Persistence curves for the sampled, subdivided, and L-SIAC vorticity \ufb01elds of the \ufb02ow over a 2D cylinder. ", "caption_bbox": [27, 451, 377, 478]}, {"image_id": 11, "file_name": "3157_11.png", "page": 7, "dpi": 300, "bbox": [397, 398, 739, 580], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14: Segmentation of the vorticity over a \ufb02ow past a cylinder described in Section 4.1. The segmentations are calculated using the contour tree and a persistence threshold in region 2 of Figure 12. The critical points denoted by saddle1 and saddle2 are the saddles corresponding the merges and splits, respectively. ", "caption_bbox": [392, 584, 742, 651]}, {"image_id": 12, "file_name": "3157_12.png", "page": 8, "dpi": 300, "bbox": [391, 936, 757, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 19: Persistence diagrams for the vorticity of the counter-rotating vortex dataset. Orange bars highlight what was removed by topological \ufb01ltering. ", "caption_bbox": [404, 890, 754, 930]}, {"image_id": 13, "file_name": "3157_13.png", "page": 8, "dpi": 300, "bbox": [39, 626, 758, 890], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                           The datasets had vorticity ranging from [0, 84], [0, 88], and [0, 176] Fig. 16: Input simulation of the counter-rotating vortices. We focus on                                                                         for sampled, subdivided and L-SIAC vorticity \ufb01elds, respectively. To analyzing data in the red cube. The counter-rotating vortices in green are iso-surfaces of vorticity (magnitude) calculated using element-wise                                                                         enable comparisons of persistence curves and diagrams, we normal- derivatives.                                                            ized the vorticity to the same range of [0, 1]. The persistence curves ", "caption_bbox": [40, 564, 757, 627]}, {"image_id": 14, "file_name": "3157_14.png", "page": 8, "dpi": 300, "bbox": [404, 353, 755, 502], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 18: Zoom in on the region associated with Figure 17. For each of datasets, we highlight three different stable regimes of critical features (numbered 1-3). ", "caption_bbox": [404, 521, 754, 561]}, {"image_id": 15, "file_name": "3157_15.png", "page": 8, "dpi": 300, "bbox": [39, 329, 392, 557], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15: Segmentation of the vorticity for the \ufb02ow past a cylinder de- scribed in Section 4.1. The segmentation for the sampled and the subdivided vorticity are computed using the contour tree along with a persistence threshold of 0.04. In the case of L-SIAC vorticity, the value of persistence threshold used is 0.001. ", "caption_bbox": [40, 244, 392, 311]}, {"image_id": 16, "file_name": "3157_16.png", "page": 8, "dpi": 300, "bbox": [404, 57, 757, 289], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 17: Persistence curves for the sampled, subdivided, and L-SIAC vorticity \ufb01elds of the counter-rotating vortex. ", "caption_bbox": [404, 306, 754, 333]}], "3158": [{"image_id": 0, "file_name": "3158_00.png", "page": 1, "dpi": 300, "bbox": [26, 29, 744, 365], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Our localized merge forest data structure partitions the Foot data set, computes local merge trees, and connects them via a bridge set. On the left, the entire superlevel set at threshold 84, where the dark lines highlight the cells in the bridge set connecting the regions of the domain decomposition and the local trees of the forest. On the right, the Components query on the merge forest returns the \ufb01ve largest connected components in the superlevel set, shown in unique colors. The query correctly resolves the connectivity and eliminates the noise. ", "caption_bbox": [63, 374, 712, 441]}, {"image_id": 1, "file_name": "3158_01.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 197], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: On the left are two local regions (red) with a scalar \ufb01eld (solid edges) connected by the bridge set (dashed edges), with three maxima (7, 9, and 8) and two saddles (6 and 4). On the right are the local trees of the two regions (\ufb01lled circles are locally regular vertices) and their local copy of reduced bridge-set edges necessary for traversal between the local trees. Note that the boundary restricted maxima [30] are not suf\ufb01cient due to the lack of a region overlap. For example, the saddle at the vertex with value 6 is not present as a node in either of the two trees (it is either a bridge-set edge end vertex or a regular vertex). ", "caption_bbox": [392, 208, 742, 327]}, {"image_id": 2, "file_name": "3158_02.png", "page": 7, "dpi": 300, "bbox": [52, 56, 744, 195], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: The region size affects the performance of the serial forest construction. Due to better locality and a smaller log factor from the sort, we can build the forest faster than the global tree. Moreover, we use a smaller index data type to reduce the memory needed to store the forest, i.e., the two jumps, from 323 to 643 and from 10243 to 20483 , are caused by switching to a larger index type. We do not split arcs in local trees with bridge-set end vertices, and thus the cost of the domain decomposition is mainly re\ufb02ected in the total size (number of edges) of the local reduced bridge sets. ", "caption_bbox": [40, 208, 755, 261]}, {"image_id": 3, "file_name": "3158_03.png", "page": 7, "dpi": 300, "bbox": [404, 308, 759, 829], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: On the left, scaling with the close con\ufb01guration (a single socket left of the stippled vertical line). FTC and TTK do not scale over multiple sockets, whereas the forest scales linearly. The exception is the Foot data set that has only 64 regions, which limits its scalability and increases load imbalance. On the right, scaling with the spread con\ufb01guration shows that the forest bene\ufb01ts from more cache available across sockets and is more robust to the system con\ufb01guration. ", "caption_bbox": [405, 840, 755, 933]}, {"image_id": 4, "file_name": "3158_04.png", "page": 9, "dpi": 300, "bbox": [410, 55, 750, 166], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Measuring the query time (log scale) as a function of threshold for the Components query on the forest with region size 643 and the global tree. In each case, a threshold of 0 extracts every data point. We observe the queries execute in comparable time, and more topologically complex data sets perform better with the forest. ", "caption_bbox": [405, 177, 755, 244]}, {"image_id": 5, "file_name": "3158_05.png", "page": 9, "dpi": 300, "bbox": [46, 55, 385, 164], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: The ComponentMax query on the Miranda data set with varying region size on the global tree and forest. We clamped the histogram (a) after 0.5 second. As the region size increases, the overhead decreases, and the absolute time difference is negligible and well within interactive time. We expect most practical queries to be in the left to the middle of the histogram (the ones on the right traverse the whole forest). ", "caption_bbox": [40, 175, 391, 255]}], "3159": [{"image_id": 0, "file_name": "3159_00.png", "page": 1, "dpi": 300, "bbox": [86, 65, 708, 392], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. LassoNet enables effective lasso-selection of 3D point clouds based on a latent mapping from viewpoint and lasso to target point clouds. LassoNet is particularly ef\ufb01cient for selecting multiple regions (insets 1, 2, 3) in a complex scene (left), since no viewpoint changing is required to select occluded points. Notice here the insets are viewed from different viewpoints. ", "caption_bbox": [73, 394, 722, 434]}, {"image_id": 1, "file_name": "3159_01.png", "page": 2, "dpi": 300, "bbox": [391, 29, 745, 191], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. To select both two wings of the airplane, two sequential lassos from different viewpoints are required, and selection results are combined using either union (top) or subtraction (bottom) Boolean operations. ", "caption_bbox": [392, 198, 742, 238]}, {"image_id": 2, "file_name": "3159_02.png", "page": 3, "dpi": 300, "bbox": [74, 54, 715, 299], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. LassoNet consists of three stages: In Interaction Encoding stage, we associate point cloud with viewpoint and lasso through 3D coordinate transformation and naive selection; In Filtering and Sampling stage, we reduce the amount of points for network processing through intention \ufb01ltering and farthest point sampling. Lastly, we build a hierarchical neural network in Network Building stage. ", "caption_bbox": [40, 305, 755, 345]}, {"image_id": 3, "file_name": "3159_03.png", "page": 4, "dpi": 300, "bbox": [29, 29, 745, 310], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Overview of network building. The DNN network is built upon (a) PointNet [28], and we employ a hierarchical structure that generates more local and global features using (b) abstraction and (c) propagation components. ", "caption_bbox": [28, 314, 742, 341]}, {"image_id": 4, "file_name": "3159_04.png", "page": 5, "dpi": 300, "bbox": [49, 52, 758, 189], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1. Statistics of lasso-selection records.", "caption_bbox": [470, 383, 690, 397]}, {"image_id": 5, "file_name": "3159_05.png", "page": 6, "dpi": 300, "bbox": [391, 459, 746, 984], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Jaccard distances per epoch in training and testing processes for ShapeNet (left) and S3DIS (right) annotations. ", "caption_bbox": [392, 990, 742, 1017]}, {"image_id": 6, "file_name": "3159_06.png", "page": 6, "dpi": 300, "bbox": [400, 218, 746, 418], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Jaccard distances of CylinderSelection and LassoNet measured upon task complexity. ", "caption_bbox": [392, 422, 742, 449]}, {"image_id": 7, "file_name": "3159_07.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 175], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Jaccard distances of CylinderSelection and LassoNet measured upon scene complexity. ", "caption_bbox": [392, 177, 742, 204]}, {"image_id": 8, "file_name": "3159_08.png", "page": 7, "dpi": 300, "bbox": [52, 804, 385, 927], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Exemplar point clouds employed in the user study. Datasets: We conducted the user study on three different datasets, as shown in Fig. 9. Besides ShapeNet (denoted as D1) and S3DIS (D2) described in Sec. 5.1, we recruited a third dataset of astronomy point clouds that were used in CAST experiments (D3). D3 consists of four point clouds, each of which is made up of 200K to 400K points ", "caption_bbox": [40, 931, 392, 1019]}, {"image_id": 9, "file_name": "3159_09.png", "page": 7, "dpi": 300, "bbox": [404, 30, 758, 987], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Comparison of completion time (left) and Jaccard distance (right) of CylinderSelection, SpaceCast, and LassoNet on three datasets. ", "caption_bbox": [405, 990, 755, 1017]}, {"image_id": 10, "file_name": "3159_10.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 240], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Comparison of LassoNet with prior methods on two different examples. (Left) LassoNet vs CylinderSelection on a chair in ShapeNet dataset. (Right) LassoNet vs CylinderSelection vs SpaceCast on an arti\ufb01cial dataset. ", "caption_bbox": [27, 243, 744, 270]}, {"image_id": 11, "file_name": "3159_11.png", "page": 9, "dpi": 300, "bbox": [64, 55, 370, 282], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Typical examples of a good (left) case and a bad (right) case.", "caption_bbox": [40, 294, 381, 308]}], "3161": [{"image_id": 0, "file_name": "3161_00.png", "page": 1, "dpi": 300, "bbox": [74, 65, 721, 502], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The topology-based visual analytics framework supports the feature-centered navigation of Cinema databases consisting of image and analysis products generated during large-scale simulation runs, where numerous features are organized into a manageable amount of hierarchical groups that can be explored in a level-of-detail approach. Here, the interface shows an ensemble member of the viscous \ufb01nger dataset, where colors encode individual \ufb01ngers for salt concentration level 30. The prime interaction device of the framework is a nested tracking graph (NTG) that simultaneously displays the temporal evolution of superlevel set components for multiple levels (bottom). The NTG is used to navigate through time and retrieve component images from the database (top left), whereas the split tree (top center) and persistence diagram (top right) support the user in selecting important levels and \ufb01lter criteria. ", "caption_bbox": [73, 509, 723, 602]}, {"image_id": 1, "file_name": "3161_01.png", "page": 2, "dpi": 300, "bbox": [27, 79, 744, 216], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Processing pipeline of the presented approach that consisits of the in situ database generation and the post hoc database exploration. During simulation runtime, the approach derives for each timestep the split tree and its associated domain segmentation to compute tracking information, images of feature groups, and re\ufb01ned split trees, which are stored in a Cinema database. During post hoc analysis, the database elements are used to dynamically compute nested tracking graphs, composite 3D views of feature groups, and to visualize the split trees and their corresponding persistence diagrams, which are all in turn integrated in a feature-centered visual analytics framework to effectively explore the underlying simulation. ", "caption_bbox": [28, 222, 745, 290]}, {"image_id": 2, "file_name": "3161_02.png", "page": 3, "dpi": 300, "bbox": [409, 451, 758, 916], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Illustration of a split tree segmentation S = (T , \u03c6 , \u03c8) for a scalar \ufb01eld (a, left) consisting of a split tree T (a, right), its scalar \ufb01eld \u03c8 (y-axis, right), and the domain segmentation \u03c6 (b). Each individual superlevel set component for a level l corresponds to one connected subtree of the split tree above the level threshold, and vice versa (c). A persistence-based branch decomposition of T can be used to further group branches into bundles whose components can be depicted in one image (d). ", "caption_bbox": [405, 923, 756, 1017]}, {"image_id": 3, "file_name": "3161_03.png", "page": 3, "dpi": 300, "bbox": [411, 30, 756, 354], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. (Bottom) Superlevel set components of a time-varying scalar- \ufb01eld for three levels (dark to light blue). (Top) 3D illustration of a NTG where components are represented by vertices, and tracking graphs for each level are shown in shades of blue, and nesting hierarchies for each timestep in shades of red. (Middle) 2D NTG layout where tracking graph edges are drawn inside each other based on the nesting hierarchies. ", "caption_bbox": [404, 360, 756, 440]}, {"image_id": 4, "file_name": "3161_04.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 520], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Illustration of the segmentation-based tracking approach that processes two split trees (bottom) and their respective domain segmenta- tions (top) of two adjacent timesteps (left and right). From the previous to the current timestep, the maximum B splits into the two maxima D and E , and the maximum A (later labeled C) moves from the left to the right side of the domain. The overlap of segments are recorded by so-called meta edges between their corresponding representatives (all arrows). For example, the dark blue and dark green segments overlap, which justi\ufb01es the meta edge  A  ,C   . The light blue and the light green segments, however, do not overlap, and thus there exists no meta edge between A and C. The \ufb01gure also highlights a single iteration of Alg. 1 for a vertex v of the domain. First, the algorithm retrieves for the segments in which v resides the corresponding edges of both split trees (split tree edges with white discs), then determines the corresponding base edges in both trees (thin split tree edges), and \ufb01nally adds meta edges (red arrows) between the representatives of the base edges and all connected edges towards the root. Note, the complete set of meta edges (red and gray arrows) correctly record the overlap of segments across all intervals. Yet, the accuracy of the matching depends on the resolution of the intervals. ", "caption_bbox": [392, 532, 744, 783]}, {"image_id": 5, "file_name": "3161_05.png", "page": 5, "dpi": 300, "bbox": [434, 30, 757, 448], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Illustration of the image generation process for \u223c 5k vortices of the jet dataset at timestep 2000 based on two groups (cool and warm) and two persistence intervals (light and dark). ", "caption_bbox": [405, 448, 755, 489]}, {"image_id": 6, "file_name": "3161_06.png", "page": 6, "dpi": 300, "bbox": [467, 257, 662, 411], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Vertex and nesting tree computation based on split trees. Vertices correspond to edge-cuts (red nodes) for a set of levels (dashed lines), where each vertex represents a single superlevel set component, and is labeled by its corresponding edge representative, level, and timestep (here omitted). To determine their nesting hierarchy (red edges), the algorithm traverses the split tree from each vertex at level li with i > 0 towards the root, until the algorithm reaches its parent at level li\u22121 . ", "caption_bbox": [392, 412, 744, 506]}, {"image_id": 7, "file_name": "3161_07.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 202], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Depth image-based rendering pipeline: multiple depth images and ID masks (a) are respectively composed into a single image (b), which are shaded based on approximated surface normals (c) and screen space ambient occlusion (d). ", "caption_bbox": [28, 205, 742, 232]}, {"image_id": 8, "file_name": "3161_08.png", "page": 7, "dpi": 300, "bbox": [404, 30, 758, 370], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. NTGs of the viscous \ufb01nger dataset for salt concentration levels 25, 30, and 35 (red to yellow). (Top) NTG generated with the explicit overlap- based tracking approach of Lukasczyk et al. [19]. (Middle and Bottom) NTGs generated with the split tree-based tracking approach, where the bottom graph is \ufb01ltered by persistence, size, and overlap thresholds. ", "caption_bbox": [405, 370, 757, 437]}, {"image_id": 9, "file_name": "3161_09.png", "page": 9, "dpi": 300, "bbox": [40, 52, 757, 517], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Analysis of the asteroid impact yA31 temperature \ufb01eld. (Top and Middle) NTGs visualizing the evolution of the temperature \ufb01eld for the levels 0.15eV , 0.2eV , and 0.28eV , where the second graph highlights level 0.2eV . (Bottom) The 3D composited view and the merge tree at timestep 108 for level 0.2eV . The proposed approach naturally partitions the temperature volumes into groups: the asteroid trail (dark blue), the cloud that raises to the stratosphere (dark red), the \ufb02ame front that thrusts forward over the ocean (dark orange), and the burning region at the impact site (dark green). ", "caption_bbox": [40, 524, 755, 577]}], "3162": [{"image_id": 0, "file_name": "3162_00.png", "page": 1, "dpi": 300, "bbox": [61, 65, 703, 358], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Potential vorticity (PV) banners are created during the detachment of \ufb02ow from the mountain topography and can extend several hundred kilometers downstream. Here, the banners appear in the lee of the Alps as line-shaped regions with signi\ufb01cantly negative (blue) or positive (red) PV values. (a) Traditional visualization of potential vorticity as 2D slice, computed at a \ufb01xed altitude level. (b) Our method robustly extracts PV banners that originate from \ufb02ow detachment at the topography and removes visual clutter. (c) Visualization of our PV banner lines in 3D before clustering into banner representatives. Our method allows for the interpretation of the spatial extent of the banners and shows PV banners across all altitude levels. ", "caption_bbox": [60, 363, 709, 443]}, {"image_id": 1, "file_name": "3162_01.png", "page": 2, "dpi": 300, "bbox": [28, 29, 745, 218], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. (a) PV banners form in the lee of a mountain when air \ufb02ows around rather than over it. Primary banners, colored in red (positive PV) and blue (negative PV), are on the scale of the whole mountain. Their width is determined by boundary layer processes and turbulence. The width of secondary banners, which are created in the lee of individual peaks and massifs, is determined by the upstream topography. (b) Vertical cross section of secondary banners in a numerically-simulated \ufb02ow, showing the velocity using an arrow plot projected onto a plane. The color denotes the PV value at each point and the arrows highlight the swirling motion. (c) The rotation direction of a potential vorticity banner depends on which side the air \ufb02ows around the mountain. ", "caption_bbox": [28, 223, 743, 303]}, {"image_id": 2, "file_name": "3162_02.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 390], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Method overview. (a) The wind velocity and potential vorticity scalar \ufb01eld serve as inputs to our algorithm. (b) The PV banner extraction algorithm distributes seed points along the mountain surface and integrates the banners using a predictor-corrector algorithm. (c) Clustering similar banners into groups and representatives by constructing a similarity matrix. (d) Using multiple linked views, a \ufb02exible node-based editor and interactive \ufb01ltering, we can highlight the desired properties of the banners. (e) Users interact with all views by picking as well as linking and brushing. ", "caption_bbox": [28, 395, 744, 448]}, {"image_id": 3, "file_name": "3162_03.png", "page": 5, "dpi": 300, "bbox": [404, 272, 757, 429], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. The PV banners are colored based on their cluster membership. This enables lines to be categorized together that are integrated from different seed points but converged into the same banner. We apply the AHC to group banners and select representative lines. ", "caption_bbox": [405, 434, 757, 487]}, {"image_id": 4, "file_name": "3162_04.png", "page": 5, "dpi": 300, "bbox": [55, 680, 758, 938], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Illustration of the correction step of the predictor-corrector al- banners, e.g., to measure their life time. Further, this allows us to raise gorithm, which corrects a predicted point by setting it to the potential   additional question, e.g., how long does a banner stay stable? How vorticity minimum (maximum) using the method of steepest descent (as-      does its curvature, length, or vertical extent behave over time? cent). Projecting the gradient on a plane approximately parallel to the        We implemented this requirement by clustering over time. As input velocity allows us to \ufb01nd the extremum at the intersection of the implicit we take the cluster representatives found with AHC of the time steps lines where the gradient of each component is equal to 0.                  ti and ti+1 . By connecting each representative in ti with the closest ", "caption_bbox": [40, 937, 755, 1017]}, {"image_id": 5, "file_name": "3162_05.png", "page": 5, "dpi": 300, "bbox": [504, 52, 662, 175], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Cluster algorithm using AHC. The grid represents a spatial data structure. Lines that share a grid cell are compared with each other, e.g., the blue or green shaded cells highlight overlapping line elements in the spatial grid. s1 and s2 have a high similarity measure and are clustered together. None of the lines have a high similarity with s3 and are therefore clustered independently. ", "caption_bbox": [405, 178, 756, 258]}, {"image_id": 6, "file_name": "3162_06.png", "page": 7, "dpi": 300, "bbox": [405, 613, 754, 678], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Distribution of PV in the model\u2019s atmosphere. Cells of constant volume are sampled in the altitude range between 0 km and 6 km above sea level. The mean of the normally distributed PV values is slightly shifted to the positive side, which was expected by the meteorologists. ", "caption_bbox": [405, 680, 755, 733]}, {"image_id": 7, "file_name": "3162_07.png", "page": 7, "dpi": 300, "bbox": [40, 52, 757, 159], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Data \ufb02ow programming enables a high degree of \ufb02exibility by combining operators through connections while hiding the underlying algorithmic complexity. This speci\ufb01c example samples seed points along the surface, integrates the banners using the predictor corrector algorithm, \ufb01lters the banners that are within a user de\ufb01ned box, and shows them in the view using the depicted color maps. ", "caption_bbox": [40, 164, 754, 204]}, {"image_id": 8, "file_name": "3162_08.png", "page": 7, "dpi": 300, "bbox": [40, 218, 757, 374], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. (a) Parallel Coordinates Diagram. Selection of positive PV banners with low updraft which automatically updates all other views. (b) Updated view according to selection in (a). (c) Limit selection by placing shapes in the scene. E.g., box shape that only shows banners within. ", "caption_bbox": [40, 379, 755, 406]}, {"image_id": 9, "file_name": "3162_09.png", "page": 7, "dpi": 300, "bbox": [407, 420, 755, 535], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. PV value along the extracted core lines. The absolute PV value declines with the length of the banner. Typically the neighboring PV banners with anomalously positive (red) and negative (blue) signs show similar trends along their length. The mean is shown as a black line. ", "caption_bbox": [405, 535, 755, 588]}, {"image_id": 10, "file_name": "3162_10.png", "page": 8, "dpi": 300, "bbox": [27, 29, 745, 215], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13. (a) Selected PV banners are shown to analyze their spatial extent. (b) A visualization of a horizontal cross section of the vertical wind component at \ufb01xed altitude along the PV banner is shown. The wavelike patterns in the \ufb02ow direction (from left to right) is associated with atmospheric gravity (or buoyancy) waves and is responsible for the altitude changes of PV banners. (c) A plot of the altitude of the PV banners along their full extent is depicted. The banners show an undulating pattern in the altitude change close to their source. ", "caption_bbox": [28, 217, 742, 270]}, {"image_id": 11, "file_name": "3162_11.png", "page": 8, "dpi": 300, "bbox": [80, 281, 328, 434], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14. Relative humidity extracted as a horizontal cross section around the PV banners on 2500m above sea level. A correlation between the PV banners and relative humidity values is visible. This highlights that PV banners separate air masses of alternating high and low humidity. ", "caption_bbox": [28, 439, 378, 492]}, {"image_id": 12, "file_name": "3162_12.png", "page": 8, "dpi": 300, "bbox": [392, 281, 745, 431], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15. 27 January 2010 20:10 UTC. PV banners color-mapped by altitude. Banners at different altitudes exhibit different directions and may cross each other. Comprehending these crossings is straightforward in our 3D visualization, but dif\ufb01cult to detect in standard horizontal displays. ", "caption_bbox": [392, 436, 744, 489]}, {"image_id": 13, "file_name": "3162_13.png", "page": 9, "dpi": 300, "bbox": [41, 185, 757, 350], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1. The timings were measured on a grid with 500 \u00d7 500 \u00d7 60 resolu- tion, based on the node graph earlier shown in Fig. 9. ", "caption_bbox": [405, 156, 757, 184]}], "3163": [{"image_id": 0, "file_name": "3163_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 774, 390], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: The eigenvalue graph of the gradient tensor \ufb01eld of a simulated \ufb02ow on the boundary of a diesel engine [19] (left) and the corresponding eigenvalue graph after the simpli\ufb01cation of the gradient tensor \ufb01eld (right). In the original tensor \ufb01eld (left), there are a number of regions dominated by either expansion (colored yellow) and contraction (colored blue). However, such regions disappear after the simpli\ufb01cation process (right), indicating that they are not as signi\ufb01cant as regions dominated respectively by counterclockwise rotations (red), clockwise rotations (green), and pure shears (antique white). ", "caption_bbox": [58, 404, 694, 472]}, {"image_id": 1, "file_name": "3163_01.png", "page": 3, "dpi": 300, "bbox": [28, 29, 745, 250], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: This \ufb01gure illustrates the notions of eigenvector manifold (a) and eigenvalue manifold (b and c), replicated from [35]. In the eigenvector manifold (a sphere), there are four types of tensor behavior: complex eigenvalues with CCW rotations (red), real eigenvalues with CCW rotations (pink), real eigenvalues with CW rotations (cyan), and complex eigenvalues with CW rotations (green). In the eigenvalue manifold (a hemisphere), there are \ufb01ve types of tensor behavior that show the underlying vector \ufb01eld is dominated by: expansion or positive scaling (yellow), contraction or negative scaling (blue), CCW rotation (red), CW rotation (green), and pure shear or anisotropic stretching (antique white). ", "caption_bbox": [27, 251, 743, 320]}, {"image_id": 2, "file_name": "3163_02.png", "page": 3, "dpi": 300, "bbox": [27, 343, 746, 951], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                           is incident to vi . Figure 4(a) shows one example of a vertex with a Fig. 3: This \ufb01gure shows the visualization of a slice in the diesel engine non-zero Gaussian curvature. For a more detailed discussion of this simulation (left) and its corresponding eigenvector graph (right). A       problem, we refer the reader to [33]. ", "caption_bbox": [28, 950, 743, 992]}, {"image_id": 3, "file_name": "3163_03.png", "page": 5, "dpi": 300, "bbox": [418, 384, 719, 545], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: This \ufb01gure illustrates the steps of connecting the separately extracted intersecting region boundaries at a junction point. (a) We start with a triangle with three region boundaries. (b) We divide the \u03b3d = \u00b1\u03b3r boundary on which junction point is located, and connect the nearest \u03b3s = \u03b3r and \u03b3s = \u03b3d boundary to the junction point. (c) The junction point is now on three intersected boundary curves. (d) We merge the adjacent regions of the same type. ", "caption_bbox": [392, 564, 742, 661]}, {"image_id": 4, "file_name": "3163_04.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 284], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Overview of region extraction: (a) given a tensor \ufb01eld de\ufb01ned on a surface of triangular mesh, (b) each edge of the triangles is visited to identify possible intersections (white disks) with region boundaries, (c) lines with two coinciding solutions (magenta) are detected within the triangle, (d) these lines partition the triangle such that boundary curve resides completely on one side of the line. The boundary can now be traced from one intersection to the other without suffering from the numerical errors, (e-f) regions are constructed within the triangle, (g) and \ufb01nally, a merging process is performed to consolidate adjacent subregions with the same type into a single region on the surface. ", "caption_bbox": [28, 288, 742, 357]}, {"image_id": 5, "file_name": "3163_05.png", "page": 5, "dpi": 300, "bbox": [27, 662, 745, 951], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                (a)                  (b)                  (c)                                                                            Once the eigenvector and eigenvalue graphs have been constructed Fig. 6: An example of an internal elliptic loop. To detect this loop, we   from the input data, we convert them to a multi-scale data structure. As partition the triangle using line segments that contain repeated solutions in the case of multi-resolution mesh representation [13] and multi-scale to the region boundary equations (a). Next, We trace the boundaries        scalar \ufb01eld topology [4], we need to address the following fundamen- and extract the region.                                                    tal questions in order to enable multi-scale asymmetric tensor \ufb01eld ", "caption_bbox": [28, 950, 745, 1015]}, {"image_id": 6, "file_name": "3163_06.png", "page": 6, "dpi": 300, "bbox": [439, 50, 721, 228], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: This \ufb01gure shows the two operations needed for the eigenvector graph simpli\ufb01cation (a) region annexation and (b) region connection. ", "caption_bbox": [404, 240, 754, 268]}, {"image_id": 7, "file_name": "3163_07.png", "page": 7, "dpi": 300, "bbox": [391, 413, 745, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: This \ufb01gure demonstrates the three additional operations needed for the eigenvalue graph simpli\ufb01cation (a) region annexation with junc- tion points, (b) region connection of different types, and (c) region split of different types. ", "caption_bbox": [392, 326, 744, 381]}, {"image_id": 8, "file_name": "3163_08.png", "page": 9, "dpi": 300, "bbox": [28, 29, 745, 281], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: An open channel \ufb02ow is visualized. There is a sill at the bed of the channel (c: inside the box). While the sill location is not clear from the visualization of the \ufb02ow velocity gradient tensor at the water surface (a), it becomes more clear after applying our multi-scale analysis to the original \ufb01eld (result shown in (b)). Notice the persistent large patches of upwelling (yellow, \ufb02ow expansion at the water surface) and downwelling (blue, \ufb02ow contraction at the water surface) around the sill. ", "caption_bbox": [27, 291, 742, 346]}], "3164": [{"image_id": 0, "file_name": "3164_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 292], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Our method splits a vector \ufb01eld v(x,t) into two components: a steady \ufb02ow w(x,t) and the ambient motion f(x,t). Feature curves such as vortex corelines (green), bifurcation lines (yellow) and their separating surfaces (blue and red in left image), can be extracted via streamline-oriented topology in w, while the motion of these features (orange and blue in right image) is extracted as pathsurfaces in f. As context, we show purple streamlines in w(x,t) (left) and purple pathlines in v(x,t) (right). ", "caption_bbox": [61, 299, 710, 357]}, {"image_id": 1, "file_name": "3164_01.png", "page": 3, "dpi": 300, "bbox": [28, 29, 745, 213], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Overview of our unsteady vector \ufb01eld topology extraction. First, a reference frame optimization is performed to split the input \ufb02ow v(x,t) into a \ufb02ow in a steady reference frame w(x,t) and the ambient motion f(x,t). From the slices of w(x,t), we extract the classic streamline-based vector \ufb01eld topology and features, while the paths of critical points are given as pathlines in the ambient \ufb02ow f(x,t). ", "caption_bbox": [27, 217, 743, 261]}, {"image_id": 2, "file_name": "3164_02.png", "page": 6, "dpi": 300, "bbox": [391, 29, 757, 403], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Comparison of our displacement optimization with previous local [13] and global [16] approaches in space-time in the B OUSSINESQ \ufb02ow. To con\ufb01rm the correctness of the vortex corelines, particles are released on them and are integrated forward and backward. If a vortex coreline is a pathline, particles stay close and swirl around it. Our method has the lowest error and \ufb01nds more vortices than the other two. ", "caption_bbox": [404, 407, 754, 490]}, {"image_id": 3, "file_name": "3164_03.png", "page": 7, "dpi": 300, "bbox": [27, 345, 380, 448], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: Comparison of local and integration-based bifurcation line extractors. The ground truth line is shown in yellow. The slices show the scalar or vector \ufb01eld, in which the lines are extracted. Note that our local approach \ufb01nds the ground truth coreline best, closely followed by the FTLE intersection, which depends on the integration length. ", "caption_bbox": [28, 452, 378, 521]}, {"image_id": 4, "file_name": "3164_04.png", "page": 7, "dpi": 300, "bbox": [391, 330, 738, 518], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Comparisons of af\ufb01ne invariance, our third-order displacement transformation, and the streakline-based topology of Sadlo et al. [40] with bifurcation lines (yellow) and vortex corelines (green). The af\ufb01ne- invariant approach cannot \ufb01t the transformation perfectly and thus \ufb01nds arti\ufb01cial bifurcation lines from which separatrices do not grow as from a real saddle. Purple lines show pathlines, which do not cross our separatrices. In the last image, our displacement-optimized separatrices are visualized transparently, showing that our method \ufb01nds the complete surfaces. The LIC visualizes the \ufb02ow in the respective optimal frame. ", "caption_bbox": [392, 186, 744, 310]}, {"image_id": 5, "file_name": "3164_05.png", "page": 8, "dpi": 300, "bbox": [456, 293, 704, 460], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13: Example in the H ALF C YLINDER \ufb02ow. Top: critical points, vortex corelines (green), bifurcation lines (yellow), and the complex separatring surfaces of one bifurcation line (red/blue). Bottom: the ambient motion f(x,t) of the feature curves and pathlines in v(x,t). ", "caption_bbox": [404, 463, 756, 518]}, {"image_id": 6, "file_name": "3164_06.png", "page": 8, "dpi": 300, "bbox": [391, 29, 758, 197], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12: Space-time visualization of the P IPED C YLINDER. Left: separating surfaces, bifurcation lines (yellow) and vortex corelines (green) are extracted in the two vortex streets. The second street is one-sided. The two standing vortices trap the \ufb02ow behind the corners. Right: in a close-up with many pathlines, vortical motion around the vortex corelines and the ordering behavior of separatrices is visible. ", "caption_bbox": [404, 200, 756, 283]}, {"image_id": 7, "file_name": "3164_07.png", "page": 8, "dpi": 300, "bbox": [64, 180, 358, 414], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Vortex corelines (green) and bifurcation lines (yellow) with separating surfaces (red and blue) in the space-time domain of the C YLINDER 2D \ufb02ow. As context, pathlines were released (purple). ", "caption_bbox": [40, 415, 390, 456]}, {"image_id": 8, "file_name": "3164_08.png", "page": 8, "dpi": 300, "bbox": [40, 29, 394, 147], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: If the magnitude of a vector \ufb01eld v changes in time, the optimal frame w contains a sink/source. The ambient motion f points out/in. ", "caption_bbox": [40, 146, 390, 177]}], "3165": [{"image_id": 0, "file_name": "3165_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 397], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Using traditional physical artistic media as input to the digital visualization pipeline provides a richer visual vocabulary and opens the door for artists to participate in creating more expressive and engaging 3D scienti\ufb01c visualizations. This example helps scientists understand commercially viable macroalgae growth in the Gulf of Mexico by encoding temperature and salinity from remote sensing together with eddy direction and curvature and three nitrate concentrations from computational simulation. ", "caption_bbox": [61, 410, 710, 463]}, {"image_id": 1, "file_name": "3165_01.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 353], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The ABR pipeline contains three main stages.", "caption_bbox": [392, 367, 653, 381]}, {"image_id": 2, "file_name": "3165_02.png", "page": 4, "dpi": 300, "bbox": [39, 29, 394, 313], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Artists will recognize the formal properties of (point, line, form, texture, and color) in these visual examples. Visualization scientists will recognize magnitude channels to encode ordered data and identify channels to encode categorical data. The volume category focuses on color schemes for volume rendering algorithms. ", "caption_bbox": [39, 326, 391, 393]}, {"image_id": 3, "file_name": "3165_03.png", "page": 4, "dpi": 300, "bbox": [390, 29, 758, 309], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. The Color Loom applet. Artists drag and drop source images into the left panel and pull swatches of color from these, which are then copied and arranged in the right panel to create a color map. ", "caption_bbox": [404, 322, 754, 362]}, {"image_id": 4, "file_name": "3165_04.png", "page": 4, "dpi": 300, "bbox": [404, 378, 757, 543], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The Texture Shaper applet. A: Original source images, B: Select- ing a cropping box; C: Output images and normal maps. ", "caption_bbox": [404, 557, 756, 584]}, {"image_id": 5, "file_name": "3165_05.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 247], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. The Glyph Aligner applet. Artists use trackball controls in the left panel to align a 3D scanned glyph. The right panel provides a glyph \ufb01eld preview using synthetic data. ", "caption_bbox": [392, 260, 742, 300]}, {"image_id": 6, "file_name": "3165_06.png", "page": 6, "dpi": 300, "bbox": [40, 410, 758, 596], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Options for applying vis assets to lines or surfaces include (a) of textures in the corresponding texture set. Blending can be applied in color mapping, (b) data-driven texturing, (c) data-driven texturing with the fragment shader to hide texture seams, with a user-de\ufb01ned blend bump mapping, (d) data-driven texturing with blending and masking, (e)   distance. Fig. 9b-e shows speci\ufb01c examples of how textures are binned data-driven texturing with masking to create an organic line pro\ufb01le.     and blended. ", "caption_bbox": [40, 595, 755, 648]}, {"image_id": 7, "file_name": "3165_07.png", "page": 6, "dpi": 300, "bbox": [39, 29, 758, 356], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Example renderings and parameters for vis layers in the ABR rendering engine.", "caption_bbox": [40, 371, 465, 385]}, {"image_id": 8, "file_name": "3165_08.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 246], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Process and results from the internal exploratory design study with ABR on the biogeochemistry data in the Gulf of Mexico [83], left to right: pre-made glyphs; glyphs painted during the study; glyphs constructed during the study; textures captured pre-study; detail of \ufb01nal visualization; visualization of the Gulf of Mexico. ", "caption_bbox": [27, 259, 743, 299]}, {"image_id": 9, "file_name": "3165_09.png", "page": 8, "dpi": 300, "bbox": [39, 29, 758, 265], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. Visualizing brain microstructure in 3D. A: Straightforward extension on slice-based methods to 3D. B: ABR designed visualization with high-resolution data in the centrum semiovale. ", "caption_bbox": [40, 278, 754, 305]}], "3166": [{"image_id": 0, "file_name": "3166_00.png", "page": 1, "dpi": 300, "bbox": [49, 65, 749, 437], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Left: Our experiments in VR with homogeneous and heterogeneous distractors, as we investigate the preattentiveness and robustness of Deadeye in such scenarios. Right: We demonstrate and evaluate volume rendering in VR as a possible real-world application scenario for our technique. ", "caption_bbox": [73, 450, 721, 490]}, {"image_id": 1, "file_name": "3166_01.png", "page": 2, "dpi": 300, "bbox": [28, 29, 745, 261], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. A selection of VR visualizations that can bene\ufb01t from Deadeye highlighting. (a) Educational visualizations of particle physics [14]: Deadeye can be used to capture and guide the attention of the students. (b) Immersive graph visualizations [42]: Utilizing Deadeye during user interaction to highlight the selected vertices and edges. (c) Dinosaur track formation [56]: Emphasizing 3D pathlines of interest in unsteady \ufb02ow visualizations. ", "caption_bbox": [28, 279, 743, 319]}, {"image_id": 2, "file_name": "3166_02.png", "page": 3, "dpi": 300, "bbox": [48, 52, 748, 244], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. (a) Color as a cue: The red circle can be recognized preattentively independent of the number of blue distractors. (b) Da Vinci stereopsis: The near surface results in different occlusions for both eyes, which leads to monocular regions in the far plane. (c) Valid and invalid setups of unpaired image points in da Vinci stereopsis according to Nakayama and Shimojo [53]. Images redrawn from [5] and [39]. ", "caption_bbox": [40, 261, 754, 301]}, {"image_id": 3, "file_name": "3166_03.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 340], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Top: Exp-1 with an increasing number of homogeneous distractors on the same depth plane with a slight positional jittering applied to each cube. Bottom: Exp-2 with heterogeneous distractors, from left to right: depth2 (two depth planes), depth2-color-shape (two depth planes, different color and shape), and depth3-color (three depth planes, different color). ", "caption_bbox": [28, 354, 743, 396]}, {"image_id": 4, "file_name": "3166_04.png", "page": 5, "dpi": 300, "bbox": [49, 52, 748, 301], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Average accuracies for Exp-1 and Exp-2 compared to our previous results [39] for the 2D case. A repeated measures ANOVA shows no signi\ufb01cant difference in accuracy between the sets, supporting our hypotheses that the transition to 3D scenes does not impact the performance of Deadeye and that our technique remains robust even in the presence of strong visual cues such as color or shape. ", "caption_bbox": [40, 315, 754, 355]}, {"image_id": 5, "file_name": "3166_05.png", "page": 5, "dpi": 300, "bbox": [40, 737, 759, 976], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Excerpt from our VR implementation of the NASA-TLX survey. All objects: M = 0.91, SD = 0.07) and Exp-2 (depth2: M = 0.93, SD = brie\ufb01ngs, pauses, and questionnaires were done via the HMD to provide  0.08; depth2-color-shape: M = 0.91, SD = 0.06; depth3-color: M = same conditions to all participants.                                   0.90, SD = 0.06) are similar to the values of other preattentive cues ", "caption_bbox": [40, 975, 754, 1017]}, {"image_id": 6, "file_name": "3166_06.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 329], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Results of the NASA-TLX survey for Exp-1 and Exp-2 compared to the prior values of the 2D case. Lower values are preferable.", "caption_bbox": [28, 345, 690, 360]}, {"image_id": 7, "file_name": "3166_07.png", "page": 7, "dpi": 300, "bbox": [44, 57, 758, 269], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Average success rates for the detection of a Deadeye-enhanced        manually via direct e-mail requests to limit the discard rate. The exper- object at each position. The matrices indicate an increase of the error rate iment took place in our VR lab and was supervised by two researchers. with increasing distance from the focus point and no notable difference      One of them was responsible for brie\ufb01ng and interviewing the partic- regarding the target\u2019s depth location.                                       ipant during the experiment. All participants agreed that we would ", "caption_bbox": [40, 268, 757, 326]}], "3167": [{"image_id": 0, "file_name": "3167_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 383], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: A procedurally generated, real-time rendered model of a microtubule assembly inside a cell cytoplasm.", "caption_bbox": [109, 393, 661, 407]}, {"image_id": 1, "file_name": "3167_01.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 240], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Components of the pipeline for generating multi-scale visualizations of microtubules and their dynamics. First, the \ufb01ber generator lays out the spatial arrangement of the microtubules. This is done by generating a set of control points for each microtubule, which are then interpolated by a cubic spline. Afterwards, models of tubulin molecules are placed along each \ufb01ber in the microtubule generator. Subsequently, the cap generator displaces the tubulin molecules so that a microtubule cap is created. Finally, the model is displayed by a molecular renderer. Each of the modules has its own set of parameters. The blue box marks the modules belonging to the procedural microtubule model. ", "caption_bbox": [28, 254, 743, 323]}, {"image_id": 2, "file_name": "3167_02.png", "page": 6, "dpi": 300, "bbox": [404, 788, 757, 947], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: (a) A microtubule generated along a straight line. (b) A low- frequency noise applied to the microtubule to produce a slight random bending. (c) A high-frequency noise applied to the microtubule to produce the state before the microtubule assembles. ", "caption_bbox": [404, 960, 756, 1015]}, {"image_id": 3, "file_name": "3167_03.png", "page": 7, "dpi": 300, "bbox": [27, 353, 381, 961], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: The interpolation between the (a) assembled and (c) disassem- bled states of a microtubule. The cap\u2019s middle part (b) is formed by the molecules currently undergoing the interpolation due to a speci\ufb01c IDT. ", "caption_bbox": [28, 974, 380, 1015]}, {"image_id": 4, "file_name": "3167_04.png", "page": 7, "dpi": 300, "bbox": [392, 873, 744, 964], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Four frames of the transition between the growing and the shrinking cap of the microtubule. This is achieved by interpolating two distinct IDTFs. ", "caption_bbox": [392, 974, 742, 1015]}, {"image_id": 5, "file_name": "3167_05.png", "page": 7, "dpi": 300, "bbox": [27, 29, 745, 282], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: (a) A microtubule produced by the microtubule generator. (b) A growing microtubule cap. (c) The growing cap with the free tubulins removed. (d) A shrinking microtubule cap. (e) The shrinking cap with the free tubulins removed. ", "caption_bbox": [28, 295, 743, 323]}, {"image_id": 6, "file_name": "3167_06.png", "page": 9, "dpi": 300, "bbox": [27, 29, 745, 207], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13: Visualizations generated by our pipeline. (a) shows the growing end of a microtubule, (b) microtubule with GDP-bound \u03b2 -tubulins highligthed in blue, and (c) shows the disassembly of a microtubule. ", "caption_bbox": [28, 217, 742, 248]}], "3168": [{"image_id": 0, "file_name": "3168_00.png", "page": 1, "dpi": 300, "bbox": [73, 65, 724, 318], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Using OpenSpace to depict astronomical phenomen\u00e6 at different spatial scales. Visualization of the Apollo service module in front of Earth (left), volumetric rendering of a solar wind density simulation in the heliosphere (center), exploring galaxy clusters using data from the Sloan Digital Sky Survey including the missing data due to the shadow of the Milky Way (right). ", "caption_bbox": [73, 324, 722, 364]}, {"image_id": 1, "file_name": "3168_01.png", "page": 2, "dpi": 300, "bbox": [27, 29, 393, 273], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. An interactive visualization session with astrophysics domain experts presenting the 1.3 billion stars of the Gaia DR2 data set during the 2018 NYC Gaia Sprint at the Hayden planetarium in New York. ", "caption_bbox": [28, 278, 378, 318]}, {"image_id": 2, "file_name": "3168_02.png", "page": 2, "dpi": 300, "bbox": [391, 29, 745, 273], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Using OpenSpace as a tool for astronomy research, here in the use case of visualizing a time-varying coronal mass ejection simulation, combining a volumetric rendering and \ufb01eldlines. ", "caption_bbox": [392, 278, 743, 318]}, {"image_id": 3, "file_name": "3168_03.png", "page": 2, "dpi": 300, "bbox": [392, 349, 746, 972], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. The New Horizons spacecraft \ufb02y-by of Pluto in 2015 at around 20000 km (a) and its \ufb02y-by of MU69 (Ultima Thule) at about 6500 km in 2019. Pluto is about 2400 km in diameter, Ultima Thule is 31 km. ", "caption_bbox": [392, 977, 742, 1017]}, {"image_id": 4, "file_name": "3168_04.png", "page": 3, "dpi": 300, "bbox": [40, 57, 757, 185], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The system architecture is divided into four layers. Arrows denote dependencies and rounded rectangles indicate sections in which the respective subsystems are described in detail. OpenSpace consists of a core component, multiple, each optional, modules which are combined to build an executable application. ", "caption_bbox": [40, 196, 755, 236]}, {"image_id": 5, "file_name": "3168_05.png", "page": 4, "dpi": 300, "bbox": [27, 29, 746, 978], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Loading of display con\ufb01guration and the asset graph involves a  the code and can be used by future dependent modules. Another exam- scripting language (Lua), which enables a simple, yet powerful, runtime ple is the Webbrowser module that includes the Chromium Embedded con\ufb01guration of the scene graph.                                        Framework used to render OpenSpace\u2019s user interface (Section 4.2.5). ", "caption_bbox": [28, 977, 745, 1017]}, {"image_id": 6, "file_name": "3168_06.png", "page": 6, "dpi": 300, "bbox": [27, 29, 745, 214], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. A selection of display devices that are supported by OpenSpace use of the Simple Graphics Cluster Toolkit. Regular laptop and desktop systems with arbitrary number of displays (a), large-scale touch tables (b), and planetarium environments (c). In all three cases, the same executable is used with different con\ufb01guration \ufb01les to generate the images. ", "caption_bbox": [28, 219, 742, 259]}, {"image_id": 7, "file_name": "3168_07.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 279], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Visualization of the excursions of the Apollo 17 astronauts using the Lunar Roving Vehicle in the context of satellite imagery of the landing site from Lunar Reconnaissance Orbiter. ", "caption_bbox": [392, 284, 742, 324]}], "3169": [{"image_id": 0, "file_name": "3169_00.png", "page": 1, "dpi": 300, "bbox": [72, 65, 725, 496], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Novel representation of electrostatic interaction energy (in the middle) helps navigating the simulation by spatially resolving contributions to the integral measure traditionally represented by a timeseries (blue line in the bottom). It captures different modes of the simulation better than the timeseries and helps localizing changes in the con\ufb01guration of the simulation by providing spatial context. The multiscale behaviour of the data is explored by seamlessly navigating the spatiotemporal scale-space. The selected timestep shows the ligand escaping from the tunnel, before it is sucked back in by the protein. ", "caption_bbox": [72, 506, 721, 575]}, {"image_id": 1, "file_name": "3169_01.png", "page": 4, "dpi": 300, "bbox": [392, 419, 737, 685], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Spherically (black) and cylindrically (blue) projected Gaussians compared with pure Gaussian kernels (red crosses) centred at positions uniformly sampling the radial axis. We can observe that the projection takes care of the boundary condition when r \u2192 0. Each kernel that would extend into the negative domain (r < 0) gets \u201cpushed out\u201d such that it zeros out at r = 0. ", "caption_bbox": [392, 695, 743, 780]}, {"image_id": 2, "file_name": "3169_02.png", "page": 5, "dpi": 300, "bbox": [93, 52, 704, 265], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Overview of the temporal scale-space construction. First row: the integral measure F(\u00b7) is evaluated for each frame of the particle data, yielding a time series f (t), which is then convolved with a set of Gaussian kernels, resulting in a traditional scale-space; Second row: A point location of interest is identi\ufb01ed; the spatial domain is decomposed into shells; the integral measure is evaluated for each region giving a set of time series; the scale-space construction is repeated for each and the results are stacked into the SSST cube. ", "caption_bbox": [39, 275, 756, 330]}, {"image_id": 3, "file_name": "3169_03.png", "page": 6, "dpi": 300, "bbox": [392, 786, 744, 922], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: The slice view: a pixelmap showing a spatio-temporal slice of the SSST cube with horizontal time axis and vertical spatial axis. The green cross consists of a horizontal line for navigation in the spatial axis and a vertical line for navigation along the time axis. A divergent colourmap is used to encode the values. The spatial scale is set to \u03c3 = 1.62 A\u030a and temporal scale is shared with the scale view \u03c4 = 2.1. ", "caption_bbox": [392, 932, 743, 1017]}, {"image_id": 4, "file_name": "3169_04.png", "page": 7, "dpi": 300, "bbox": [404, 52, 757, 179], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: An example of the 3D view from the molecular dynamics simu- lation showing the peeling of spatial scales around the point of interest. The particles in the selected region are coloured based on their contri- bution to the integral measure. The atoms are coloured based on their contribution to the integral measure and the colourmap is shared with the slice view. The ligand representing the point of interest is in green. ", "caption_bbox": [404, 189, 757, 272]}, {"image_id": 5, "file_name": "3169_05.png", "page": 9, "dpi": 300, "bbox": [40, 52, 757, 350], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: The analysis of behaviour of ligand no. 5 at large spatial scale. The two most prominent features in the slice view (middle) are the shear towards the beginning of the simulation (a,b) and the abrupt modality change towards the end (1,2,3). The shear is caused by movement of the ligand in the binding pocket. Protein is not shown to avoid occlusion, instead the movement is referenced by the plane that ligands lie in. The shear follows by energy minima which is largely contributed by the protein atoms located at a distance between 16.6 - 22.2 A\u030a (c). The change of modality (1,2,3) results after a period of unstable behaviour (d) and is resolved as ligand\u2019s rotation (the arrows indicate the ligand\u2019s orientation). ", "caption_bbox": [39, 360, 754, 429]}], "3170": [{"image_id": 0, "file_name": "3170_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 246], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Steps along the voyage into the genomic detail as enabled by ScaleTrotter, showing the semantic scale levels. ScaleTrotter and its visual embedding allow us to seamlessly transition between independent representations and interactively explore them. ", "caption_bbox": [61, 254, 709, 281]}, {"image_id": 1, "file_name": "3170_01.png", "page": 4, "dpi": 300, "bbox": [391, 29, 758, 161], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Plates 1013, 1048, and 1216 from Gray\u2019s Anatomy [20], demon- strate layered composition of multi-scale 3D objects by traditional illustra- tors. The images are in the public domain p. ", "caption_bbox": [404, 169, 756, 211]}, {"image_id": 2, "file_name": "3170_02.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 438], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Sequence of visual scale embeddings, based on the data levels.", "caption_bbox": [392, 449, 743, 463]}, {"image_id": 3, "file_name": "3170_03.png", "page": 5, "dpi": 300, "bbox": [28, 29, 394, 239], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Visual embedding, schematic principle.", "caption_bbox": [28, 238, 256, 252]}, {"image_id": 4, "file_name": "3170_04.png", "page": 5, "dpi": 300, "bbox": [27, 262, 380, 418], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Two snapshots of scale transition views, (a) between the chro- mosome and the detailed chromosome scales, as well as (b) between nucleotides and detailed nucleotides scales. ", "caption_bbox": [28, 424, 380, 464]}, {"image_id": 5, "file_name": "3170_05.png", "page": 6, "dpi": 300, "bbox": [40, 29, 757, 155], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                     (a) Fig. 7. Scale-dependent hierarchy scope realized for nucleosomes by showing \ufb01ve \ufb01ber locations around the focus and fading out the ends. ", "caption_bbox": [40, 154, 420, 188]}], "3171": [{"image_id": 0, "file_name": "3171_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 315], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: We propose two orthogonal strategies to accelerate the rendering of Monte Carlo-based visualizations of \ufb01nite-time Lyapunov exponent \ufb01elds: (1) gradient domain rendering for single-scattered participating medium and (2) an acceleration of the light transmittance estimator. For the latter, we propose (2a) a joint estimator that combines ratio tracking [38] with a pre-computed Fourier approximation of the transmittance and (2b) a pure Fourier approximation of the transmittance. The above visualizations compare the method of Gu\u0308nther et al. [17] with our two new approaches after rendering for 75 seconds for the M IXER data set. ", "caption_bbox": [61, 326, 710, 395]}, {"image_id": 1, "file_name": "3171_01.png", "page": 3, "dpi": 300, "bbox": [26, 220, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Illustration of single-scattered light transport in participating   mator of the transmittance Tr (xs \u2194 xL ) from the scattering point xs to medium showing the entry and exit distance dmin and dmax , the view         the light source xL . The simplest option is to compute the expected                  ) and a real scattering event (blue) found at xs , where a ray (direction \u03c9                                                            value of a binary experiment, where photons are repeatedly sent using transmittance estimate Tr (xs \u2194 xL ) connects to the light point xL .       delta tracking to test if they reach the scattering point (rated 1) or are ", "caption_bbox": [28, 162, 743, 220]}, {"image_id": 2, "file_name": "3171_02.png", "page": 5, "dpi": 300, "bbox": [27, 29, 746, 951], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                         P                                                                      frequency bands (a0 and ai , bi with i \u2208 {1, . . . , 13}), requires 7 four- Fig. 3: The Fourier approximation of transmittance \u03c4(d) is computed  component variables (using 31 \ufb02oats). We always used a resolution of over the domain [0, P] along the light ray, where d = 0 is the entry 512 \u00d7 512 pixels for the Fourier maps, which we found to be suf\ufb01cient. ", "caption_bbox": [28, 950, 745, 993]}, {"image_id": 3, "file_name": "3171_03.png", "page": 6, "dpi": 300, "bbox": [407, 53, 757, 348], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Comparison of our Fourier transmittance estimator for different numbers of frequency bands. All images were computed with 100 iterations of gradient domain in the ECMWF \ufb02ow. We also list Root- mean-square error (RMSE) and structural similarity index (SSIM). ", "caption_bbox": [404, 353, 756, 408]}, {"image_id": 4, "file_name": "3171_04.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 895], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Equal-time comparisons for 360 sec in the WALL -M OUNTED C YLINDER \ufb02ow (left) and for 680 sec in the CTBL \ufb02ow (right). From top to bottom: ground truth image with gradient domain (1k iterations, left) and ratio (10k iterations, right), delta tracking as used by Gu\u0308nther et al. [17], gradient domain rendering applied to delta tracking, the ratio tracking of Nova\u0301k et al. [38], our joint ratio-Fourier tracking with gradient domain rendering and a pure Fourier approximation with gradient domain rendering. We list root-mean-square error (RMSE) and structural similarity index (SSIM). In the WALL -M OUNTED C YLINDER, our methods visibly reduce the noise with little bias. In the CTBL, our joint method shows only a small improvement. The pure Fourier approximation, however, reduces the noise successfully, but also contains a visible bias in the illumination. Since all FTLE structures can still be perceived well, this bias is an acceptable trade-off for the reduced noise. ", "caption_bbox": [28, 905, 743, 1002]}, {"image_id": 5, "file_name": "3171_05.png", "page": 9, "dpi": 300, "bbox": [18, 29, 746, 362], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                     applicable in the \ufb01nal connection of a path to the light source. Fig. 8: Two frames of the A STEROID time animation (top), the camera \ufb02ight around the CTBL simulation (middle) and a slicing animation      Other Function Approximations. We followed Jansen and through the C LOUD E VOLUTION (bottom), all using 200 it per frame.  Bavoil [24] in their choice to Fourier-approximate an extinction signal, ", "caption_bbox": [28, 362, 744, 413]}], "3172": [{"image_id": 0, "file_name": "3172_00.png", "page": 2, "dpi": 300, "bbox": [391, 29, 745, 205], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Instantaneous \ufb02ow \ufb01eld represented through the \u03bb2 = \u2212150 isocontours. Vortices are colored by the velocity magnitude. ", "caption_bbox": [392, 224, 742, 255]}, {"image_id": 1, "file_name": "3172_01.png", "page": 2, "dpi": 300, "bbox": [391, 568, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Cut plane at z = 0.02cax showing the time-averaged \ufb02ow. The color scale indicates the magnitude of the mean \ufb02ow and superimposed arrows indicate the direction on the plane. Superimposed are 3D stream- lines, which start from a straight line at a wall distance of z = 0.02cax in front of the pro\ufb01le and which are colored by the time-averaged vertical (z) velocity, denoted as wm in the legend. ", "caption_bbox": [392, 446, 744, 532]}, {"image_id": 2, "file_name": "3172_02.png", "page": 3, "dpi": 300, "bbox": [440, 66, 721, 175], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: (a) Localized surface characterization scheme employed in our method. The blue trajectory shows a path line terminated under the condition stated in Equation 1. With the indicated main \ufb02ow direction (purple) and the attachment \ufb02ow trajectories (green), the pathline (blue) seeded at the offset surface (red) is sampled for the computation of the spatial gradient. (b) This image shows the tensor glyph of the strain of an isotropic \ufb02uid volume in a reference con\ufb01guration, after advection, and subsequently projected onto the surface. Note that the eigenvalues and eigenvectors vary in every step. Images source: [26] ", "caption_bbox": [404, 188, 756, 312]}, {"image_id": 3, "file_name": "3172_03.png", "page": 4, "dpi": 300, "bbox": [27, 205, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Illustration of the domain transformation applied in this study. proportionally to the integration time. It shows the transformations J and \u03a6 between a valid curvilinear grid (physical space) and the uniform grid (computational space). Examples for invalid grids are also shown (left), both containing grid lines with instead of using \u03be , x is used, except for where otherwise noted. The substantial non-orthogonality with respect to the wall.                  transformation matrix J(x) is computed for every grid point using the ", "caption_bbox": [27, 137, 742, 206]}, {"image_id": 4, "file_name": "3172_04.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 100], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Color mapping scheme for (a) splats and (b) antisplats employed in our structural visualization. The saturation of the coloring increases proportionally to the integration time. ", "caption_bbox": [392, 110, 742, 151]}, {"image_id": 5, "file_name": "3172_05.png", "page": 5, "dpi": 300, "bbox": [422, 52, 740, 155], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: Distribution of slopes of all trajectories. Top: Histogram with the number of slopes occurring at time t. Bottom: Distribution of slopes separated by the changes of a negative to a positive splat candidate evaluation \u201d+\u201d and the change from a positive to a negative splat candidate evaluation \u201d\u2212\u201d. ", "caption_bbox": [405, 165, 755, 234]}, {"image_id": 6, "file_name": "3172_06.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 186], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Splat and antisplat (circled) visualization performed on the surface of the pressure side in the mean \ufb02ow u\u0304(x). The side wall is located at the rear side of the visualized blade. ", "caption_bbox": [392, 196, 742, 237]}, {"image_id": 7, "file_name": "3172_07.png", "page": 6, "dpi": 300, "bbox": [27, 310, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Color map visualizing difference d de\ufb01ned in Equation 11 in the   located at the rear side of the visualized blade. computational space of the (offset) side wall. The coloring scheme is as follows. d(x) = 0 : yellow. d(x) = 1 : green. As the sum of distances decreases with increasingly small \u0394t , we highlight the regions including \u03be j , with j \u2208 {1, 2, 3} of the structured grid employed, so that the offset a point with d = 1 with a red circle. According to Equation 11 the        boundaries are conveniently chosen to be slices of an index \u03be j directly following values are shown here. (top): \u0394t1 = 0.01 and \u0394t2 = 0.001.       mapping to computational space. ", "caption_bbox": [28, 224, 743, 311]}, {"image_id": 8, "file_name": "3172_08.png", "page": 7, "dpi": 300, "bbox": [83, 52, 714, 368], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: Visualization result of the splat detection approach at the surface of the suction side using the \ufb02uctuation \ufb01eld u (x). (a),(b), and (c) are from the in\ufb02ow perspective and (d), (e), and (f) are from the out\ufb02ow perspective. (a), (d) 2D visualization of detected splat and antisplat regions as footprints in the offset plane. 3D visualizations (b) and (e) were generated using the forward integration time t = 0.3 and t = 1.0 for splats and antisplats respectively. The backward integration time is t = 1.0 and t = 0.3 for splats and antisplats respectively. 3D visualizations (c), (f) were generated analogously with integration times t = 4.0 instead of t = 1.0. The x-direction is indicated by the red arrow of the orientation axes. The green arrow indicates the y-direction. The side wall is located at the rear side of the visualized blade. ", "caption_bbox": [40, 375, 755, 461]}, {"image_id": 9, "file_name": "3172_09.png", "page": 8, "dpi": 300, "bbox": [27, 763, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12: Visualization result of the splat detection approach at the      (iii) How much is the \ufb02ow near the side walls altered with respect surface of the side wall using the \ufb02uctuation \ufb01eld u (x). (a) 2D visu- to the \ufb02ow far from the side walls by secondary \ufb02ows, and what are the ", "caption_bbox": [28, 731, 742, 764]}, {"image_id": 10, "file_name": "3172_10.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 403], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Visualization result of the splat detection approach on the pressure side using the \ufb02uctuation \ufb01eld u (x). (a),(b), and (c) are from the in\ufb02ow perspective and (d), (e), and (f) are from the out\ufb02ow perspective. (a), (d) 2D visualization of detected splat and antisplat regions. 3D visualizations (b) and (e) are generated using the forward integration time t = 0.3 and t = 1.0 for splats and antisplats respectively. The backward integration time is t = 1.0 and t = 0.3 for splats and antisplats respectively. 3D visualizations (c), (f) were generated analogously with integration times t = 4.0 instead of t = 1.0. Note that the side wall is located at the rear side of the visualized blade. ", "caption_bbox": [27, 410, 742, 484]}, {"image_id": 11, "file_name": "3172_11.png", "page": 8, "dpi": 300, "bbox": [392, 509, 745, 639], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13: Visualization of \u03bb2 = \u22122000 isocontours (yellow). (a) Eval- uation for the mean \ufb02ow u\u0304(x). (b) Evaluation based on the velocity \ufb02uctuations u (x), (c) the same isosurface as (b), viewed from a differ- ent angle. ", "caption_bbox": [392, 646, 744, 704]}], "3173": [{"image_id": 0, "file_name": "3173_00.png", "page": 1, "dpi": 300, "bbox": [27, 29, 745, 543], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: We select 1 % from 500,000 data points by random (a) and using our void-and-cluster (d) sampling technique. Our approach chooses a set of samples that uniformly covers the spatial domain, whilst avoiding regularity artifacts, which leads to a blue noise spectrum for our technique (e) in contrast to random sampling (b). Our approach leads to a more accurate reconstruction of the dataset using the same amount of samples (c, f). Furthermore, we have sampled pathlines of the ABC \ufb02ow with our technique (g, h), which implicitly de\ufb01nes a continuous level-of-detail, to render a greater (g) and smaller subset (h) of the trajectories. ", "caption_bbox": [60, 569, 711, 638]}, {"image_id": 1, "file_name": "3173_01.png", "page": 3, "dpi": 300, "bbox": [27, 280, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Overview of the void-and-cluster sampling technique of Ulichney [25] extended to scattered data. After initial random sampling (a), the samples are optimized by \ufb01nding (b) and exchanging (c) the largest void pmin with the tightest cluster smax until pmin = smax . We then iteratively \ufb01nd and add (d) the largest void pmin until we have enough samples. ", "caption_bbox": [28, 208, 743, 251]}, {"image_id": 2, "file_name": "3173_02.png", "page": 4, "dpi": 300, "bbox": [39, 52, 356, 170], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: We compute the entropy of a point in its local neighborhood from a histogram of the value distribution, weighted by the radially symmetric kernel k. ", "caption_bbox": [40, 180, 390, 221]}, {"image_id": 3, "file_name": "3173_03.png", "page": 4, "dpi": 300, "bbox": [404, 52, 757, 178], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: In (a), we sample trajectories that bundle and separate over time. We optimize the distribution of sampled trajectories in (b), by stopping (blue) and starting (red) trajectories in t1 . ", "caption_bbox": [404, 188, 757, 231]}, {"image_id": 4, "file_name": "3173_04.png", "page": 6, "dpi": 300, "bbox": [405, 403, 750, 563], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Comparison of the local error measure after sampling a single time step of the von Ka\u0301rma\u0301n vortex street (left) and the surface-mounted cylinder (right). ", "caption_bbox": [404, 572, 754, 613]}, {"image_id": 5, "file_name": "3173_05.png", "page": 6, "dpi": 300, "bbox": [405, 195, 744, 354], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Comparison of the mean error over all time steps in the von Ka\u0301rma\u0301n vortex street dataset. ", "caption_bbox": [404, 359, 754, 387]}, {"image_id": 6, "file_name": "3173_06.png", "page": 6, "dpi": 300, "bbox": [41, 195, 383, 356], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Left: Comparison of different sampling strategies using our proposed error measure. Right: Error measured during sampling. ", "caption_bbox": [40, 361, 390, 389]}, {"image_id": 7, "file_name": "3173_07.png", "page": 6, "dpi": 300, "bbox": [40, 51, 755, 151], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Reconstruction of the sinc dataset using scattered data interpolation after taking 5000 samples with different strategies.", "caption_bbox": [82, 162, 712, 176]}, {"image_id": 8, "file_name": "3173_08.png", "page": 6, "dpi": 300, "bbox": [40, 404, 388, 522], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: Fourier transform of the sinc dataset after taking 5000 samples using our uniform void-and-cluster method, loose capacity constrained Voronoi diagrams (LCCVD), and Poisson disk sampling. ", "caption_bbox": [39, 533, 389, 574]}, {"image_id": 9, "file_name": "3173_09.png", "page": 7, "dpi": 300, "bbox": [27, 543, 741, 707], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: The surface-mounted cylinder, after sampling 466,103 particles, is shown in (a). We use the continuous level-of-detail, in addition to a transfer function, to further reduce the amount of particles. Slices of the dataset using the uniform (b) and entropy (c) sampling illustrate the difference between the sampling strategies. The entropy strategy samples the less interesting region above the empty cylinder less densely. ", "caption_bbox": [28, 719, 743, 760]}, {"image_id": 10, "file_name": "3173_10.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 463], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: The von Ka\u0301rma\u0301n vortex street after ten time steps using random (a), uniform (b), and entropy (c) void-and-cluster trajectory sampling. The corresponding u-velocity \ufb01elds are shown, which have been created using scattered data interpolation. Our error measure is shown in (d). The entropy void-and-cluster sample distribution in the \ufb01rst time step is illustrated in (e). ", "caption_bbox": [27, 475, 744, 516]}, {"image_id": 11, "file_name": "3173_11.png", "page": 8, "dpi": 300, "bbox": [404, 53, 756, 282], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14: The Dark Sky dataset reduced to 5 % using the uniform void- and-cluster technique (a). A slice of the dataset is shown in (b), with the corresponding slice from the original dataset in (c). ", "caption_bbox": [404, 289, 756, 330]}, {"image_id": 12, "file_name": "3173_12.png", "page": 8, "dpi": 300, "bbox": [39, 66, 388, 192], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13: Slices of the \ufb01nite-time Lyapunov exponent (FTLE) from the ABC \ufb02ow. The reference, computed on all trajectories, is shown in (a). We have computed the FTLE after sampling 10 % of the trajectories by random sampling (b) and using the uniform void-and-cluster (c) technique. ", "caption_bbox": [39, 201, 392, 270]}, {"image_id": 13, "file_name": "3173_13.png", "page": 9, "dpi": 300, "bbox": [29, 279, 746, 451], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 16: We evaluate the scalability of our algorithm using differently strati\ufb01ed sampling, the single-threaded CPU implementation is con- sized sinc datasets, whilst always sampling 10 %.                       siderably slower. This highlights the bene\ufb01ts of parallelization and ", "caption_bbox": [28, 450, 745, 483]}, {"image_id": 14, "file_name": "3173_14.png", "page": 9, "dpi": 300, "bbox": [28, 29, 745, 226], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15: The sampling performance on the von Ka\u0301rma\u0301n vortex street, the surface-mounted cylinder, the ABC \ufb02ow, and the Dark Sky dataset.", "caption_bbox": [34, 238, 735, 252]}], "3174": [{"image_id": 0, "file_name": "3174_00.png", "page": 1, "dpi": 300, "bbox": [27, 430, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Computing a 1-center tree as a structural average for an input ensemble of six leaf-labeled merge trees in partial agreement. Top row on the left: six input leaf-labeled merge trees, each visualized by a node-link diagram. Bottom row on the left: each input tree is visualized with original labels in red and updated labels in green. Right: various uncertainty visualizations for the 1-center tree: (a) 1-center tree with leaf labels; (b) variational vertex consistency plot; (c) statistical vertex consistency plot. (d): summary plot that shows the interleaving distance between each input tree and the 1-center tree (denoted as AMT). ", "caption_bbox": [60, 351, 711, 418]}, {"image_id": 1, "file_name": "3174_01.png", "page": 4, "dpi": 300, "bbox": [40, 52, 756, 234], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. An example of a labeled merge tree (left) that permits labels on internal vertices and multiple labels on a single leaf, together with its induced matrix (middle). The solid black vertex is the root, and white vertices are labeled. The tree has four nonroot vertices and two leaves. Right: construction of a labeled merge tree from the ultra matrix that recovers the original tree. ", "caption_bbox": [40, 250, 756, 290]}, {"image_id": 2, "file_name": "3174_02.png", "page": 4, "dpi": 300, "bbox": [404, 304, 756, 423], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. (a) A 2D scalar \ufb01eld f is generated by a mixture of three Gaussian functions. It is visualized using a rainbow color map: red means high and blue means low values. (b) The graph of f , i.e., the set of all ordered pairs (x, f (x)), is visualized together with the corresponding augmented merge tree of f . (c) A straight line drawing of an (unaugmented) merge tree of f in 3D. ", "caption_bbox": [404, 435, 754, 515]}, {"image_id": 3, "file_name": "3174_03.png", "page": 5, "dpi": 300, "bbox": [26, 430, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Computing a 1-center for a pair of leaf-labeled merge trees in   labeling into complete information so that the algorithm of Sec. 4.1 full agreement. The top row shows two input trees T 1 and T 2 (with      can be utilized. Given an ensemble of trees whose labels do not fully three labeled leaves each) together with the output 1-center T . The     agree, \ufb01nding the best ways to label the remaining vertices boils down middle row displays the induced ultra matrices D 1 and D 2 together with to \ufb01nding the best correspondences under the interleaving distance, their element-wise 1-center, matrix D, which encodes a function f on the which is unfortunately NP-hard [1]. We thus aim to develop a heuristic complete graph K. The bottom row shows K and its sublevel set \ufb01ltration. approach that is ef\ufb01cient and effective in practice for both the partial ", "caption_bbox": [28, 347, 744, 431]}, {"image_id": 4, "file_name": "3174_04.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 269], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Updating the labeling of T i against a pivot tree T p .", "caption_bbox": [392, 281, 682, 297]}, {"image_id": 5, "file_name": "3174_05.png", "page": 6, "dpi": 300, "bbox": [406, 306, 751, 426], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Updating the labeling of T i against a pivot tree T p .", "caption_bbox": [404, 432, 694, 448]}, {"image_id": 6, "file_name": "3174_06.png", "page": 6, "dpi": 300, "bbox": [51, 52, 739, 247], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Computing a 1-center for an ensemble of six leaf-labeled trees in full agreement (top row) and disagreement (botton row). (a, d) 1-center tree; (b, e) variational vertex consistencies plots; (c, f) statistical vertex consistency plots. For the ensemble in disagreement, original labels are omitted; updated labels are green. ", "caption_bbox": [39, 254, 755, 294]}, {"image_id": 7, "file_name": "3174_07.png", "page": 7, "dpi": 300, "bbox": [407, 413, 751, 517], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Left: circular glyphs are used to encode vertex consistencies for an ensemble member. Graduated circular glyphs are used to encode variational (middle) and statistical (right) vertex consistencies for two 1-center trees. ", "caption_bbox": [405, 523, 755, 576]}, {"image_id": 8, "file_name": "3174_08.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 380], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. User interface for the interactive visualization of labeled merge trees and their 1-center.", "caption_bbox": [40, 386, 501, 400]}, {"image_id": 9, "file_name": "3174_09.png", "page": 8, "dpi": 300, "bbox": [407, 827, 746, 988], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. An animated sequence between an ensemble member (a) and a 1-center tree (b) via a geodesic. ", "caption_bbox": [404, 990, 754, 1017]}, {"image_id": 10, "file_name": "3174_10.png", "page": 8, "dpi": 300, "bbox": [438, 52, 720, 281], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Label diagnostics and correction. Given an input ensemble of six partially labeled merge trees, by changing a possibly inaccurate initial label from 3 (b) to 4 (e) in Tree 4, we reduce the structural variation for the 1-center, comparing (c) and (f). ", "caption_bbox": [404, 286, 754, 339]}, {"image_id": 11, "file_name": "3174_11.png", "page": 9, "dpi": 300, "bbox": [391, 29, 745, 218], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14. (a)-(c): Vertex consistencies for each input tree. Leaf labels are inferred based on Euclidean distances among geometric embeddings of vertices. (d) Variational and (e) statistical vertex consistency plot for the 1-center tree. ", "caption_bbox": [392, 220, 742, 273]}], "3175": [{"image_id": 0, "file_name": "3175_00.png", "page": 1, "dpi": 300, "bbox": [49, 65, 749, 510], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Exploration of an ensemble of 500 possible human nucleosome con\ufb01gurations, based on the following components: a) Overview Graph with main application controls; b) 3D View, showing the exploded Complex Con\ufb01guration, amino acids from contact interfaces are colored according to the frequency of their interactions; c) Property View, showing properties of all Complex Con\ufb01gurations; d) Protein View with range \ufb01lter panel; e) Residue Matrix, which also can be switched to Contact Zone List View, and f) Filter View. ", "caption_bbox": [73, 512, 722, 565]}, {"image_id": 1, "file_name": "3175_01.png", "page": 2, "dpi": 300, "bbox": [28, 29, 745, 154], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Structure of a protein complex ensemble containing possible spatial con\ufb01gurations of a protein complex consisting of proteins A, B, and C.", "caption_bbox": [28, 161, 739, 175]}, {"image_id": 2, "file_name": "3175_02.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 312], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Overview Graph showing a protein complex with 8 interacting proteins. The primary Protein Pair Ensemble, related to H2A(C) and H31(A), is highlighted in a darker color, as well as the primary protein H2A(C). ", "caption_bbox": [392, 325, 742, 378]}, {"image_id": 3, "file_name": "3175_03.png", "page": 5, "dpi": 300, "bbox": [40, 902, 757, 977], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Protein View of t-PA protein (primary) with the selected Amino Acid Pairs interacting between LRP protein and the Finger domain of t-PA. The AAs and Amino Acid Pairs that were \ufb01ltered out based on this selection are indicated with white crosses and diagonals. ", "caption_bbox": [40, 990, 755, 1017]}, {"image_id": 4, "file_name": "3175_04.png", "page": 6, "dpi": 300, "bbox": [27, 437, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Contact Zone List View showing the comparison of Amino Acid          Exploded View Once the proteomic experts narrow down the set Pairs from the primary Protein Pair Con\ufb01guration (PPC) (left) with two    of explored Complex Con\ufb01gurations to units of CCs, they want to look other PPCs (middle, right). AAs and Amino Acid Pairs of compared          at the 3D structure of the individual CCs and their interacting Amino PPCs corresponding to the primary PPC are highlighted in green, while     Acid Pairs. To solve the problem of occlusions at the contact interfaces, elements present in primary PPC but missing in compared PPCs are          we adopt the exploded view technique. This technique allows us to shown with empty cells. It can be seen that only the \ufb01rst of the compared explode the proteins such that their relative spatial arrangement is PPCs shares some amino acids (K49, L66, R57) with the primary PPC.        preserved. To enable the identi\ufb01cation of contact interfaces between ", "caption_bbox": [28, 342, 744, 437]}, {"image_id": 5, "file_name": "3175_05.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 224], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. 3D Density Overview of Complex Ensemble. In this case, t-PA protein (blue) was selected as the primary protein. The green and pink isosurfaces then indicate the positions of PAI-1 and LRP proteins w.r.t. the primary protein. We see that while PAI-1 seems to be interacting only with the Protease domain of t-PA, there are some con\ufb01gurations where LRP is interacting with the Finger domain of t-PA. ", "caption_bbox": [392, 237, 744, 317]}, {"image_id": 6, "file_name": "3175_06.png", "page": 7, "dpi": 300, "bbox": [405, 763, 753, 924], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. a) Overview Graph of the tPA \u2013 PAI-1 \u2013 LRP docking results. The small height of the colored bars below the core of each node indicates that the contact interfaces between the proteins are not very stable. b) Property View showing the relationship between the HADDOCK score and Van der Waals energy. The \ufb01ltered out Complex Con\ufb01gurations are depicted in gray. ", "caption_bbox": [405, 937, 755, 1017]}, {"image_id": 7, "file_name": "3175_07.png", "page": 8, "dpi": 300, "bbox": [73, 1016, 707, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Complex Con\ufb01guration with LRP (pink) interacting with basic     To verify the scalability of our solution, we wanted to test it also on residue H261 on PAI-1 (green) helix. It can be seen that the position of an ensemble of a larger protein complex. Therefore, we created an LRP disrupts the interaction between t-PA (blue) and PAI-1.              ensemble of prediction for human nucleosome structure (published ", "caption_bbox": [28, 977, 742, 1017]}, {"image_id": 8, "file_name": "3175_08.png", "page": 8, "dpi": 300, "bbox": [25, 608, 746, 973], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                       the highest scores to compare in the Contact Zone List View and 3D, Fig. 9. 3D Density Overview of the subset of 35 Protein Pair Con\ufb01gura-                                                                        but concluded that the \ufb01rst structure was still the best among them. The tions of PAI-1 (green) with LRP (pink). ", "caption_bbox": [28, 574, 744, 609]}, {"image_id": 9, "file_name": "3175_09.png", "page": 8, "dpi": 300, "bbox": [391, 29, 746, 338], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Exploded view of the Complex Con\ufb01guration with the best binding between t-PA (blue) and LRP (pink) showing the interacting AAs. It can be seen that unlike the case in Figure 10, the interaction between the t-PA and PAI-1 is preserved here. ", "caption_bbox": [392, 352, 744, 405]}], "3176": [{"image_id": 0, "file_name": "3176_00.png", "page": 1, "dpi": 300, "bbox": [73, 65, 724, 489], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 1. Visual computing for cohort-based radiation therapy (RT) prediction. A stylized 3D view of the predicted radiation plan of the current patient is placed centrally; top pale markers (front and back of eyes) receive the least radiation; tumors (black markers) receive the most. Additional RT views show the most similar patients under our novel T-SSIM measure, who contribute to the prediction; the most similar patient is currently highlighted (white) for comparison. A scatterplot (left) shows 4 clusters generated through the T-SSIM measure, with the current (cross) and comparison patient highlighted. A parallel-marker encoding (bottom) shows the predicted (blue cross) per-organ dose distribution within the context of the most similar patients; spatially collocated organs are in contiguous sections of the x-axis. ", "caption_bbox": [73, 503, 722, 596]}, {"image_id": 1, "file_name": "3176_01.png", "page": 4, "dpi": 300, "bbox": [27, 29, 745, 244], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 2. Construction of the spatial similarity measure. (A) A sliding window (a sphere, illustrated in 2D here) steps through the centroids of the organs, to identify nearby organs. (B) Each step in the sliding window constructs a variable-length vector based on the set of nearby organs (e.g., 2 organs in Step 1, 3 in Step 2, 4 in the n step). (C) We create two sets of vectors populated with tumor-organ distances and volumes, respectively, for each patient. These vectors are used as inputs into a similarity function (T-SSIM) to compare two patients. The vectors can be represented in matrix form (Subsection 3.2.1). ", "caption_bbox": [28, 268, 743, 335]}, {"image_id": 2, "file_name": "3176_02.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 312], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 3. Three stylized views of the 3D radiation plan for Patient 152 showing the actual (left), the predicted (center), and the prediction error (right, in blue) in the radiation plan. Circular markers indicate the location of organs at risk, and black markers indicate the tumors. Red luminance is mapped to the radiation dose (higher dose mapped to darker shades) and blue luminance is mapped to error size. Transparent organ models are shown for context. The pale markers at the top correspond to the eyes, and the lowest marker is located down the spine. ", "caption_bbox": [28, 325, 744, 378]}, {"image_id": 3, "file_name": "3176_03.png", "page": 7, "dpi": 300, "bbox": [40, 52, 757, 425], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 4. Two con\ufb01gurations of the scatterplot. The data can be plotted across the principal components of the radiation doses (top), primary                                                                         Figu and secondary tumor volumes (bottom), and principal components of the                                                                         the  distances between each organ and the primary tumor volume (see Fig. 1                                                                         Top ", "caption_bbox": [40, 424, 425, 484]}, {"image_id": 4, "file_name": "3176_04.png", "page": 8, "dpi": 300, "bbox": [27, 29, 745, 470], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 6. Snapshots of key moments during the qualitative evaluation. (A) Picture of the dose-PCA scatterplot on the reduced cohort using the clustering provided by GC. Clusters visibly divide the feature space despite being done without dose information. (B) RT plan for the patient being inspected (shown in (A) as the cross cyan marker, circled here in red). (C) RT prediction error for the patient. Error rates are highest on the left side of the head. (D) Close up of the dose-distribution. One of the matches (highlighted) is signi\ufb01cantly further from the other matches. (E) Parallel-marker dose plot of the patient and its matches. Doses from the suspicious match (highlighted) are signi\ufb01cantly lower for several adjacent areas. (F) Radiation plan of the suspiciously-matched patient, who, despite a similar tumor location, received almost no radiation to the left side of their head. ", "caption_bbox": [28, 483, 745, 563]}], "3177": [{"image_id": 0, "file_name": "3177_00.png", "page": 4, "dpi": 300, "bbox": [39, 52, 391, 186], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: The \ufb02owchart of dataset generation.", "caption_bbox": [106, 197, 324, 211]}, {"image_id": 1, "file_name": "3177_01.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 355], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: The architecture of our DeepOrganNet. The DeepOrganNet \ufb01rst encodes the input image into a descriptor using MobileNets (without fully-connected layers) followed by a 1 \u00d7 1 convolution layer (dimension reduction). DW refers to the depthwise separable convolution block (two separable convolutional layers, functionally equivalent to a standard convolutional layer) and the numbers are output channel sizes (i.e., widths) of each layer / block. Every template in either left or right lung branch learns its own selection weight w and deformation parameters  P through an independent fully-connected layer with dimension 193, including 192 for  P and 1 for w. The deformed templates with the highest selection weights (e.g., templates L1 and R1 are selected in this example) in both branches are arranged according to the translation vector  Tl and  Tr learned from another fully-connected layer to generate the \ufb01nal combined multi-organ meshes. ", "caption_bbox": [27, 366, 744, 464]}, {"image_id": 2, "file_name": "3177_02.png", "page": 7, "dpi": 300, "bbox": [27, 29, 393, 488], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Qualitative comparison between P2M and our method on right lung model. Our results generate meshes with no non-manifold issue, while the results from P2M have self-intersections (highlighted in red). ", "caption_bbox": [392, 657, 744, 698]}, {"image_id": 3, "file_name": "3177_03.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 327], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Qualitative comparison between P2M and our method on left lung model. Our results generate meshes with no non-manifold issue, while the results from P2M have self-intersections (highlighted in red). ", "caption_bbox": [392, 333, 744, 374]}, {"image_id": 4, "file_name": "3177_04.png", "page": 7, "dpi": 300, "bbox": [456, 376, 682, 654], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 2: Quantitative comparison between PSGN and our method on our synthetic dataset. ", "caption_bbox": [392, 941, 742, 969]}, {"image_id": 5, "file_name": "3177_05.png", "page": 8, "dpi": 300, "bbox": [419, 393, 746, 717], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Qualitative comparison between the traditional SART and our method. The reconstructed 3D-CBCT images by SART from one and \ufb01ve views are unable to be used to segment lungs. SART requires CBCT projections from at least 50 views to reconstruct a good-quality 3D volumetric image such that the corresponding segmented lung model is comparable to our result. Failure parts (e.g., wrong connectivities and not-good shape preserving parts) of SART-based meshes are red-cycled. ", "caption_bbox": [404, 725, 754, 822]}, {"image_id": 6, "file_name": "3177_06.png", "page": 8, "dpi": 300, "bbox": [131, 54, 667, 333], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 3: Quantitative comparison between SART (with different num- bers of views) and our method. ", "caption_bbox": [39, 680, 391, 708]}, {"image_id": 7, "file_name": "3177_07.png", "page": 9, "dpi": 300, "bbox": [391, 29, 745, 273], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Three expiration phases of Case 8 in 4D-CT DIR-LAB dataset. Maximal deformation can be traced according to the red dashed line across the input 2D images, and the corresponding deformations on the reconstructed 3D mesh models are mapped into a uni\ufb01ed colormap range (hotter colors indicate larger deformations). The solid surface and wireframe meshes show the front-view and occluded (diaphragm) deformations, respectively. The deformations of Phases 1 and 2 are computed based on Phase 0 as the reference. ", "caption_bbox": [392, 276, 744, 387]}, {"image_id": 8, "file_name": "3177_08.png", "page": 9, "dpi": 300, "bbox": [27, 679, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: Three expiration phases of 4D NCAT phantom model. Maxi-       patients\u2019 organ shapes on the \ufb02y, signi\ufb01cantly improve the procedure mal deformation can be traced according to the red dashed lines across time for patients and doctors, and dramatically reduce the imaging dose the input 2D images, and the corresponding deformations on the recon-  during the treatment. Some further interactive techniques based on structed 3D mesh models are mapped into a uni\ufb01ed colormap range        DeepOrganNet will be developed in collaboration with domain experts. (hotter colors indicate larger deformations). The solid surface and       Discussion and Future Work: In the current framework, the wireframe meshes show the front-view and occluded (diaphragm) de-      lightweight MobileNets are computational ef\ufb01cient but limit the power formations, respectively. The deformations of Phases 8 and 10 are      of feature extraction in the encoder block. In the future, we will explore computed based on Phase 6 as the reference.                            some more powerful deep neural networks for the encoder part and ", "caption_bbox": [27, 566, 744, 680]}, {"image_id": 9, "file_name": "3177_09.png", "page": 9, "dpi": 300, "bbox": [71, 351, 336, 565], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Top: qualitative visualization results of 3D lung shape recon- struction from single-view phase-0 projections of \ufb01ve cases in DIR- LAB dataset. Bottom: qualitative visualization results of 3D lung shape reconstruction from single-view real X-ray images in JSRT database and real 3D-CBCT patient datasets. These sample results are picked from the challenging cases with large variations of the lung shapes. ", "caption_bbox": [28, 253, 380, 336]}], "3178": [{"image_id": 0, "file_name": "3178_00.png", "page": 2, "dpi": 300, "bbox": [28, 29, 745, 269], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: MV anatomy. a) Position of the MV between the left atrium and ventricle. Attached chordae tendineae and papillary muscles hold the valve in a closed position, when blood is pushed into the body. b) Top of the closed MV as seen from the left atrium. The anterior lea\ufb02et (A1-A3) and the posterior lea\ufb02et (P1-P3) are often partitioned into the shown segments [6]. c) MV lea\ufb02ets cut at the lateral commissure and \ufb02attened. ", "caption_bbox": [27, 290, 743, 331]}, {"image_id": 1, "file_name": "3178_01.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 376], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Views of our proposed framework. 1: 3D view. An interactive 3D MV model corresponding to a single time step of the data set is shown. Different color maps can be applied. The current color scheme is based on lea\ufb02et height. 2: 2D view, showing the \ufb02attened geometry analogous to 1. 3: 3D+t view. All time steps in the data set are shown simultaneously, while important features are emphasized. 4: 2D+t view. The \ufb02attened geometries of all time steps are stacked and the annuli are interpolated to a surface. The currently selected time step is marked in all views. 5: Variation of valve orientation over time. 6: Variation of valve position over time. Camera perspective is linked to 3. ", "caption_bbox": [27, 386, 744, 455]}, {"image_id": 2, "file_name": "3178_02.png", "page": 5, "dpi": 300, "bbox": [53, 59, 752, 238], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Color maps quantifying local surface properties indicative of pathologies. Valves are shown in 3D and 2D. a) Normal valve with color map corresponding to lea\ufb02et height relative to annulus plane. Note that the area around the saddle horn is supposed to be above the plane, the remaining lea\ufb02et area should be below. b) Pathological valve with lea\ufb02et prolapse. The area of the prolapse, as well as its location can be determined based on the color map. c) Normal valve with color map corresponding to coaptation, i.e., distance between lea\ufb02ets. d) Pathological valve with coaptation map. The \ufb02attened overview can be used to identify areas where the coaptation zone is not closed and leakage is probable. Due to self-occluding geometry, this is not obvious from the 3D model alone. ", "caption_bbox": [40, 256, 757, 339]}, {"image_id": 3, "file_name": "3178_03.png", "page": 7, "dpi": 300, "bbox": [75, 52, 721, 299], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Normal valve (top) and a valve with lea\ufb02et prolapse (bottom) shown in the 2D+t view. A homogeneous surface indicates a stable mitral annulus. Dents in the surface, as can be seen in e), are common with pathological valves, which often have a more turbulent annulus. Rotating the view to a side perspective allows to compare lea\ufb02et size changes over time. The spacing of the w-axis can be determined by the user. Collapsing this axis gives a representation as in c) and f). The distribution of prolapsed areas and coaptation can thus be assessed. Note how this view also emphasizes the difference in annulus and lea\ufb02et homogeneity, when comparing normal versus pathological valves. ", "caption_bbox": [40, 309, 755, 378]}], "3179": [{"image_id": 0, "file_name": "3179_00.png", "page": 1, "dpi": 300, "bbox": [94, 65, 704, 412], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Using FlowSense for a comparative study on the street speed changes between two slow zones: West Village (blue) and Alphabet City (red). The analysts start by drawing locations of speed limit signs, which appear as dots with speed limits encoded by color. The selected speed limit signs are interactively linked with the line chart that shows the changes of average vehicle speed over time on the corresponding streets. All diagram elements are created via FlowSense. The NL queries shown are executed in the numbered order. FlowSense processes rich data\ufb02ow context and allows the user to reference data\ufb02ow elements at different speci\ufb01city levels, e.g. node labels, node types, or implicitly. ", "caption_bbox": [73, 426, 722, 506]}, {"image_id": 1, "file_name": "3179_01.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 325], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. An example FlowSense query and its execution over the Auto MPG dataset. The derivation of the query is shown as a parse tree in the middle. The sub-diagram expanded by the query is illustrated at the bottom. The \ufb01ve major components of a query pattern are underscored. Each component and its relevant parts in the parse tree and the data\ufb02ow diagram are highlighted by a unique color. The result of executing this query is to create a parallel coordinates plot of columns mpg, horsepower, and origin, with its input coming from the selection port of the node labelled MyChart. ", "caption_bbox": [28, 343, 745, 396]}, {"image_id": 2, "file_name": "3179_02.png", "page": 5, "dpi": 300, "bbox": [425, 55, 736, 184], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The FlowSense input box and its query and token auto-completion. Special utterances are identi\ufb01ed by unique colors. (a) Query auto- completion suggestions; (b) Special utterance token completion: \u201cscatter- plot\u201d is presented after the letter \u201cR\u201d is entered; (c) Dropdown for handling tagging ambiguity: \u201cmpg\u201d are both column name and node label. ", "caption_bbox": [405, 187, 757, 254]}, {"image_id": 3, "file_name": "3179_03.png", "page": 7, "dpi": 300, "bbox": [77, 55, 720, 213], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Using FlowSense to study the aggregated monthly average vehicle speed on NYC streets with different speed limits. The queries are applied in the numbered order. The result shows a histogram for speed distribution and a line chart for speed changes over time. Both charts use color encoding based on the speed limit of the roads. The smaller histogram snapshot shows the speed histogram without color encoding before step 3. ", "caption_bbox": [40, 220, 757, 260]}, {"image_id": 4, "file_name": "3179_04.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 257], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. (a) Verdict distribution of participant answers to each of the user study tasks. (b) Box plot of completion time for each user study step4 . ", "caption_bbox": [392, 262, 742, 289]}, {"image_id": 5, "file_name": "3179_05.png", "page": 9, "dpi": 300, "bbox": [52, 580, 757, 680], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Number of failed queries grouped by the reasons of their failures. 7 C ONCLUSIONS", "caption_bbox": [40, 684, 509, 704]}], "3180": [{"image_id": 0, "file_name": "3180_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 371], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Approach Overview: A trained neural network-based surrogate model acts as the backend analysis framework, driving our interactive visual analysis system for analyzing a computationally expensive yeast simulation model. ", "caption_bbox": [61, 381, 710, 409]}, {"image_id": 1, "file_name": "3180_01.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 141], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: (a) Microscopic image of a highly polarization yeast cell. (b) Pedagogical illustration of the yeast cell structure. (c) The computa- tional domain used in the simulation to model the cell membrane. ", "caption_bbox": [392, 151, 744, 192]}, {"image_id": 2, "file_name": "3180_02.png", "page": 4, "dpi": 300, "bbox": [412, 52, 748, 179], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: (a) Architecture of our surrogate model. (b) Dropout-based uncertainty visualization of neural networks for a synthetic dataset. ", "caption_bbox": [404, 190, 754, 218]}, {"image_id": 3, "file_name": "3180_03.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 250], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Primary Visualizations and Interaction techniques: (a) Predicted Cdc42 concentration across the membrane along with uncertainty bands and selection brushes. (b) Parameter control bar. (c) Spatial parameter sensitivity. (d) Linear cluster tree for average parameter sensitivity. (e,f) Average parameter sensitivities. (g) Radial cluster tree for predicted Cdc42. (h) First weight matrix. (i) Final weight matrix. (j) Row selection probe. (k) Average parameter sensitivity for selected pattern. ", "caption_bbox": [27, 260, 743, 315]}, {"image_id": 4, "file_name": "3180_04.png", "page": 6, "dpi": 300, "bbox": [415, 55, 745, 478], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Multiple high-level analysis views of our visual analysis system.", "caption_bbox": [404, 488, 754, 502]}, {"image_id": 5, "file_name": "3180_05.png", "page": 7, "dpi": 300, "bbox": [400, 347, 735, 483], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: (a) Comparative evaluation of the simulation results using parameter con\ufb01gurations discovered by our system (black) and previous analysis work (red) [48]. Comparison curves of Cdc42 concentration for (b) a highly uncertain prediction and (c) a good prediction instance. ", "caption_bbox": [392, 494, 742, 549]}, {"image_id": 6, "file_name": "3180_06.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 254], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Discover new parameter con\ufb01gurations: (a) Predicted Cdc42 of a speci\ufb01c parameter instance with relatively high polarization pro\ufb01le. (b) Spatial parameter sensitivity of the parameter instance. (c) Corresponding average parameter sensitivities. Results for slightly changing the highly sensitive parameters k 42a(d), k 42d(e) and a less sensitive parameter k RL(f). Maximizing (g) and minimizing (h) predicted Cdc42 values in the selected regions. (i,j) Maximizing and minimizing the predicted values for the selected regions at the same time to get highly polarized predictions (i1 , j1 ). ", "caption_bbox": [28, 264, 743, 335]}, {"image_id": 7, "file_name": "3180_07.png", "page": 8, "dpi": 300, "bbox": [430, 52, 730, 398], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Knowledge extraction: (a) Connections of one parameter with H0 layer. (b) Row-wise sorted \ufb01rst weight matrix. (c) Connections of a neuron in H2 layer with the output layer. (d) Few selected weight patterns with high weights at the center. (e) Corresponding average parameter sensitivity sorted in descending order. ", "caption_bbox": [404, 409, 754, 478]}, {"image_id": 8, "file_name": "3180_08.png", "page": 9, "dpi": 300, "bbox": [391, 29, 745, 137], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Design study: Using circles (a,b) versus using rectangular boxes (c,d) across the membrane. Parameter control bar with (e) all the values displayed versus using (f) mouse-hovering. ", "caption_bbox": [392, 147, 742, 188]}], "3181": [{"image_id": 0, "file_name": "3181_00.png", "page": 2, "dpi": 300, "bbox": [391, 29, 745, 133], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: The analysis work\ufb02ow.", "caption_bbox": [494, 132, 641, 146]}, {"image_id": 1, "file_name": "3181_01.png", "page": 3, "dpi": 300, "bbox": [40, 29, 758, 361], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: A screenshot of our prototype system. The dimensionality reduction (DR) view (a) visualizes a result after DR and clustering. The feature contributions view (b) shows the measures of each feature\u2019s contribution to contrasting each cluster with the others. The feature values of the selected cells in (b) are visualized as histograms, as shown in (c). In (d), we can change the settings for the analysis methods and visualizations. ", "caption_bbox": [40, 360, 755, 400]}, {"image_id": 2, "file_name": "3181_02.png", "page": 3, "dpi": 300, "bbox": [43, 407, 386, 585], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Parallel coordinates showing all features in the Wine Recognition dataset. The corresponding polylines for the wines are highlighted with (a) black, (b) green, (c) orange, and (d) brown clusters. It is dif\ufb01cult to discern the essential features from this visualization. ", "caption_bbox": [40, 584, 390, 637]}, {"image_id": 3, "file_name": "3181_03.png", "page": 3, "dpi": 300, "bbox": [448, 407, 714, 645], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Comparison of features\u2019 relative contributions of MNIST digits. We compare LDA, PCA, and ccPCA. All of these methods can calculate the features\u2019 relative contributions to the \ufb01rst component by respectively referring to either LDA\u2019s loadings, PC loadings, or cPC loadings de- scribed in Sect. 4.3. We scale each loading in the range from -1 to 1 by dividing the maximum absolute value of the loadings. We visualize the scaled loading for each pixel with a blue-to-red colormap. For LDA, we perform classi\ufb01cation between the target digit and the others. The LDA results, placed on the left column, show that the outside pixels have high contributions. We can consider that LDA tries to distinguish each target digit from the others by referring to the pixels that are less frequently used in the other digits. We apply PCA to each target cluster in the same manner as [36, 69]. We can see that the PCA results show variations of the strokes when drawing each digit. The cPCA results are obtained from ccPCA with the automatic selection of \u03b1 (refer to Sect. 4.2). When compared with PCA, the cPCA results clearly show the strokes contrast- ing the target digit with the others. For example, for Digit 5, the pixels on the upper right have high contributions, as indicated in dark red. When only drawing Digit 5, we tend to use these pixels, and thus, we can see that cPCA captures Digit 5\u2019s characteristics. Similarly, for Digit 4, we can see that there are dark red pixels around the middle left. ", "caption_bbox": [404, 644, 756, 921]}, {"image_id": 4, "file_name": "3181_04.png", "page": 4, "dpi": 300, "bbox": [27, 29, 393, 172], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: cPCA results of the Mice Protein Expression dataset [30] from [4]. A different contrast parameter \u03b1 value is used for each result. When \u03b1 = 0, cPCA generates the same result when applying PCA to the target dataset. In this result, we cannot see clear differences between down syndrome (DS) and non-DS mice, indicated with red and blue points, respectively. While clear differences between DS and non-DS start to appear when \u03b1 = 1.7, we can see that DS is further separated into two groups when \u03b1 = 36.7. More examples can be found in [4, 5]. ", "caption_bbox": [27, 171, 379, 278]}, {"image_id": 5, "file_name": "3181_05.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 181], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: The DR results of the Wine Recognition dataset. The cluster labels generated in Sect. 3.2 are used. Here, we try to \ufb01nd the (c)PC contrasting the green cluster. In (a), we apply the classical PCA to the entire dataset. Though there is a separation of the green cluster when using the \ufb01rst and second PCs, there are overlaps of the green and orange clusters when only using the \ufb01rst PC (PC 1). In (b), the data points in the green cluster are used as the target dataset and the other data points are used as the background dataset. \u03b1 value is selected from the suggestions using the semi-automatic selection in Sect. 4.1.2. We cannot see a clear separation of the green cluster from the others. In (c), we use the entire data points instead of only the green cluster as the target dataset. \u03b1 value is selected with our automatic selection method in Sect. 4.2.3. We can see a better separation when compared to that of (a) and (b) even when using only the \ufb01rst cPC (cPC 1). ", "caption_bbox": [392, 181, 744, 366]}, {"image_id": 6, "file_name": "3181_06.png", "page": 5, "dpi": 300, "bbox": [39, 252, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: A comparison of centering effects to the cPCA results. For this right of Fig. 8. example, we generate two sets of data points from different 2D Gaussian    Similar to the semi-automatic selection in Sect. 4.1.2, our automatic distributions. In (a), the green circle and arrow show the centroid and selection lists multiple candidates of \u03b1 (our default is also 40 values). ", "caption_bbox": [40, 208, 757, 253]}, {"image_id": 7, "file_name": "3181_07.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 131], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Sign \ufb02ipping of cPCs. We generate the cPCA results of the Wine Recognition dataset used in Sect. 3.2 with different \u03b1 values. Sign \ufb02ipping occurs between \u03b1 = 3.3 and \u03b1 = 3.6; \u03b1 = 3.6 and \u03b1 = 3.9. ", "caption_bbox": [392, 130, 742, 172]}, {"image_id": 8, "file_name": "3181_08.png", "page": 7, "dpi": 300, "bbox": [40, 29, 758, 257], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "  (a) Original      (b) Reordered               (c) Aggregated                                                                        the selected cluster\u2019s histogram with a categorical color representing Fig. 10: Reordering and aggregation of the FCs. (a) shows the original its cluster label, while the gray color is used for the other data points\u2019 FCs. There are 8 clusters and 60 features. (b) shows the reordered FCs histogram. When hovering over a certain (representative) feature name, ", "caption_bbox": [40, 256, 756, 297]}, {"image_id": 9, "file_name": "3181_09.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 371], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13: The result after \ufb01ltering out the \u2018calories\u2019 and \u2018fat\u2019 features from the Nutrient dataset. In (a), a point size represents the value of \u2018carbohydrate\u2019. The histograms of the selected cells in (b) are shown in (c). ", "caption_bbox": [392, 370, 743, 410]}, {"image_id": 10, "file_name": "3181_10.png", "page": 8, "dpi": 300, "bbox": [27, 29, 394, 336], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: An analysis result of female players from the Tennis Major Tournament Match Statistics dataset. ", "caption_bbox": [27, 335, 377, 362]}, {"image_id": 11, "file_name": "3181_11.png", "page": 9, "dpi": 300, "bbox": [40, 29, 758, 423], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14: The results for the Communities and Crime dataset. The top   separation and does not take into account the variety (i.e., variance) for of (a) shows the result with t-SNE and DBSCAN. In the bottom of (a),  each feature. the pink cluster which was not identi\ufb01ed by DBSCAN is manually added.    Another potential option is using the two-group differential statistics ", "caption_bbox": [40, 422, 755, 467]}], "3182": [{"image_id": 0, "file_name": "3182_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 616], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Analyzing two different data sets in the What-If Tool: US Census Income dataset from UCI (a) to (e), and CelebA dataset (f). Three machine-learning tasks are represented: income prediction (a) to (d), age prediction (e), and smile prediction (f). ", "caption_bbox": [61, 628, 710, 656]}, {"image_id": 1, "file_name": "3182_01.png", "page": 4, "dpi": 300, "bbox": [39, 29, 766, 500], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Datapoint Editor in WIT. The left panel displays information related to machine-learning aspects of a selected data point in the dataset, including a list of feature values (B), inference values (C) and counterfactual controls (A). The right panel visualizes the dataset and offers controls for visually slicing the data according to its features. In (B) users can edit values of the data point and re-run inference to examine results. (B) also shows the closest counterfactual value to the selected data point, highlighting the delta between the two points. (D) is the selected data point. ", "caption_bbox": [40, 510, 755, 579]}, {"image_id": 2, "file_name": "3182_02.png", "page": 5, "dpi": 300, "bbox": [27, 29, 745, 271], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Features tab in WIT, showing summary statistics for the UCI Census dataset. Features are sorted by non-uniformity of the distributions of their values. Capital gains and capital loss have a large percentage of zeros and a long tail of non-zero values. ", "caption_bbox": [28, 281, 743, 309]}, {"image_id": 3, "file_name": "3182_03.png", "page": 6, "dpi": 300, "bbox": [39, 538, 756, 1021], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Datapoint editor view showing an edited value for capital-gain (A), which the user has changed from 3,411 to 20,000. After rerun- ning inference (B), this edit causes the score for label 1 (\u226550K) to    4.2.3   Partial Dependence Plots change from 0.336 to 0.991, essentially \ufb02ipping the prediction for this datapoint.                                                              Often practitioners have questions a ", "caption_bbox": [40, 470, 587, 539]}, {"image_id": 4, "file_name": "3182_04.png", "page": 6, "dpi": 300, "bbox": [393, 29, 756, 350], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Partial Dependence (PD) plot view. Top: categorical PD plot for education level. Bottom: numeric PD plot for capital gains feature. The neural network has learned that low but non-zero capital gains are indicative of lower income, high capital gains are indicative of high income, but zero capital gains is not necessarily indicative of either. In both plots, the horizontal gray line represents the positive classi\ufb01cation threshold for the models. ", "caption_bbox": [404, 363, 754, 460]}, {"image_id": 5, "file_name": "3182_05.png", "page": 8, "dpi": 300, "bbox": [39, 29, 757, 440], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Performance view of our Census example models broken down by sex, (a) showing performance when the positive classi\ufb01cation thresholds are left at their default levels of 0.5 for each sex, and (b) showing performance when the thresholds have been set to achieve demographic parity between sexes. Each confusion matrix refers to a given model: teal for model 1 (neural network) and orange for model 2 (linear classi\ufb01er). Achieving demographic party on either model requires lowering the threshold for females and raising it for males. ", "caption_bbox": [40, 452, 755, 507]}], "3183": [{"image_id": 0, "file_name": "3183_00.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 254], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Participants\u2019 diverse levels of expertise in terms of computational skill and domain knowledge. Most participants belong to one of the following two groups: group \u201cmDomainExperts\u201d (mostly domain experts), who have more domain knowledge than programming skills; and partici- pants with mostly strong computational skills but little knowledge of the application domain, \u201cmHackers.\u201d ", "caption_bbox": [392, 272, 744, 352]}, {"image_id": 1, "file_name": "3183_01.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 311], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. (a), left: Alternatives characterized based on the data worker\u2019s degree of attention. (b) A work\ufb02ow showing alternatives for P5. Alternatives are grouped along the y-axis according to their degree of attention. Along the x-axis are different stages along the analysis. We can see different alternatives migrate between different attention degrees at different stages of the analysis as their frames of reference evolve. ", "caption_bbox": [28, 330, 742, 370]}, {"image_id": 2, "file_name": "3183_02.png", "page": 6, "dpi": 300, "bbox": [406, 55, 754, 271], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The \ufb01ve high-level processes around alternatives: Generate, Update, and Reduce are distinguished by their different \u201cshapes\u201d within the alternative space (divergent, parallel, convergent). Reason and Manage do not directly produce alternatives, but operate on them. ", "caption_bbox": [404, 288, 755, 341]}], "3184": [{"image_id": 0, "file_name": "3184_00.png", "page": 1, "dpi": 300, "bbox": [102, 65, 695, 555], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The VASABI interface realises our multifaceted, visual user behaviour analysis approach through hierarchical pro\ufb01les. We concurrently visualise and interrelate: clusters of users based on tasks extracted with a topic-modelling based approach (top-left), user pro\ufb01les with multiple features (top-right) and distribution of sessions over time (middle). Selected sessions (brown brush over temporal histogram) are also highlighted both within the user pro\ufb01les as orange dots and analysed further in the session timeline (bottom). ", "caption_bbox": [73, 568, 722, 621]}, {"image_id": 1, "file_name": "3184_01.png", "page": 5, "dpi": 300, "bbox": [42, 53, 390, 82], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Visual representation of a user cluster showing the dominant tasks performed by its users (right) and the 10 most relevant users (left). ", "caption_bbox": [40, 97, 392, 124]}, {"image_id": 2, "file_name": "3184_02.png", "page": 5, "dpi": 300, "bbox": [406, 52, 757, 206], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Visualisation of user clusters. Each row represents a cluster of users with circles mapping to the proportion of tasks performed. Top 10 relevant users in a cluster are shown on the left. ", "caption_bbox": [405, 219, 755, 259]}, {"image_id": 3, "file_name": "3184_03.png", "page": 6, "dpi": 300, "bbox": [27, 29, 745, 254], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Visualisation of user pro\ufb01les. Each row is a visual pro\ufb01le for a user, consisting of visual summary of multiple features. These users belong to the same group, G0, whose pro\ufb01le is placed at the top. Sessions of interest (external input) are shown as orange dots and the slightly deviated one is manually highlighted here with a blue circle. ", "caption_bbox": [28, 267, 743, 307]}, {"image_id": 4, "file_name": "3184_04.png", "page": 6, "dpi": 300, "bbox": [73, 1015, 707, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Visualisation of user tasks. Each task is shown as a set of       proportionally by their timestamps or sequentially by their temporal coloured squares, each representing a dominant action in the task. On     order to avoid overlapping. By having a compact representation of the left, two task distributions are shown using lighter and darker grey, a session, it is possible to stack multiple session timelines to enable enabling comparison, e.g., single user vs. all other users in a group.    comparison (addressing T7). ", "caption_bbox": [28, 963, 743, 1016]}, {"image_id": 5, "file_name": "3184_05.png", "page": 7, "dpi": 300, "bbox": [45, 56, 758, 305], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. A semi-automated clustering approach is used for the simpli\ufb01-    comprehensive task. The task was to use VASABI to characterise user cation of the colourmapping. A word2vec representation is fed into the   groups, to identify users of interest and to investigate unusual sessions. t-SNE algorithm to \ufb01nd a projection of actions where action similarities We used two comparable datasets that include logs collected from two are preserved (as much as possible) in the resulting 2D space. We then   similar applications. One of the datasets was used to design VASABI ", "caption_bbox": [40, 304, 757, 362]}, {"image_id": 6, "file_name": "3184_06.png", "page": 8, "dpi": 300, "bbox": [27, 29, 745, 214], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Exploring multiple features of user groups. Group G9 stands out as abnormal: its users perform many sessions with a high number of unique actions. The sessions have high anomaly scores, start late and have different browser distribution compared to other groups. ", "caption_bbox": [28, 227, 743, 254]}, {"image_id": 7, "file_name": "3184_07.png", "page": 9, "dpi": 300, "bbox": [40, 52, 757, 171], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Exploring users of interest. User Khoryphos appears to be abnormal with high anomaly scores and using both mobile and desktop operating systems equally frequently. ", "caption_bbox": [40, 183, 755, 211]}, {"image_id": 8, "file_name": "3184_08.png", "page": 9, "dpi": 300, "bbox": [40, 225, 757, 506], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Exploring sessions of interest. Selecting sessions from two different operating systems for investigation. There are clear differenences between the two. Note: This \ufb01gure is composed from two screenshots to save space. ", "caption_bbox": [40, 519, 754, 546]}], "3185": [{"image_id": 0, "file_name": "3185_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 475], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: With GUIRO we address three essential matrix analysis questions: (1) Which matrix reordering algorithm produces useful results? (2) Can we steer these algorithms to support analytical tasks? (3) How to compare reorderings quantitatively & qualitatively? ", "caption_bbox": [61, 485, 710, 513]}, {"image_id": 1, "file_name": "3185_01.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 231], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: An adjacency matrix (left) and a Node-Link diagram (right) allow interpreting a range of visual patterns. A network analysts\u2019 tasks is to (a) to \ufb01nd the row/column ordering that represents most topological structures best and (b) to represent the results e\ufb00ectively to the audience. Figure adapted from [54]. ", "caption_bbox": [392, 244, 744, 313]}, {"image_id": 2, "file_name": "3185_02.png", "page": 4, "dpi": 300, "bbox": [404, 30, 758, 355], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Processing pipeline for our user-steerable matrix reordering approach: the rows, resp. columns, of a matrix are interpreted as high-dimensional (HD) vectors (1 st image) and projected into a 2D space (2nd image) forming a set of vertices. Similar HD vectors are projected to similar 2D positions. A matrix reordering result is depicted by a path acyclically connecting all vertices. Selecting vertex groups allows a local application of reordering algorithms on submatrices (3rd image). The path can be manually modi\ufb01ed, such that locally optimized submatrices or single vertices can be placed next to another (4th image). ", "caption_bbox": [404, 367, 754, 491]}, {"image_id": 3, "file_name": "3185_03.png", "page": 4, "dpi": 300, "bbox": [404, 505, 756, 719], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: A simple example demonstrating the basic design of GUIRO. The screenshot shows the Petit Test Suite G95c dataset with three reordering options (left). The projection space (right) depicts that distinct groups of rows/columns can be formed. ", "caption_bbox": [404, 729, 754, 784]}, {"image_id": 4, "file_name": "3185_04.png", "page": 6, "dpi": 300, "bbox": [406, 30, 757, 253], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Two matrix reordering strategies are possible: A forward edge modi\ufb01cation (top) and a backward edge modi\ufb01cation (bottom). De- pending on the type of reordering strategy, di\ufb00erent approaches to reestablish a formally correct linear arrangement have to be applied. ", "caption_bbox": [404, 267, 756, 322]}, {"image_id": 5, "file_name": "3185_05.png", "page": 8, "dpi": 300, "bbox": [39, 29, 756, 270], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: In cases where global matrix reordering algorithms do not lead to satisfying reordering results, as depicted left, GUIRO can help to reveal hidden matrix substructures. We apply Optimal-Leaf-Ordering\u2014a hierarchical clustering extension\u2014 to our dataset. The algorithm \ufb01nds many distinct clusters on the diagonal, but fails to reveal structures in the regions on the top right and bottom left. The Kernel PCA projection depicts one large cluster and two subclusters. A lasso selection of the top left projection cluster reveals that many of the fragmented rows/columns on the top left are related to an existing, already coherent, block in the matrix. Reordering this submatrix with MultiScale has a major in\ufb02uence on the visual quality in the top right of the matrix, but re\ufb02ects only mildly on the o\ufb00-diagonal structures in this quadrant. Selecting and reordering the other projection cluster (for completeness) has only minor impact on the overall matrix appearance. ", "caption_bbox": [40, 280, 755, 377]}], "3186": [{"image_id": 0, "file_name": "3186_00.png", "page": 1, "dpi": 300, "bbox": [49, 65, 770, 379], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Multiple coordinated views in Facetto for interactive and hierarchical phenotype analysis of 36-channel image data (image resolution: 31, 616 \u00d7 22, 272 pixels; raw image size: 49.8 GB). (a) Phenotype tree resulting from hierarchical data \ufb01ltering and cell calling. (b) Multi-channel visualization of high-resolution CyCIF image data showing the current clustering and classi\ufb01cation results. (c) Ridgeplot of high-dimensional feature data to steer visual analysis and data \ufb01ltering. (d) UMAP projection of the sampled feature space of cells, colored by cluster ID. (e) Scatterplots showing feature value correlations. (f) Table view of all cells and their features. ", "caption_bbox": [73, 392, 722, 459]}, {"image_id": 1, "file_name": "3186_01.png", "page": 3, "dpi": 300, "bbox": [80, 53, 720, 138], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The CyCIF imaging work\ufb02ow. Steps 1-4: Acquisition and Preprocessing. Tissue sections are stained with antibodies chemically linked to a \ufb02uorophore; typically 3-5 antibodies are combined. The sections are imaged and \ufb02uorophores are then chemically inactivated (bleached) making it possible to perform another round of staining and imaging. Multiple cycles allow collecting images with 60 or more different \u201dchannels,\u201d each of which represents the staining pattern from a single antibody. Images are collected in successive tiles, which are then stitched together. Cells are segmented, and features such as total intensity per cell are extracted. The resulting data are analyzed interactively in Facetto (step 5). ", "caption_bbox": [40, 138, 755, 205]}, {"image_id": 2, "file_name": "3186_02.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 287], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The phenotype data \ufb02ow tree supports the analyst in the pheno- type analysis work\ufb02ow. Users make selections (e.g., by \ufb01ltering, cluster- ing, or classi\ufb01cation) in an initial dataset (i.e., root). Selections can be stored as subsets and applied in a hierarchical manner, leading to a tree structure that maintains analytical provenance. Each node has a label, color, and displays the number of contained cells as a \ufb01ll line. ", "caption_bbox": [392, 291, 744, 371]}, {"image_id": 3, "file_name": "3186_03.png", "page": 5, "dpi": 300, "bbox": [117, 45, 682, 154], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Multi-resolution image rendering. Left: The input to our rendering pipeline is a stack of high-resolution images (one image per CyCIF channel) and a segmentation mask containing a cell ID per pixel. Middle: We pre-compute multi-resolution image pyramids for all input data. Right: During rendering, we blend channels and choose between different rendering modes based on a pixel\u2019s cell ID, e.g., for highlighting a selected set of IDs. ", "caption_bbox": [40, 158, 755, 198]}, {"image_id": 4, "file_name": "3186_04.png", "page": 5, "dpi": 300, "bbox": [44, 206, 752, 353], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Image viewer render modes. (a) shows single channel rendering, while (b-d) show different zoom levels of a 49.8 GB dataset. The rendering shows three channels (proteins pS6, HES1 and DNA) with a selection on a metabolically active area. (d) Top: Selected nuclei are highlighted in orange. Bottom: 3-channel rendering without nuclei highlights. ", "caption_bbox": [40, 357, 755, 397]}, {"image_id": 5, "file_name": "3186_05.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 257], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. (a) Image viewer showing a region with colored cell nuclei from clustering, overlaid onto the DNA channel. (b) UMAP projection scatter- plot of an active subset with color-coded clusters. The clustering sepa- rates cancer regions (red) from immune cells (blue). (c) Small-multiple scatterplots show correlations of active rendering channels (here DNA, the tumor marker S100, and immune cell marker CD45). ", "caption_bbox": [392, 261, 744, 342]}, {"image_id": 6, "file_name": "3186_06.png", "page": 6, "dpi": 300, "bbox": [27, 29, 393, 259], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. The ridgeplot (middle and right) shows intensity distributions for each of the 44 image channels, arranged on parallel x-axes. Feature values for the currently selected cells are visualized using vertical polyline overlays (in orange). Users can interactively \ufb01lter individual channels, select channels for display in the image viewer (left), and adjust transfer functions used for rendering (bottom left and right). ", "caption_bbox": [27, 261, 379, 341]}, {"image_id": 7, "file_name": "3186_07.png", "page": 7, "dpi": 300, "bbox": [49, 53, 758, 393], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. (a) Facetto allows users to employ clustering and classi\ufb01cation    that contain any of the selected cells), using a quad-tree acceleration in an iterative (loop) setting where users can use the result of one to    structure. We then classify each cell in the loaded tiles. Subsequently, steer the other, and vice versa. (b) EM Clustering: User-selected data     the results are \ufb01ltered to match only the requested labels to be displayed. is normalized, clustered, and \ufb01nally visualized. (c) Classi\ufb01cation: Multi- Users can visually inspect results and manually sort out or relabel channel images of labeled cells are used as input to train a CNN. For      questionable cell classi\ufb01cations. ", "caption_bbox": [40, 392, 757, 461]}, {"image_id": 8, "file_name": "3186_08.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 277], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Ovarian cancer phenotype analysis (use case 2). Left to right: Experts start with a high-level view of the data (a, e), extract a ROI (b, f), cluster cells in the region into cancer and immune subpopulations (c, g), and subsequently subdivide one of the clusters into subtypes (d, h). ", "caption_bbox": [28, 278, 744, 305]}], "3187": [{"image_id": 0, "file_name": "3187_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 488], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: The ProtoSteer interface for interactively re\ufb01ning prototype sequence network. The prototype overview (A) presents a list of prototype sequences and their statistics on datasets. The sequence detail view (B) displays the neighboring sequences of selected prototypes. The user can interactively add, delete, and revise prototypes. All edits are traceable in the editing history (C), where the user can easily compare or revert changes. For advanced analysis, the sequence encoder view (D) projects prototypes as trajectories with a contour map showing its neighboring hidden state distribution. ", "caption_bbox": [60, 500, 709, 569]}, {"image_id": 1, "file_name": "3187_01.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 212], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: ProSeNet Architecture. It consists of the sequence encoder layer r, the prototype layer p, and the fully connected layer f with softmax outputs for classi\ufb01cation tasks. ", "caption_bbox": [392, 225, 742, 266]}, {"image_id": 2, "file_name": "3187_02.png", "page": 4, "dpi": 300, "bbox": [404, 614, 757, 788], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: The ProtoVis system consists of a storage layer, a service layer (containing the model manager, the analytics module and the sequence query module), and a visual interface. ", "caption_bbox": [404, 799, 754, 840]}, {"image_id": 3, "file_name": "3187_03.png", "page": 7, "dpi": 300, "bbox": [32, 605, 744, 936], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Pruning and analyzing vehicle fault sequences. A: The interaction history. B: The prototype list sorted by the number of closest instances. Solid \ufb01ll encodes true positive, and pattern \ufb01ll indicates false negative. Prototypes #171 and #135 present recurring faults of P0007 and P006d. Prototype #127 indicates an unexpected predicted risk from a single P2XXX fault. C: The neighbor instances of #127 in the validation set. D: Filtering prototypes by faults P2XXX and P2XXY. ", "caption_bbox": [28, 947, 745, 1002]}, {"image_id": 4, "file_name": "3187_04.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 501], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Steering ProSeNet trained on Yelp Review. A: Evaluate the similarity between two prototypes #0 and #56 via sequence encoder view (A1 ), and reduce the duplicity of them (A2 ). B: Reduce the verboseness of a prototype by removing unimportant events and subsequences with little state change (B1 ). C1 : Search prototype candidates with key phrases \u201cgood food\u201d (exact match) and \u201cbad service\u201d (soft match), and C2 : edit the sequence for cleaner semantics and submit the create operation. ", "caption_bbox": [27, 511, 742, 566]}], "3188": [{"image_id": 0, "file_name": "3188_00.png", "page": 3, "dpi": 300, "bbox": [393, 29, 758, 660], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Left: the performance comparison between the CPU baseline streaming implementation and the CUDA accelerated version (5D sample with varying sizes). Right: a plot showing the total edges extracted from a symmetric, relaxed Gabriel graph computed on the same dataset as a function of the k used in the initial k-nearest neighbor graph. ", "caption_bbox": [404, 661, 754, 728]}, {"image_id": 1, "file_name": "3188_01.png", "page": 4, "dpi": 300, "bbox": [392, 561, 740, 808], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The proposed visualization interface consists of three views: topological spine (a), density scatterplot (b), density parallel coordinates plot (c). These views provide complementary information, and the linked selection enables a joint analysis of both geometric and topological features. In (a1), we show the persistence plot, which controls the number of extrema displayed in the topological spine. ", "caption_bbox": [392, 807, 743, 887]}, {"image_id": 2, "file_name": "3188_02.png", "page": 5, "dpi": 300, "bbox": [40, 29, 758, 158], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Topological spine. A 2D terrain metaphor of high-dimensional laying the groundwork for using fusion as a clean and safe energy topological information [10].                                        source. Scientists utilize high-energy lasers to heat and compress a ", "caption_bbox": [40, 157, 755, 188]}, {"image_id": 3, "file_name": "3188_03.png", "page": 5, "dpi": 300, "bbox": [40, 651, 758, 979], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Inertial con\ufb01nement fusion (ICF). Lasers heat and compress the the entire system is trained jointly to obtain the \ufb01tted models F\u0302 and target capsule containing fuel to initiate controlled fusion.          G\u0302, assuming that we have access to a pretrained autoencoder. In order ", "caption_bbox": [40, 978, 755, 1008]}, {"image_id": 4, "file_name": "3188_04.png", "page": 5, "dpi": 300, "bbox": [404, 189, 759, 571], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. System diagram of the deep learning-based surrogate modeling used in the ICF study. A 5D input parameter space X is mapped onto a 20D latent space L, which is fed to a multimodal decoder D returning the multiview X-ray images of the implosion as well as diagnostic scalar quantities. ", "caption_bbox": [405, 583, 755, 650]}, {"image_id": 5, "file_name": "3188_05.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 188], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Joint exploration of both topological and geometric characteristics of the surrogate\u2019s errors as functions in the input parameter space. In (a1), the topological spine shows two local peaks of the output error (Fo (x)). The user can select the high error samples through parallel coordinates plot in (b2). These samples will also be highlighted in topological spine (b1). Also, the user can focus on an individual extremum (see c1, c2). ", "caption_bbox": [28, 189, 744, 229]}, {"image_id": 6, "file_name": "3188_06.png", "page": 7, "dpi": 300, "bbox": [404, 344, 757, 827], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. The comparison of cyclic loss (a measure of surrogate self- consistency) among models with different training sets and training times. As shown in (a), the 100K samples trained model exhibits a peculiar parabola pattern, which indicates the lack of self-consistency for a certain parameter range. By increasing the training set (from 100K to 1 Million), we are able to eliminate these empty regions (see (b)), but the model ends up with larger errors, especially among outliers, which can be mitigated by increasing the training time (from 40 to 80 epochs, see (c)). In (d), we show the topological structure of the self-consistency loss of the best model and reveal that the highly inconsistent region is characterized by high values of physical parameter betti v of the surrogate input. ", "caption_bbox": [404, 831, 756, 976]}, {"image_id": 7, "file_name": "3188_07.png", "page": 7, "dpi": 300, "bbox": [39, 344, 394, 696], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. X-ray images with different energy and error conditions.", "caption_bbox": [40, 701, 348, 715]}, {"image_id": 8, "file_name": "3188_08.png", "page": 7, "dpi": 300, "bbox": [40, 29, 758, 300], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. The autoencoder error (R(l)) and the latent space error (Flat (x)) is shown in (a),(b) respectively. The yield of the simulation is shown in (c). In (d), we illustrate the latent space error (Flat (x)) of the model that are trained for 80 epochs instead of 40. ", "caption_bbox": [40, 301, 755, 331]}, {"image_id": 9, "file_name": "3188_09.png", "page": 8, "dpi": 300, "bbox": [395, 708, 737, 835], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Multiscale simulation of RAS-membrane biology for cancer research. As the continuum model (left) evolves, patches around the RAS proteins (shaded balls) are projected into a high-dimensional latent space, which is used to drive an importance sampling. Patches that are different from previous simulations (yellow) are used to initialize new MD simulations, whereas patches too close to existing samples (red) are discarded. Signi\ufb01cant challenges exist in the analysis of high-dimensional latent spaces to improve the sampling. ", "caption_bbox": [392, 847, 742, 953]}, {"image_id": 10, "file_name": "3188_10.png", "page": 9, "dpi": 300, "bbox": [40, 29, 757, 451], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11. Visualization of different sampling patterns (middle and bottom) when compared to the overall distribution (top) highlights that the adaptive importance-based approach has a wider coverage and lower and fewer modes, as required to execute the targeted multiscale simulation. ", "caption_bbox": [40, 452, 754, 479]}], "3189": [{"image_id": 0, "file_name": "3189_00.png", "page": 1, "dpi": 300, "bbox": [391, 65, 757, 676], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 1. Scenario of scatterplot scaling. Bob \ufb01nds a pattern of interest in a small scatterplot view and expands the scatterplot to a large view for further analysis. To exchange \ufb01ndings, he shares the scatterplot, and other collaborators examine the scatterplot using their own devices. The potential perceptual bias of scatterplots in various displays caused by scaling may lead to understanding inconsistency. ", "caption_bbox": [405, 677, 755, 757]}, {"image_id": 1, "file_name": "3189_01.png", "page": 3, "dpi": 300, "bbox": [404, 56, 757, 289], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 3. Example of a 2WS sequence. The y-axis shows visual feature levels. The x-axis shows the number of trials. (a) Fifteen trials of the forward-staircase. (b) Fifteen trials of the backward-staircase. The gray background shows the \ufb02uctuating interval of both staircases. ", "caption_bbox": [405, 294, 755, 347]}, {"image_id": 2, "file_name": "3189_02.png", "page": 5, "dpi": 300, "bbox": [405, 52, 756, 187], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 4. Interface of the experiment system. (a) Interface of the tutorial. In this example, the visual feature is numerosity. The scatterplot on the left is the reference. The 13 scatterplots on the right show 13 numerosity levels in ascending order, which can be scrolled through to browse. (b) Interface of the formal experiment. The judgment question is shown on the top. The scatterplot-pair and a center \ufb01xation point are located in the middle. Buttons for reporting judgments and proceeding to the next trial are found at the bottom. ", "caption_bbox": [405, 188, 757, 294]}, {"image_id": 3, "file_name": "3189_03.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 218], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 5. Examples of abnormal choice patterns: (a) \ufb01ercely \ufb02uctuating and (b) early \ufb02uctuating. (c) Illustration of the bias measurement. The plot is drawn on the basis of the choices of a sequence of trials made by a certain participant on numerosity at scale ratio = 40%, where the x-axis is the actual feature value of the test scatterplot, and the y-axis is the probability of selecting \u201cthe feature value of test scatterplot is perceived to be smaller than the feature value of the reference scatterplot.\u201d Furthermore, a data point represents the proportion of \u201ctest < reference\u201d choices made by the participant at a feature value of the test scatterplot, and the blue curve is the best-\ufb01tting cumulative Gaussian function of all data points. Thus, the POE is the baseline feature value point on the x-axis, whereas the PSE is the point on the best-\ufb01tting curve where y = 0.5. The bias is the value of the PSE minus the POE. ", "caption_bbox": [392, 224, 743, 396]}, {"image_id": 4, "file_name": "3189_04.png", "page": 7, "dpi": 300, "bbox": [59, 53, 736, 548], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 6. Objective results of the bias measurement with regard to experiments (E1, E2, E3 [scale ratio = 63%], and E3 [scale ratio = 252%]) and three visual features (numerosity, correlation, and cluster separation). A plot presents the biases of 7 scale ratios/point radii for a certain experiment and feature, in which the x-axis is a logarithmic (base 10) axis of scale ratio for E1 and E2 or point radius for E3 because 7 levels of scale ratios/point radii form a geometric sequence. The y-axis is the value of a bias. A point represents the mean of biases of all participants at a certain scale ratio/point radius. The error bar of a point encodes a 95% con\ufb01dence interval, and two dashed lines in a plot represent the baselines of 0 bias and 100% ratio/2.0 radius. ", "caption_bbox": [40, 554, 755, 634]}, {"image_id": 5, "file_name": "3189_05.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 435], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 7. Subjective questionnaire results. (a) Results of DL questions. A violin plot, which consists of a box plot and an area plot, presents the results of all participant ratings on the DL questions for an experiment and feature. The y-axis indicates the judgment dif\ufb01culty levels ranging from 1 (lowest dif\ufb01culty) to 5 (highest dif\ufb01culty). A box plot presents the median, upper quartile, lower quartile, 1.5 IQR of the lower quartile, and 1.5 IQR of the upper quartile of ratings. An area plot presents the kernel density distribution of ratings on the \ufb01ve dif\ufb01culty levels. (b) Results of DT questions. A bar chart presents the results of all participants who select the tendency options of the DT question for an experiment and feature. The x-axis encodes the \ufb01ve options (A. increasing, B. decreasing, C. increasing after decreasing \ufb01rst, D. decreasing after increasing \ufb01rst, and E. not sure) and the y-axis encodes the counts of the participants selecting a certain option. ", "caption_bbox": [27, 437, 744, 530]}], "3190": [{"image_id": 0, "file_name": "3190_00.png", "page": 4, "dpi": 300, "bbox": [27, 29, 745, 360], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. A taxonomy of summative evaluation methods based on surveying 82 papers published in VAST-17 and 18. The leaves represent categories of evaluation methods distinguished by the dimensions shown in the left. The percentages show the distribution of surveyed studies. ", "caption_bbox": [28, 376, 743, 403]}, {"image_id": 1, "file_name": "3190_01.png", "page": 7, "dpi": 300, "bbox": [46, 54, 757, 635], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. A summary of our analysis of evaluation methods. We capture the main activities taken by evaluation methods which could introduce risk to evidence validity, generalizability and feasibility. We assign 3 risk categories for these criteria per activity, classify each risk factor to high, normal or reducer class, then compare the methods using their summative quality (SQ) and feasibility. ", "caption_bbox": [40, 648, 755, 688]}], "3191": [{"image_id": 0, "file_name": "3191_00.png", "page": 1, "dpi": 300, "bbox": [56, 65, 740, 365], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. CourtTime visual analytics system. CourtTime provides an overview of the match score (A) and lets the user switch seamlessly between high-level overview of played points (B) and a detailed view on the shot level (C) that displays the serve, return, and last three shots in the point from each player. To facilitate access to the analysis, we provide a rich set of different spatio-temporal encodings, as well as ordering and aggregation capabilities (not shown). An interconnected \ufb01lter list (D) provides a multi-faceted means to effectively drill-down to speci\ufb01c points. We enable shot-speci\ufb01c, spatial feature-driven re-orderings (E) to aid in \ufb01nding shot patterns. A 1-D Space-Time Chart displaying the left/right dimension of a point over time is shown in the inset in (B). Solid circles are forehands and hollow circles are backhands. The inset in (C) shows a second serve by player one up the middle. Clicking on a shot or point opens a new window with a video playing the selected situation. ", "caption_bbox": [73, 379, 722, 485]}, {"image_id": 1, "file_name": "3191_01.png", "page": 3, "dpi": 300, "bbox": [61, 764, 758, 948], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Finite-state machine used to collect match context data. Points       T Wide serve. Boolean value that is true if the returner hit the service start with a \ufb01rst or second serve and then go into a rally state until either   return from outside the singles sidelines (i.e., was a wide serve). a winner or error ends the point. ", "caption_bbox": [40, 957, 755, 998]}, {"image_id": 2, "file_name": "3191_02.png", "page": 5, "dpi": 300, "bbox": [422, 52, 737, 220], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Final encoding of points for depth and left/right player and ball movement. The colored line on the left side of the graphic indicates who is serving (blue for player one and red for player two). A solid line indicates a \ufb01rst serve and a dashed line indicates a second serve. The colored line on the right indicates who won the point (blue or red) and how it was won (solid = winner, dashed = unforced error). ", "caption_bbox": [404, 233, 754, 313]}, {"image_id": 3, "file_name": "3191_03.png", "page": 7, "dpi": 300, "bbox": [47, 312, 757, 388], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Color-codings for hierarchical clusters. Width of each bar corre- actionable insights these players could use to evaluate their relative sponds to number of points in cluster.                                    strengths or weaknesses or how these points were won or lost. ", "caption_bbox": [40, 387, 755, 417]}, {"image_id": 4, "file_name": "3191_04.png", "page": 7, "dpi": 300, "bbox": [58, 52, 739, 194], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The inset shows tennis court graphics that, in the application, appear above each column in the Shot Analyzer and that allow users to select shot attributes used to sort similar points. They indicate that player two\u2019s and the ball\u2019s x-coordinate (left/right dimension) were selected for the last shot in the point by player two and player two\u2019s y-coordinate (depth dimension) and player one\u2019s and the ball\u2019s x- and y-coordinates were selected for player one\u2019s last shot. The last shots in the \ufb01rst point listed here (highlighted in blue) serve as the baseline used to \ufb01nd similar points. The similarity between the last two shots of all four of these points illustrates the effect. Reverse Shot Index (RSI) indicates how many more shots until the last shot by a player. so RSI 0 here indicates the last shot by each player. ", "caption_bbox": [40, 208, 755, 288]}, {"image_id": 5, "file_name": "3191_05.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 458], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. (A) Every point in every game of the third set of a professional men\u2019s tennis match where an unusually long game (with two of the longest points in the match - the 2nd and 5th point) resulted in the only service break in the set. (B) Two left/right movement patterns identi\ufb01ed in the Point Analyzer (see Section 5.2.1 for an explanation of the encodings). The top set shows each player playing from their weaker side of the court (their backhands) cross-court to their opponent\u2019s weaker side. The bottom set shows examples of each player moving the other back and forth. (C) Three examples of player one (in blue) coming to the net to hit a winning shot while keeping his opponent deep by the baseline. (D) Four points showing similar point-winning patterns for player one, discovered using the user-driven spatial feature similarity capabilities of the Shot Analyzer. Each row shows a backhand attacking shot by player one who then comes further into the court to hit an effective shot that sets up the \ufb01nal winning shot. ", "caption_bbox": [27, 471, 742, 564]}], "3192": [{"image_id": 0, "file_name": "3192_00.png", "page": 3, "dpi": 300, "bbox": [404, 208, 757, 400], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The overview of the system. The system consists of three components, i.e., the data processing component, the model component, and the visualization component. ", "caption_bbox": [405, 411, 756, 451]}, {"image_id": 1, "file_name": "3192_01.png", "page": 3, "dpi": 300, "bbox": [404, 845, 757, 947], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The example of the structure of a match. M refers to \u201dMatch\u201d, Gi                                               i, j refers to \u201dGame\u201d, Rij refers to \u201dRally\u201d, and Sk refers to \u201dStroke\u201d. The red parts are newly added compared with the old model. (A) illustrates the division of the four phases. ", "caption_bbox": [405, 960, 755, 1015]}, {"image_id": 2, "file_name": "3192_02.png", "page": 4, "dpi": 300, "bbox": [391, 29, 746, 761], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The original model and the new model. Blue encodes the original model, red encodes the new model. (A) and (B) present the state vectors. Speci\ufb01cally, in (A), Sk represents the stroke, Vk represents the state vector of each stroke Sk , vi,p1 and vi,p2 represents the state value of stroke attributes and w p1 and w p2 represents the state value of winning of the two players. (C) presents the simulation process. ", "caption_bbox": [392, 774, 744, 854]}, {"image_id": 3, "file_name": "3192_03.png", "page": 6, "dpi": 300, "bbox": [392, 540, 746, 968], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. The explanation mode of simulation view. (B) presents the correlation among strokes. (C) and (H) present the former and latter two strokes of the adjustment (E). ", "caption_bbox": [392, 971, 742, 1011]}, {"image_id": 4, "file_name": "3192_04.png", "page": 6, "dpi": 300, "bbox": [27, 29, 745, 454], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The system interface. The donut charts and the pie charts encode the scoring rates and utilization rates, respectively. (A) is the player view which displays all matches of Ito Mima and her opponents. It provides navigation of matches. (B) is the tactic view which displays the tactics speci\ufb01ed by stroke placement at the serve phase. It provides navigation of tactics. (C) is the simulation view which is under the exploration mode. It contains (D), the exploration component for implementation of adjustments and (E), the evaluation component for evaluation of adjustments. (D) displays all of the adjustment options speci\ufb01ed by stroke placement and stroke technique. (E) displays the three optimum strategies generated by the system. ", "caption_bbox": [27, 460, 742, 527]}, {"image_id": 5, "file_name": "3192_05.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 584], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. The \ufb01gure of Section 6.2. (A) presents the tactics used by Ito Mima in the serve phase. (B) presents the adjustment strategies in the evaluation component. (C) presents explanation of the adjustments. ", "caption_bbox": [392, 595, 742, 635]}], "3193": [{"image_id": 0, "file_name": "3193_00.png", "page": 1, "dpi": 300, "bbox": [78, 65, 720, 362], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Exploratory cohort selection in high-dimensional datasets can lead to selection bias\u2013unintended side-effects in variable distributions\u2013that may go unnoticed by the user. Our selection bias tracking system and detailed cohort comparison visualizations, deployed in a medical temporal event sequence visual analytics tool, include (a) a cohort provenance tree to keep track of created cohorts and indicate when selection bias may have occurred, (b-d) a suite of high-dimensional cohort comparison visualizations that employ hierarchical aggregation to display the differences between two cohorts in detail, and (e) data-type dependent comparisons of individual variable distributions. ", "caption_bbox": [73, 375, 723, 455]}, {"image_id": 1, "file_name": "3193_01.png", "page": 3, "dpi": 300, "bbox": [53, 52, 379, 262], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. CONSORT \ufb02ow diagram for a two-group randomized trial [31].", "caption_bbox": [40, 276, 380, 290]}, {"image_id": 2, "file_name": "3193_02.png", "page": 3, "dpi": 300, "bbox": [410, 57, 757, 320], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Overview of the Cadence visual analytics tool, with selection bias tracking (the focus of this paper) on the left, and temporal event sequence visualization on the right. Closeup images of the selection bias tracking components show the (a) cohort provenance tree, (b) cohort overlap, (c) detailed cohort distance, and (d) selected variable distribution visualizations. ", "caption_bbox": [405, 335, 755, 415]}, {"image_id": 3, "file_name": "3193_03.png", "page": 4, "dpi": 300, "bbox": [391, 29, 751, 205], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Examples of changing drift in a section of a hierarchy. Each dimension is a circle, with x-position the depth in the hierarchy and y- position the amount of drift. Links indicate parent-child relationships. (a) The green node does not have the highest drift, but does differ the most from its parent, indicating an area of the hierarchy where drift has increased. (b) Similarly, the circled red and blue nodes indicate an area where drift has decreased. ", "caption_bbox": [392, 225, 744, 318]}, {"image_id": 4, "file_name": "3193_04.png", "page": 5, "dpi": 300, "bbox": [412, 60, 759, 216], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Cohort provenance visualization and iconography. (a) Each cohort and \ufb01lter operation is shown as a node-link tree diagram. (b) Cohort glyph inner circle area is proportional to cohort size. Drift is encoded by a grey-red color map. The current baseline and focus cohorts are indicated by rectangular and triangular icons. Excluded cohorts that have been made visible are indicated by a diagonal slash. (c) Link color and thickness, and \ufb01lter glyph color, indicate the amount of drift introduced at that \ufb01lter step. Glyph appearance changes to indicate excluded cohort visibility. ", "caption_bbox": [405, 234, 755, 353]}, {"image_id": 5, "file_name": "3193_05.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 247], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Sorting by descending order in an icicle plot. The highest 5 values are displayed. With a standard icicle plot (left), recursive sorting strategies can be used to sort (a) from leaf to parent or (b) from parent to leaf. However, nodes may not be correctly ordered, and nodes with high values can become \u201cburied\u201d far from the top, resulting in gaps and inversions. With the split icicle plot layout (right), (c) the path from each leaf node to the root is \ufb01rst separated, and then sorted based on the maximum value along that path. (d) Adjacent nodes are then merged back together. For a given node, all paths above it will contain a value greater than or equal to its value, avoiding issues of gaps and inversions. However, some nodes may remain split to achieve this guarantee. ", "caption_bbox": [28, 263, 743, 343]}, {"image_id": 6, "file_name": "3193_06.png", "page": 7, "dpi": 300, "bbox": [464, 56, 704, 210], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Two hierarchical aggregation methods for split icicle plots. Given (a) a split, sorted, and unmerged icicle plot with salient nodes outlined in black, (b) breadth-\ufb01rst aggregation prioritizes the merging of adjacent non-salient nodes within the same level of the hierarchy. This preserves much of the hierarchical structure. (c) Depth-\ufb01rst aggregation, which can be more ef\ufb01cient for deep hierarchies, prioritizes the merging of non-salient nodes along each path from the root to a leaf. ", "caption_bbox": [405, 223, 755, 316]}, {"image_id": 7, "file_name": "3193_07.png", "page": 7, "dpi": 300, "bbox": [49, 329, 758, 599], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Hierarchical aggregation using gradient-based saliency. Nodes   a large total drift. The user can inspect the contents of each group that do not meet the saliency criterion (Equation 4) are merged into    with the cursor, which will display a standard icicle plot of the nodes groups. Nodes that do meet the criterion are outlined in black. The     contained within the group (Figure 11c). The user can select a node blue arrows indicate a group of nodes that is merged from (a) to (b),   from within the expanded group to manually de\ufb01ne the corresponding maintained from (b) to (c), and then merged with another group from (c) dimension as salient. In response, the overall layout will be updated to (d). The orange arrows indicate the same extricated node from Figure accordingly. ", "caption_bbox": [40, 598, 755, 682]}, {"image_id": 8, "file_name": "3193_08.png", "page": 9, "dpi": 300, "bbox": [96, 54, 712, 428], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12. The example use case described in Section 7.1. The temporal event sequence visualization, which is not the focus of this paper, is faded.", "caption_bbox": [40, 444, 754, 458]}], "3194": [{"image_id": 0, "file_name": "3194_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 338], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The Cadence system for temporal event sequence visualization. (a) Interactive bar charts and histograms summarize non- temporal attributes and a variety of temporal event statistics. (b) An interactive \ufb02ow-based timeline allows users to dynamically de\ufb01ne and explore pathways within the temporal event data. Selections within the timeline link to (c) a Kaplan-Meier chart to summarize outcomes-over-time. Timeline selections also link to (d and e) a novel scatter-and-focus visualization which leverages dynamic hierarchical aggregation, scenting, and optimization-based layout to support navigation of high-dimensional hierarchical event data. ", "caption_bbox": [61, 337, 711, 404]}, {"image_id": 1, "file_name": "3194_01.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 232], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. In a dataset of 16,983 diabetes patients, 5,084 also had a heart failure diagnosis (ICD-10 codes with the I50 pre\ufb01x). A total of 26,153 heart failure codes events were observed across 14 different variants. ", "caption_bbox": [392, 231, 742, 271]}, {"image_id": 2, "file_name": "3194_02.png", "page": 5, "dpi": 300, "bbox": [27, 223, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. (a) The most informative cut through the event type hierarchy is     milestone enables exploratory analysis by causing the creation of new determined by descending until the fraction of children more informative     time edges for which statistics are recursively calculated and visualized. than their parent is below a threshold R. (b) A higher R results in a higher cut. (c) A lower R will descend deeper into the hierarchy.                   4.2 Key Algorithms and Interactions ", "caption_bbox": [28, 167, 745, 220]}, {"image_id": 3, "file_name": "3194_03.png", "page": 6, "dpi": 300, "bbox": [442, 440, 714, 535], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. (a) Clicking on an event type in the scatter-plus-focus visualization transitions the chart to a focused mode. (b) The focused mode displays the selected event type, its children, and all supertypes up to the root of the hierarchy. All other types are hidden from view. ", "caption_bbox": [404, 537, 754, 590]}, {"image_id": 4, "file_name": "3194_04.png", "page": 6, "dpi": 300, "bbox": [431, 231, 723, 383], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Using only leaf event types results in many low-frequency event types. As this data from a test use case shows, hierarchical aggregation results in fewer and higher-frequency event types. ", "caption_bbox": [404, 388, 754, 428]}, {"image_id": 5, "file_name": "3194_05.png", "page": 6, "dpi": 300, "bbox": [40, 52, 756, 196], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. The level of aggregation is controlled by R, with higher R values resulting in more aggressive event type grouping and fewer visual marks.", "caption_bbox": [40, 202, 745, 216]}, {"image_id": 6, "file_name": "3194_06.png", "page": 7, "dpi": 300, "bbox": [465, 292, 666, 361], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Scented glyphs help users ef\ufb01ciently navigate the type hierarchy.", "caption_bbox": [392, 363, 744, 377]}, {"image_id": 7, "file_name": "3194_07.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 219], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Clicking on an event type in (a) the scatter-plus-focus visualization transitions to a focused view. (b) Maintaining the axes in the focused view results in overplotting, especially for the large number of low frequency events common to high-dimensional event sequences. (c) A log scale reduces overplotting for low frequency event types, but worsens problems for more frequent events and makes interpretation less intuitive. (d) An optimization-based layout maintains an intuitive scale while overcoming challenges of overplotting. ", "caption_bbox": [27, 228, 742, 281]}, {"image_id": 8, "file_name": "3194_08.png", "page": 9, "dpi": 300, "bbox": [26, 589, 394, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. Screenshots from the use case described in Section 5.1.", "caption_bbox": [28, 559, 342, 573]}], "3195": [{"image_id": 0, "file_name": "3195_00.png", "page": 1, "dpi": 300, "bbox": [27, 366, 746, 960], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Our interactive learning framework allows users to train text relevance classi\ufb01ers in real-time to improve situational awareness. In this example, a real-time tweet regarding a car accident is incorrectly classi\ufb01ed as \u201cIrrelevant\u201d. Through the SMART 2.0 interface, the user can view its label and correct it to \u201cRelevant\u201d, thereby retraining and improving the classi\ufb01er for incoming streaming data. ", "caption_bbox": [61, 314, 712, 354]}, {"image_id": 1, "file_name": "3195_01.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 192], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. High-level work\ufb02ow of our framework with three main components: tweet vectorization, tweet classi\ufb01cation, and user feedback. ", "caption_bbox": [392, 203, 743, 230]}, {"image_id": 2, "file_name": "3195_02.png", "page": 5, "dpi": 300, "bbox": [391, 631, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The total CPU time required for each model to complete the testing simulation. The CNN model is noticeably faster than both the LSTM and RNN models. ", "caption_bbox": [392, 564, 742, 604]}, {"image_id": 3, "file_name": "3195_03.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 437], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. SMART 2.0 overview: (a) The control panel provides several \ufb01lters, visualizations, and views. (b) The content lens visualization provides the most frequently used words within a selected area. (c) The tweet classi\ufb01er visualization provides keyword-based \ufb01lters to help reduce noisy data. (d)(e) Clicking a tweet on the map with the tweet tooltip visualization displays the tweet\u2019s time, message, and relevance label. (f) The topic-modeling view, based on Latent Dirichlet Allocation, extracts trending topics and the most frequently used words associated with each topic among tweets with speci\ufb01ed relevancy. (g)\u2013(j) The message table aggregates the tweets for ef\ufb01cient exploration with (g) the model\u2019s estimated classi\ufb01cation performance (F1 score), (h) a drop down box to \ufb01lter data by their relevance labels, (i) color-coded relevance labels that can be changed by clicking on the label itself, and (j) associated relevance probabilities. Tweet map symbols are colored orange and purple to distinguish Twitter data from Instagram-linked tweets, respectively, since the latter contains potentially useful images for situational awareness. ", "caption_bbox": [27, 446, 744, 552]}, {"image_id": 4, "file_name": "3195_04.png", "page": 9, "dpi": 300, "bbox": [27, 29, 747, 337], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Tweets with correctly predicted relevancy from the Purdue vs. Virginia 2019 March Madness game after the user (re)labels 80 tweets.", "caption_bbox": [28, 345, 716, 359]}], "3196": [{"image_id": 0, "file_name": "3196_00.png", "page": 1, "dpi": 300, "bbox": [73, 65, 724, 452], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The components of LightGuider : (a) a 3D modeling view to place and modify luminaires, augmented with (b) a provenance tree, depicting several sequential modeling steps and parallel modeling branches, integrating information on the quality of the individual solutions, and providing guidance by pre-simulating and suggesting possible next steps to improve the design. A \ufb01lm-strip-like visualization (c) of screenshots helps to depict the evolution up to the currently selected state. A quality view (d) informs about the ful\ufb01llment level of the illumination constraints that need to be met, using bullet charts. Changing the weights of these constraints (e), and therefore, the lighting designer\u2019s focus, triggers an update of the provenance tree node visualizations (re\ufb02ecting the weights of the constraints in the distribution of the treemap space). Moreover, the de\ufb01ned weights are also considered in the generation of new suggestions, which are tailored towards satisfying constraints with higher weights. ", "caption_bbox": [73, 465, 723, 571]}, {"image_id": 1, "file_name": "3196_01.png", "page": 3, "dpi": 300, "bbox": [403, 57, 758, 630], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. A measuring surface on the desktop, and glare probes (simu- lating a person\u2019s \ufb01eld of view while working on this desk) are used to gather illumination information, such as the average illuminance and the uniformity on an area, or if a person is likely to be blinded. Depending on the semantics, the target values of the measurement objects are set according to industry norms. Only designs in which all of these target values are reached are considered valid. To fully support linking and brushing in case of occlusions, outline-based highlighting enhancements and semi-transparent area overlays are rendered without depth-testing. ", "caption_bbox": [405, 644, 757, 763]}, {"image_id": 2, "file_name": "3196_02.png", "page": 5, "dpi": 300, "bbox": [50, 52, 738, 274], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The ful\ufb01llment levels of the illumination constraints of a modeling state, represented both as a treemap (see Sect. 5.4.1) and using bullet charts (see Sect. 5.3). In the summary-view (a and b), the worst illumination values of a group are depicted. When requesting more details, the information for all measuring surfaces is displayed (c and d). Linking & brushing and tooltips help to easily grasp shortcomings of the modeling state. ", "caption_bbox": [40, 287, 757, 327]}, {"image_id": 3, "file_name": "3196_03.png", "page": 6, "dpi": 300, "bbox": [28, 29, 745, 210], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. A simple, exemplary hierarchy of illumination constraints in a scene, mapped to the treemap layout. Measurement objects can be grouped into semantic groups by the designer, such as groups with measuring surfaces for desktops and \ufb02oors, and a general group for glare probes. In the summary-view of the treemap, the worst values of the groups are visualized, and each color occupies a sixth of the treemap space (distributed over the groups). In detail-view, which can be activated for each group separately, the values for the individual measurement objects are displayed. ", "caption_bbox": [28, 228, 743, 281]}, {"image_id": 4, "file_name": "3196_04.png", "page": 6, "dpi": 300, "bbox": [444, 306, 692, 479], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The global comparison visualization of the provenance tree (see Sect. 5.4.2) uses a grayscale color scheme to get an overview on the quality differences of different design states with respect to the currently selected node (in red). Its ful\ufb01llment levels of illumination constraints act as the reference, and are visualized in medium gray. Illumination constraints that are ful\ufb01lled better in other solutions are visualized lighter, and the ones that are ful\ufb01lled worse are displayed darker. ", "caption_bbox": [392, 492, 743, 585]}, {"image_id": 5, "file_name": "3196_05.png", "page": 7, "dpi": 300, "bbox": [58, 52, 737, 158], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Table 1. The unweighted performance values ai j describing the useful- ness of modeling actions to improve a speci\ufb01c illumination constraint. ", "caption_bbox": [405, 249, 757, 276]}], "3197": [{"image_id": 0, "file_name": "3197_00.png", "page": 1, "dpi": 300, "bbox": [40, 29, 763, 508], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. PlanningVis supports interactive exploration, comparison and optimization of production plans. (a) The control panel enables interactively building the con\ufb01guration data of the planning algorithm. (b) The plan overview summarizes each plan and their differences. (c) The product view reveals the distribution of all the products (c1 ) and presents various properties of the selected products (c2 ). (d) The production detail view displays the dependency between products (d1 , d2 ) and the daily production details in related plants (d3 , d4 ). ", "caption_bbox": [73, 520, 724, 576]}, {"image_id": 1, "file_name": "3197_01.png", "page": 5, "dpi": 300, "bbox": [40, 29, 758, 225], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The glyph design to represent a plan in the plan overview. (a) data, which follow the same order as that of the plan glyph. The size The glyph design employed in PlanningVis. (b) A star-plot based glyph  of the triangle encodes the value while the orientation of the triangle design. (c) A treemap-based glyph design.                              describes the increase (up) or the decrease (down) of the value. A ", "caption_bbox": [40, 224, 755, 269]}, {"image_id": 2, "file_name": "3197_02.png", "page": 7, "dpi": 300, "bbox": [40, 29, 394, 190], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. System architecture. PlanningVis consists of three major mod- ules: (a) data storage, (b) data processing and (c) visualization. The data storage module collects and stores the con\ufb01guration data and the result of a hybrid production planning algorithm. The data processing module pre-processes the anomalous data, computes the performance indica- tors and evaluates the difference between two production plans. The visualization module supports interactive exploration and comparison of production plans through three well-coordinated views. ", "caption_bbox": [40, 207, 392, 314]}, {"image_id": 3, "file_name": "3197_03.png", "page": 7, "dpi": 300, "bbox": [405, 308, 756, 444], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The product glyphs (a) and daily production (b) of Ser- vice Router 20 and Gateway 60 after increasing the capacity sets by 50% for 30 days. The order delay rate decreases because the production is increased or \ufb01nished before the delivery date of the order. ", "caption_bbox": [405, 235, 756, 290]}, {"image_id": 4, "file_name": "3197_04.png", "page": 7, "dpi": 300, "bbox": [391, 558, 759, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. The production dependency and daily production of Routers 22, Routers 491 and Service Router 18 after increasing the initial inven- tory of the raw material common 32 from 1000 to 8000. The order delay rate of Routers 22 and Service Router 18 decreases while that of Routers 491 is still high. ", "caption_bbox": [405, 456, 757, 524]}, {"image_id": 5, "file_name": "3197_05.png", "page": 8, "dpi": 300, "bbox": [391, 29, 745, 382], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. (a) The overview of plans generated during the rapid response to typhoon Mangkhut. (b, c) Plant 1 is shut down for two days due to the typhoon Mangkhut. The order delay rate of many products is increased since the production is put off. (d, e) After increasing Plant 1 Capacity Set 11 and Plant 1 Capacity Set 3 by 50% for 30 days, the order delay rate of many products decreases, while the production cost and the smoothing rate of production capacity use increase. It is because the production is increased or advanced. ", "caption_bbox": [392, 396, 743, 502]}, {"image_id": 6, "file_name": "3197_06.png", "page": 8, "dpi": 300, "bbox": [28, 29, 394, 199], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. (a) The order demand of Routers 491 is delayed because of the requirement that common 32 can only be used to serve the order of Routers 22 and Service Router 18. (b) After removing the restriction, the production of Routers 491 is increased and the order delay rate drops. ", "caption_bbox": [28, 211, 378, 266]}], "3198": [{"image_id": 0, "file_name": "3198_00.png", "page": 1, "dpi": 300, "bbox": [400, 849, 747, 934], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 2. Traditional spectrum diagrams used to visualize SpeData and SigData: (a) a SpeData frame in a spectrum amplitude-frequency diagram, (b) a group of sequential spectrum frames in a spectrum time-frequency diagram, (c) SigData in a spectrum amplitude-frequency diagram, and (d) SigData in a spectrum time-frequency diagram. ", "caption_bbox": [396, 935, 750, 999]}, {"image_id": 1, "file_name": "3198_01.png", "page": 1, "dpi": 300, "bbox": [403, 697, 743, 781], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 1. Acquisition of SpeData and SigData: (a) radio sensing equipment, (b) AD sampling sequence data, (c) SpeData converted from AD sampling sequence data through fast Fourier transform, and (d) SigData extracted from SpeData and AD sampling sequence data. ", "caption_bbox": [396, 786, 750, 837]}, {"image_id": 2, "file_name": "3198_02.png", "page": 3, "dpi": 300, "bbox": [401, 683, 749, 766], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 3. System work\ufb02ow.", "caption_bbox": [397, 772, 527, 785]}, {"image_id": 3, "file_name": "3198_03.png", "page": 5, "dpi": 300, "bbox": [28, 29, 747, 429], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 4. System interface consists of a control panel (a) and two functional modules. The monitoring module provides a situation monitoring view (b) and a signal monitoring view (c) to help users achieve situation perception. The situation monitoring view (b) presents an overview of the current electromagnetic situation and its recent change trend. The signal monitoring view (c) uses an STF diagram with a river visualization mode to depict the signal distribution in time and frequency and the time-varying patterns of the signals\u2019 characteristics. Its visual encoding is illustrated in (d). ", "caption_bbox": [30, 437, 749, 488]}, {"image_id": 4, "file_name": "3198_04.png", "page": 6, "dpi": 300, "bbox": [39, 29, 758, 426], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 5. Exploring module provides four coordinated visualization views to help users achieve situation understanding. The situation exploring view (a) provides the situation information of the explored time period and frequency band. The signal exploring view (b) uses an STF diagram with a scatter visualization mode to support a multi-perspective and interactive SigData exploration. The spectrum time-frequency view (c) and spectrum amplitude-frequency view (d) present the corresponding SpeData to facilitate a joint analysis of SigData and SpeData for high-risk situation understanding. ", "caption_bbox": [40, 437, 761, 488]}, {"image_id": 5, "file_name": "3198_05.png", "page": 8, "dpi": 300, "bbox": [391, 563, 758, 814], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 8. Situation understanding of RISK2. (a) Two sub-signals of authorized signal A23 are found. (b) The two sub-signals last for the entire time period. (c) Two bright ribbons at the frequencies of A23-1 and A23-2 overlap. (d) A23-2 is interactively set as a separated signal by using a signal splitting interaction. ", "caption_bbox": [406, 820, 758, 884]}, {"image_id": 6, "file_name": "3198_06.png", "page": 8, "dpi": 300, "bbox": [391, 29, 762, 494], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Figure 7. Situation understanding of RISK1. (a) Three unauthorized signals (U1, U2, and U3) with low SNR values are found. (b) No clear bright vertical ribbon is observed at the frequencies of U1\u2212U3. (c) The spectrum amplitudes at the frequencies of U1\u2212U3 are very low. ", "caption_bbox": [406, 502, 759, 553]}], "3199": [{"image_id": 0, "file_name": "3199_00.png", "page": 1, "dpi": 300, "bbox": [73, 65, 724, 462], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. With sPortfolio, we can observe the management and strategy of stock portfolios from different perspectives. A) The portfolio cluster view gives an overview of all stock portfolios within a given time-period. B) The factor correlation view reveals the return correlations of risk factors, which can be used to validate the effectiveness of the factors in a multi-factor model. C) The comparison view is designed to compare the risk preference and sector position of portfolios, which reveals their strategies. D) The individual portfolio view illustrates the stock holdings and trading style of a speci\ufb01c portfolio. ", "caption_bbox": [73, 477, 722, 542]}, {"image_id": 1, "file_name": "3199_01.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 314], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. System Overview. The three modules included in our system sPortfolio: 1) Database; 2) Data Service; 3) Visualization, showing the detailed work\ufb02ows inside each module. ", "caption_bbox": [392, 326, 742, 365]}, {"image_id": 2, "file_name": "3199_02.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 243], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Overview of two portfolios. The upper parts are factor signatures (L1 and R1) and the lower parts are sector graphs (L2 and R2). The blue lines, in L1, are sparsely distributed and do not converge on any axis. This means that the portfolio does not have a consistent risk preference. On the right side, the lines in R1 are densely distributed and often converge to one to two points where they pass through the axes. This means the factor exposures of the portfolio are less \ufb02uctuated, indicating a clear factor strategy. The horizons of the sector graph have longer segments in L2. Conversely, in R2 there are many short segments with many breaks in between. These observations show that the portfolio 8248 is managed by sector strategy while 5766 by factor strategy. ", "caption_bbox": [392, 257, 744, 396]}, {"image_id": 3, "file_name": "3199_03.png", "page": 7, "dpi": 300, "bbox": [51, 52, 748, 318], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Various portfolios displayed in the comparison view. The line chart on the top of each cluster shows the cumulative returns in the selected cluster. The portfolio overviews (5192, 4359, 4868, 7291) are exhibited in two different periods. This shows that, 5192 and 4359 had very similar risk factor exposures as a group during 2016/06-2016.09. The same was true for 4868 and 7291 during 2016/07-2017/01. Finally, their returns differed drastically from each other within the group. Meanwhile, within the groups, the portfolios had very different industry holdings. ", "caption_bbox": [40, 317, 755, 368]}, {"image_id": 4, "file_name": "3199_04.png", "page": 8, "dpi": 300, "bbox": [392, 310, 746, 872], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Performance and factor return of the portfolio 5716 during different time periods. A) The portfolio took high size and non-linear size risks during 2016/01-2016/07. The portfolio enjoyed high return. B) The same portfolio (5716) maintained a similar strategy (factor and sector position) during 2017/01-2017/07, whereas the returns performed adversely. C) The size factor\u2019s return dropped during 2016/01-2016/07, which meant the portfolios that took negative size factor exposure would have ben- e\ufb01ted. D) The size factor\u2019s return was raised during 2017/01-2017/07, which meant the portfolio that took positive size factor exposure would have suffered from loss. The same strategy may perform totally differently under different market situations and factor crowdedness. ", "caption_bbox": [392, 880, 744, 1019]}, {"image_id": 5, "file_name": "3199_05.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 244], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Stock position details for the portfolios in Case 1, including 4359 (top) and 7291 (bottom). The vertical levels of points on the lines of 5192 do not vary so frequently comparing to the case of 7291, which shows that 7291 possessed a higher trading frequency. More sticks appear in 7291, indicating that more stocks are involved in the portfolio management. ", "caption_bbox": [28, 249, 744, 288]}], "3200": [{"image_id": 0, "file_name": "3200_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 466], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The system interface of AirVis: (A) the control panel enables the interactive mining, \ufb01ltering, and selection of the propagation patterns; (B) the motif view presents the extracted signi\ufb01cant motifs with uncertainty-aware visualizations; (C) the pattern view depicts the patterns with compact pattern glyphs and pattern graphs; (D) the instance view helps users inspect the propagation instances. ", "caption_bbox": [61, 479, 709, 518]}, {"image_id": 1, "file_name": "3200_01.png", "page": 4, "dpi": 300, "bbox": [40, 29, 758, 221], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The pipeline of the proposed frequent pattern mining framework: (A) two transportation instances that describe the pollutants transported between 1 and 2 (larger) and between 1 and 3 (smaller); (B) a propa- gation instance that involves four transportation instances; (C) a propa- Fig. gation pattern identi\ufb01ed from four propagation instances, each encoded    The with a color; (D) a propagation graph (i.e., a propagation instance) con- sam structed from massive transportation graphs; (E) a frequent subgraph (i.e., a propagation pattern) extracted from massive graph transactions.  esti ", "caption_bbox": [39, 220, 425, 326]}, {"image_id": 2, "file_name": "3200_02.png", "page": 6, "dpi": 300, "bbox": [40, 29, 394, 153], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. The extraction of motifs: (A) three structurally similar propagation graphs, where the summary topology is circled in blue; (B) the summary (i.e., signi\ufb01cant motif) and corrections generated by the MDL principle. ", "caption_bbox": [39, 160, 389, 200]}, {"image_id": 3, "file_name": "3200_03.png", "page": 6, "dpi": 300, "bbox": [411, 166, 744, 232], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. The design alternatives to (A) the uncertainty visualiztion in the motif glyphs, (B) pattern glyphs, and (C) (D) pattern graphs. ", "caption_bbox": [404, 239, 754, 266]}, {"image_id": 4, "file_name": "3200_04.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 221], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Three methods to generate the vector representation of a pat- tern, (A) concatenating the one-hot encoded feature vectors, (B) using transactions with one-hot encodings directly, and (C) learning the vector representations of the patterns based on the word2vec model. ", "caption_bbox": [392, 228, 744, 281]}, {"image_id": 5, "file_name": "3200_05.png", "page": 8, "dpi": 300, "bbox": [391, 30, 758, 447], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. (A) The signi\ufb01cant motif that involves Beijing. There are three types of the propagation processes involving Beijing, namely, those (B) from Baoding to Beijing, (C) from Beijing to Baoding, and (D) from Beijing to Huailaixian. (E) The propagation instances from Baoding to Beijing. ", "caption_bbox": [404, 455, 754, 508]}, {"image_id": 6, "file_name": "3200_06.png", "page": 8, "dpi": 300, "bbox": [39, 29, 394, 221], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. The propagation pattern involving Binhaixinqu. (A) Air pollutants are propagated from Tangshan and the downtown of Tianjin. (B) Signi\ufb01- cant pollution correlations among the districts (dashed circles enclose the correlated temporal ranges among these districts). ", "caption_bbox": [40, 229, 392, 282]}], "3201": [{"image_id": 0, "file_name": "3201_00.png", "page": 1, "dpi": 300, "bbox": [73, 65, 724, 440], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Morphing transitions from OD bundling (left) to trajectory heatmap (right) improves faithfulness to actual movement paths.", "caption_bbox": [73, 450, 721, 464]}, {"image_id": 1, "file_name": "3201_01.png", "page": 3, "dpi": 300, "bbox": [50, 52, 391, 147], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Examples of concepts.", "caption_bbox": [140, 149, 291, 163]}, {"image_id": 2, "file_name": "3201_02.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 254], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: OD Morphing Pipeline.", "caption_bbox": [307, 265, 463, 279]}, {"image_id": 3, "file_name": "3201_03.png", "page": 5, "dpi": 300, "bbox": [87, 56, 757, 152], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                            4.4    Iteration process for degrees of faithfulness Fig. 5: An example of movement directions of points on a bundled curve. The point c is the closest point to the waypoint w and it will       OD Morphing is an iterative process with multiple parallel processes. move toward directly to w. The rest of points on the curve, such as         We summarize them here. Given a set of bundled OD curves {B} and a xi and x j , will move along a combined direction: one towards their        set of paths {P} taken between those ODs, the recursive scheme has                          #\u00bb                  # \u00bb and wd                                                 the following steps: projections on ow           and the other is local kernel density gradient. ", "caption_bbox": [40, 151, 757, 232]}, {"image_id": 4, "file_name": "3201_04.png", "page": 7, "dpi": 300, "bbox": [52, 426, 757, 654], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Plots showing how faithfulness (overlap, closeness, and Fre\u0301chet is further clari\ufb01ed by the later morphing steps I(c) to I(d) where those similarity) vary with simplicity (ink saving and curvature), comparing   trips are gradually aligned to the ring roads. ", "caption_bbox": [40, 653, 755, 685]}, {"image_id": 5, "file_name": "3201_05.png", "page": 7, "dpi": 300, "bbox": [40, 52, 752, 379], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: Frames of (I) OD Morphing and (II) TB on taxi       data. sink and scurvature are the simplicity scores, and fcloseness , foverlap and fshape are faithfulness scores. The color of the bundles ranges from blue (more simple) to red (more faithful w.r.t. closeness and overlap). The green dots in frames (I) are the critical waypoints. The size of a dot is in proportion to its waypoint importance (Eq. 1). ", "caption_bbox": [40, 378, 755, 420]}, {"image_id": 6, "file_name": "3201_06.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 187], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Frames of OD Morphing on USA \ufb02ight data. (b) shows the critical waypoints w1 = ELP, w2 = REDFN and w3 = LEV, respectively.", "caption_bbox": [39, 192, 731, 208]}, {"image_id": 7, "file_name": "3201_07.png", "page": 9, "dpi": 300, "bbox": [131, 64, 304, 203], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: Critical waypoints generated by G-MC and BK-MC.", "caption_bbox": [61, 202, 368, 216]}, {"image_id": 8, "file_name": "3201_08.png", "page": 9, "dpi": 300, "bbox": [391, 190, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Ef\ufb01ciency vs optimal gap.", "caption_bbox": [492, 170, 667, 184]}], "3202": [{"image_id": 0, "file_name": "3202_00.png", "page": 1, "dpi": 300, "bbox": [83, 65, 713, 511], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: A screenshot of the REMAP system. In the Model Overview, section A, a visual overview of the set of sampled models is shown. Darkness of circles encodes performance of the models, and radius encodes the number of parameters. In the Model Drawer, section B, users can save models during their exploration for comparison or to return to later. In section C, four tabs help the user explore the model space and generate new models. The Generate Models tab, currently selected, allows for users to create new models via ablations, variations, or handcrafted templates. ", "caption_bbox": [73, 522, 723, 591]}, {"image_id": 1, "file_name": "3202_01.png", "page": 5, "dpi": 300, "bbox": [41, 52, 756, 275], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: (a) The model inspection tab lets users see more granular information about a highlighted model. This includes a confusion matrix showing which classes the model performs best on or misclassi\ufb01es most frequently. Users can also view training curves to determine if an architecture might be able to continue to improve if trained further. (b) By selecting individual classes from the validation data, users can update the darkness of circles in the the Model Overview to see how all models perform on a given class. ", "caption_bbox": [40, 288, 755, 343]}, {"image_id": 2, "file_name": "3202_02.png", "page": 6, "dpi": 300, "bbox": [27, 29, 746, 725], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Controls for creating (a) Ablations and (b) Variations. Users described in section 4. Two months later, a validation study was held toggle between the two types of model generation with a radio button. with the same four model builders that participated in the design study. Ablations create a set of models, one for each layer with that layer  The goal of the validation study was to assess whether the features of removed, to communicate the importance of each layer. The Variations  REMAP were appropriate and suf\ufb01cient to enable a semi-automated feature runs constrained searches in the neighborhood of a selected   model search, and to determine if the system aligned with the mental model. Users toggle which types of variations are allowed for each    model of deep learning model builders. Users were asked to complete layer, as well as the number of variations allowed per model          two tasks using REMAP, and then provide feedback on how individual ", "caption_bbox": [27, 724, 744, 821]}, {"image_id": 3, "file_name": "3202_03.png", "page": 7, "dpi": 300, "bbox": [45, 58, 755, 221], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: (a) The SNAC visual encoding of a neural network architectures, seen at four different resolutions. This architecture has a three convolutions, each followed by an activation, and concludes with a fully connected layer. (b) Three alternative visual overviews of the model space. Section A shows the set of models on a set of interpretable axes, validation accuracy vs. log of the number of parameters. Sections B and C use multidimensional scaling to lay out the same set of models based on structural similarity (B) and prediction similarity (C). The darkness of the circle encodes the model accuracy, and the radius of the circle encodes the log of the number of parameters. ", "caption_bbox": [40, 234, 755, 303]}, {"image_id": 4, "file_name": "3202_04.png", "page": 8, "dpi": 300, "bbox": [27, 29, 393, 252], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: The ability to handcraft models was added based on feedback from a validation study with model builders. Starting from a model baseline, users can remove, add, or modify any layer in the model by clicking on a layer or connections between layers. This provides \ufb01ne-grained control over the models that are generated. ", "caption_bbox": [28, 262, 378, 331]}, {"image_id": 5, "file_name": "3202_05.png", "page": 8, "dpi": 300, "bbox": [391, 29, 751, 255], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: In our use case, the model builder \ufb01rst samples models 1, 2, and 3 on the pareto front of accuracy vs. model size. He then selects models 4 and 5 from the two alternative Model Overviews provided. ", "caption_bbox": [392, 266, 743, 307]}], "3203": [{"image_id": 0, "file_name": "3203_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 499], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. The default layout of the front-end of VASSL: A) the timeline view, B) the dimensionality reduction view, C) the user/tweet detail views, D) & E) the topics view (clustering / words), F) the feature explorer view, G) the general control panel, H) the labeling panel, and I) the control panels for all the views (the opened control panel in the \ufb01gure is for topics clustering view). ", "caption_bbox": [60, 512, 709, 552]}, {"image_id": 1, "file_name": "3203_01.png", "page": 4, "dpi": 300, "bbox": [404, 52, 756, 209], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The timeline view visualizing three time series features at the year aggregation level. Blue shows unlabeled accounts, green for genuine accounts, and purple indicates spambots. ", "caption_bbox": [404, 222, 754, 262]}, {"image_id": 2, "file_name": "3203_02.png", "page": 4, "dpi": 300, "bbox": [404, 278, 756, 455], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The three details views. Selecting one or more accounts from the cards view shows their tweets in the tweets view and a word cloud of all the tweets in the word cloud view. ", "caption_bbox": [404, 468, 754, 508]}, {"image_id": 3, "file_name": "3203_03.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 306], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. The topics clustering view with one topic selected. The bubble chart in the bottom shows the topics with bubble size communicating topics\u2019 scores. The word cloud on the top shows the topics\u2019 words with word sizes representing words-topics distribution. ", "caption_bbox": [392, 319, 742, 372]}, {"image_id": 4, "file_name": "3203_04.png", "page": 7, "dpi": 300, "bbox": [27, 459, 744, 756], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. 1) Abnormality in tweet count on 2014. Scrolling changes the timeline view to month-aggregation-level for the 2014 period. 2) Brush on the month of October 2014 to select a group of accounts with abnormal tweeting frequency. 3) Identify frequent words in the most frequent topics the selected accounts post in. 4) Open tweet view to examine selected accounts tweets. 5) Search tweets for the captured most frequent word in outlined topics. 6) Examine the tweets contain that word which leads to a discovery of automated-like tweets. 7) Label selected accounts as spambots. ", "caption_bbox": [28, 769, 742, 822]}, {"image_id": 5, "file_name": "3203_05.png", "page": 7, "dpi": 300, "bbox": [28, 29, 745, 414], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. The control panels. Users can open a control panel of a view by pressing the \u201cC\u201d button on the keyboard while hovering over the view.", "caption_bbox": [28, 431, 718, 445]}, {"image_id": 6, "file_name": "3203_06.png", "page": 8, "dpi": 300, "bbox": [404, 52, 756, 405], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. The objective performance of VASSL compared to manual labeling in terms of the precision, recall, and F1 score for the spambot class. We also include the overall accuracy of the labeling, which consider both spambot and genuine classes. ", "caption_bbox": [404, 418, 754, 471]}, {"image_id": 7, "file_name": "3203_07.png", "page": 9, "dpi": 300, "bbox": [27, 29, 745, 345], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. The subjective opinion about VASSL in terms of perceived usefulness and ease of use. The \ufb01gure compares VASSL and manual labeling from user perspective. ", "caption_bbox": [28, 359, 742, 386]}], "3204": [{"image_id": 0, "file_name": "3204_00.png", "page": 2, "dpi": 300, "bbox": [391, 29, 758, 657], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. We de\ufb01ne the three levels of integration between models and visual interfaces: (a) passive observation, (b) interactive observation, and (c) interactive collaboration. Each successive stage extends the design space by adding new potential interactions. In the interactive collaboration stage, the model itself is extended with \u201chooks\u201d to enable semantic interactions. ", "caption_bbox": [404, 682, 756, 762]}, {"image_id": 1, "file_name": "3204_01.png", "page": 4, "dpi": 300, "bbox": [40, 29, 758, 572], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The advantage of latent variables is the transparent reasoning       becomes increasingly more challenging and the model performance process. CSI requires model reasoning, illustrated by a lever that dictates  might degrade. Moreover, the increase in potential user interactions which turn a train is taking. In the forward inference direction, we know    with additional hooks might overwhelm users. As a consequence, we in both cases where the train will end up, but only the lever allows us to   focus on a model with a single hook throughout our use case and show know what path it will take. Similarly, in the backward inference direction, how a single hook can already enable many powerful interactions. we know where the train originated, but only the lever shows the track it       We now describe our co-design approach for a CSI system for docu took to get there.                                                           ment summarization and present the lessons we learned from our CSI ", "caption_bbox": [39, 571, 754, 664]}, {"image_id": 2, "file_name": "3204_02.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 449], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Overview of the CSI:Summarization visual interface. (a) shows the input and overlays the currently selected content selection (blue) and the current content of the summary (red). (b) shows a connection between the input and the current summary through the attention, which shows explicitly where words in the summary came from. (c) shows the current summary, (d) shows proxy elements for both input and output groups. This enables an overview of a document, even when the text does not \ufb01t on one page. (e) allows the user to request suggestions from the model, (f) enters the edit mode and adds a new sentence to the summary. (g) toggles whether the text should be aggregated into sentences. (h) provides quick selections for the content selection (blue) by being able to match the red highlights, or (de-)select everything. ", "caption_bbox": [28, 469, 743, 549]}, {"image_id": 3, "file_name": "3204_03.png", "page": 6, "dpi": 300, "bbox": [47, 667, 749, 951], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Anna selects a sentence that is currently not covered by the summary, indicated by the lack of a red border on the right (a). She generates a new sentence (b) and corrects the repeated phrase \u201cnasa scientists say\u201d (c). With the \ufb01nished draft of her summary, she can now evaluate the coverage of the input document (d) and the \ufb01nal summary (e). ", "caption_bbox": [40, 965, 755, 1005]}, {"image_id": 4, "file_name": "3204_04.png", "page": 6, "dpi": 300, "bbox": [40, 29, 758, 367], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. After selecting the content shown on the left, Anna requests the model to generate a fourth sentence (a). She does not like the suggestion and deletes it (b). To in\ufb02uence the model to generate about other topics in the input, she deselects the sentences that caused the suggestion (c). ", "caption_bbox": [40, 382, 755, 409]}, {"image_id": 5, "file_name": "3204_05.png", "page": 6, "dpi": 300, "bbox": [58, 441, 737, 594], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Anna wants to generate a sentence about the water on Mars and starts typing \u201cThe water is ...\u201d (a). This initiates the model to \ufb01nish the sentence for her (b). To correct a minor mistake in the generated sentence, Anna activates the edit mode (c) and replaces the wrong word (d). ", "caption_bbox": [40, 607, 755, 635]}, {"image_id": 6, "file_name": "3204_06.png", "page": 8, "dpi": 300, "bbox": [39, 713, 758, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                            ately. If, for example, a speci\ufb01c model hook is performing so poorly Fig. 7. Visual design iterations for CSI:Summary. (a) shows radio                                                                             that it cannot facilitate the user\u2019s mental model, it will be immediately buttons for selecting a speci\ufb01c selection (content selection vs. backward                                                                             revealed. On the other hand, requirements for a visualization might model). In (b), we introduced a mixed mode that showed the user content selection in the \ufb01nal blue color and the result from the backward model                                                                             over-constrain a model such, that it breaks. E.g., creating a system for with a red highlight (c). Underlines later replaced the dominant highlight. poem writing which should suggest lines of text that rhyme, even for ", "caption_bbox": [39, 640, 754, 714]}, {"image_id": 7, "file_name": "3204_07.png", "page": 8, "dpi": 300, "bbox": [391, 29, 756, 253], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. CSI interfaces require a design process that spans machine learning and visual interaction design. The feedback loop is used to produce interpretable semantic representations that inform the reasoning procedure of the model while providing useful visual interaction. ", "caption_bbox": [404, 272, 754, 325]}], "3205": [{"image_id": 0, "file_name": "3205_00.png", "page": 1, "dpi": 300, "bbox": [73, 65, 723, 479], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Our visualization system supports emotion analysis across three modalities (i.e., face, text, and audio) at different levels of details. The video view (a) summarizes each video in the collection and enables quick identi\ufb01cation of videos of interest. The channel coherence view (b) shows emotion coherence of the three modalities at the sentence level and provides extracted features for channel exploration. The detail view (c) supports detail exploration for a selected sentence with some highlighted features and transition points. The sentence clustering view (d) provides a summary of the video and reveals the temporal patterns of emotion information. The word view (e) enables ef\ufb01cient quantitative analysis at the word level in the video transcript. ", "caption_bbox": [73, 478, 721, 561]}, {"image_id": 1, "file_name": "3205_01.png", "page": 4, "dpi": 300, "bbox": [27, 181, 746, 556], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Our visualization system pipeline for multimodal emotion          users need to go through a trial-and-error process. Thus, it is helpful for analysis of presentation videos. In the data processing phase, we utilize users to interact with the data directly, so they can observe and interpret well-established methods to extract emotion information from different    the results based on knowledge. ", "caption_bbox": [27, 555, 742, 600]}, {"image_id": 2, "file_name": "3205_02.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 119], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: A design for summarizing the emotion information of the three channels of a video. The line at the top explicitly shows the emotion coherence of the three channels. The bar code chart at the bottom shows more details about the exact emotions of each channel. ", "caption_bbox": [393, 124, 743, 179]}, {"image_id": 3, "file_name": "3205_03.png", "page": 5, "dpi": 300, "bbox": [44, 56, 391, 151], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Two alternative designs for summarizing emotion information from the three channels. (a) Three different curves represent the three channels. The position of each curve indicates the current emotion, as the background has eight different-colored emotion bands. (b) Three straight lines represent three channels. Each color dot represents an emotion at one moment. ", "caption_bbox": [40, 150, 390, 233]}, {"image_id": 4, "file_name": "3205_04.png", "page": 5, "dpi": 300, "bbox": [405, 48, 756, 212], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: An augmented Sankey diagram design for summarizing emo- tion coherence from three channels as well as for providing extracted features for explanations. Each node represents one type of emotion and each link represents a collection of sentences with certain emotions shared by two channels, either the face and text channels or the text and audio channels. (a) A treemap-based design to show a quick overview of representative detected faces in the video. (b) A word cloud design to highlight some important words, which gives users some hints about the corresponding context. (c) A histogram design to show the audio feature distribution for different emotions. ", "caption_bbox": [405, 211, 757, 349]}, {"image_id": 5, "file_name": "3205_05.png", "page": 6, "dpi": 300, "bbox": [27, 268, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Two alternative designs that were considered for the channel coherence view. (a) A chord diagram design for showing emotion         Fig. connection of different channels. Each channel is represented by an    base arc, and each connection is represented by a link. (b) A normal Sankey with diagram design for showing emotion connection between different        as w channels. Emotions in each channel are represented by nodes; their connections are represented by links.                                     J ", "caption_bbox": [28, 172, 416, 269]}, {"image_id": 6, "file_name": "3205_06.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 621], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: A deadpan humor presentation style. The speaker lacks emotions most of the time when saying something interesting. (a) Select a video with many neutral facial emotions and happy text content. (b) Click the link with happy text and audio emotions in the channel coherence view (Fig. 5), then two sentences represented by the Glyph 1 and Glyph 27 are highlighted. Further, brush an area to \ufb01nd out similar sentences with the highlighted sentences. (c) Sort the word view by frequency to \ufb01nd out which words the speaker tends to use. (d) Some sentences are highlighted with red dashed rectangles in the bar code due to the brush interaction in (b). In particular, we examine three example sentences with neutral facial emotions at \ufb01rst and happy emotions at the end. The detailed context of the \ufb01rst example sentence is shown. ", "caption_bbox": [27, 620, 742, 703]}], "3206": [{"image_id": 0, "file_name": "3206_00.png", "page": 1, "dpi": 300, "bbox": [40, 29, 756, 519], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. M OTION B ROWSER interface showing how to analyze patients\u2019 limb muscles and movement with data collected from muscle sensors, motion sensors, and video recordings.      A Muscle Bundle Comparison View displays the muscle signals of affected and unaffected limbs side by side. Statistics from motion sensors (a1 ) and stacked muscle activities (a2 ) are shown. Visual highlighting technique allows the extraction of the relatively stronger muscle activities on both sides (a3 ).                                                                                                     B Time Series View on raw muscle EMG signals. Each view visualizes the signals from an individual motion and users can align the x- and y-axis of all views (b1 ).     C Video Inspection View displays the cut scenes and \ufb01ltered signals from    A and allows the export to the presentation slide show (c1 ). ", "caption_bbox": [73, 521, 722, 602]}, {"image_id": 1, "file_name": "3206_01.png", "page": 2, "dpi": 300, "bbox": [391, 29, 745, 184], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Examples of current tools for analyzing EMG data of muscle activities. (a) Signals of muscle activities and different corresponding physical motions are put together to be inspected in Spike2. (b) Excel Spreadsheet displaying the statistics of muscle activities such as max and min value and maximum range of motions. ", "caption_bbox": [392, 191, 742, 258]}, {"image_id": 2, "file_name": "3206_02.png", "page": 4, "dpi": 300, "bbox": [27, 29, 745, 358], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Hierarchical task abstraction of the clinical work\ufb02ow. Each box represents a task or subtask and each level of hierarchy has a plan. The horizontal line at the bottom of the box means a termination. The highlighted purple text represents the task abstraction derived from the tasks. ", "caption_bbox": [28, 371, 742, 398]}, {"image_id": 3, "file_name": "3206_03.png", "page": 5, "dpi": 300, "bbox": [391, 29, 757, 351], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Entropy-based visual highlighting: (a) Transform the signals of two limbs to histograms of value, then (b) compare the ordinal distributions of the histogram. (c) The similar and smaller muscle signals are reduced so that signi\ufb01cant signals eventually stand out. ", "caption_bbox": [405, 364, 755, 417]}, {"image_id": 4, "file_name": "3206_04.png", "page": 6, "dpi": 300, "bbox": [391, 29, 745, 136], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. When users remove a muscle, the corresponding bar charts will collapse and the remaining ones may rescale to become more visible. ", "caption_bbox": [392, 149, 742, 176]}, {"image_id": 5, "file_name": "3206_05.png", "page": 7, "dpi": 300, "bbox": [391, 29, 758, 184], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. System architecture for data storage, modeling, and visualization.", "caption_bbox": [405, 192, 757, 206]}, {"image_id": 6, "file_name": "3206_06.png", "page": 8, "dpi": 300, "bbox": [27, 29, 745, 332], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. After using the analytic work\ufb02ow shown in Fig. 5 on every patient to compare each affected and unaffected limbs, our experts summarized three groups of patients to evaluate the usefulness of trapezius muscles (pink and red) on shoulder motions. A: Patients using extra powers on almost every muscle on the affected limbs shown in  ;  i  B: Patients with signi\ufb01cantly stronger trapezius muscle activities on his unaffected limb shown in  ;ii C: Patients using more trapezius muscles shown in  .                                                                  iii Further inspections in                                                                                               ii suggested that profound debilitation was not signi\ufb01cant among all patients in group B, while it could be observed among several patients in group C. These provide evidence to support spinal accessory denervation on patients with Obstetrical Brachial Plexus Palsy. ", "caption_bbox": [28, 343, 743, 423]}], "3207": [{"image_id": 0, "file_name": "3207_00.png", "page": 1, "dpi": 300, "bbox": [79, 65, 719, 426], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Guided relevance feedback for the targeted re\ufb01nement of incoherent areas in the Semantic Concept Space. This user guidance component tours through the space and highlights potentially uncertain areas, suggesting a recommended action for re\ufb01nement. ", "caption_bbox": [73, 436, 721, 464]}, {"image_id": 1, "file_name": "3207_01.png", "page": 2, "dpi": 300, "bbox": [391, 29, 745, 221], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: The human-in-the-loop work\ufb02ow for Semantic Concept Spaces.", "caption_bbox": [392, 232, 742, 246]}, {"image_id": 2, "file_name": "3207_02.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 304], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Semantic Abstraction Levels for Concepts. By default, the entry point for the visualization (0) shows all major concepts. Users can opt to start at a lower abstraction level (-2), revealing more concepts, or choose a higher abstraction level (+2), resulting in fewer initially visible concepts. ", "caption_bbox": [28, 316, 742, 344]}, {"image_id": 3, "file_name": "3207_03.png", "page": 6, "dpi": 300, "bbox": [27, 310, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Duality of Concept and Topic Views. Selected layers from each view \u2018shine through\u2019 the other view to give context. In this example, the left side of the (a) concept view represents a region on renewable energy (bottom) and terrorism (top), while the corresponding (b) topic view places the topic on oil production to renewable energy and the topic on a terror attack in Libya between the two concepts as it is related to both. ", "caption_bbox": [28, 258, 743, 300]}, {"image_id": 4, "file_name": "3207_04.png", "page": 7, "dpi": 300, "bbox": [45, 59, 751, 195], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Two options for Concept Re\ufb01nement. The direct manipulation enables exploratory re\ufb01nement, while the guided relevance feedback is designed for targeted re\ufb01nement. Both options can be used anytime throughout the visual analytics process to adjust the concept hierarchy. ", "caption_bbox": [40, 206, 755, 234]}], "3208": [{"image_id": 0, "file_name": "3208_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 465], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: A screenshot of our STBins prototype that shows Flickr user activities in 7 hours during the anti-austerity movement in Spain named 15M: (a) the main view that presents Flickr user behavior, (b) the control panel that allows control of time \ufb02ow and visual design, and (c) the data view that presents details of selection in the main view. ", "caption_bbox": [60, 475, 709, 516]}, {"image_id": 1, "file_name": "3208_01.png", "page": 2, "dpi": 300, "bbox": [456, 52, 704, 180], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Overview of our user tasks.", "caption_bbox": [492, 190, 667, 204]}, {"image_id": 2, "file_name": "3208_02.png", "page": 3, "dpi": 300, "bbox": [392, 325, 746, 964], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: (a) Windows are highlighted with white boxes; (b) two different visual encodings of sequence similarity (highlighted by red stippled boxes). ", "caption_bbox": [392, 974, 742, 1015]}, {"image_id": 3, "file_name": "3208_03.png", "page": 3, "dpi": 300, "bbox": [28, 29, 746, 270], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: a, b, and c are three sequences of [A-H]: (a) partially displayed juxtaposed sequences; (b) temporal binning on time windows (Wi\u22121 to       where [\u03b8 , \u0398] is the range of data element counts that users have indi- Wi+1 ), segments of sequences are marked by braces; (c) segments are      cated to be of interest. In our case studies, \u03b8 is set to 0, while \u0398 can be represented as circles (the darker the more data elements inside), and    adjusted by users to \ufb01t different data and tasks. ", "caption_bbox": [27, 269, 742, 324]}, {"image_id": 4, "file_name": "3208_04.png", "page": 4, "dpi": 300, "bbox": [404, 52, 756, 162], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: By hovering over a segment of a sequence, meta information is shown on the bottom of the screen and similar segments in the same window are highlighted in red. ", "caption_bbox": [404, 172, 754, 213]}, {"image_id": 5, "file_name": "3208_05.png", "page": 4, "dpi": 300, "bbox": [39, 52, 392, 265], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Alternative visual designs based on different layouts of similarity links (arc or bipartite) and vertical sequence ordering strategies (global or local). ", "caption_bbox": [40, 276, 390, 317]}, {"image_id": 6, "file_name": "3208_06.png", "page": 4, "dpi": 300, "bbox": [404, 229, 756, 339], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7: Top values contained by selected segments (red border) are shown in a ranking in the data view. ", "caption_bbox": [404, 349, 754, 377]}, {"image_id": 7, "file_name": "3208_07.png", "page": 4, "dpi": 300, "bbox": [404, 839, 756, 950], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Segments containing the selected values are highlighted by a red border. Their greyscale \ufb01ll color shows the ratio of the number of selected values inside to the total number of data elements inside, the darker the higher. ", "caption_bbox": [404, 960, 754, 1015]}, {"image_id": 8, "file_name": "3208_08.png", "page": 5, "dpi": 300, "bbox": [28, 29, 394, 233], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Sample of Task1 (top) and Task2 (bottom). The transparent \ufb01lled circles are given. The other circles indicate the expected answers. ", "caption_bbox": [28, 243, 378, 271]}, {"image_id": 9, "file_name": "3208_09.png", "page": 5, "dpi": 300, "bbox": [391, 576, 746, 1065], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Analyzing result of the F1 score and time spent for Task2.", "caption_bbox": [401, 530, 734, 546]}, {"image_id": 10, "file_name": "3208_10.png", "page": 5, "dpi": 300, "bbox": [391, 29, 745, 266], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10: Analyzing result of the F1 score and time spent for Task1.", "caption_bbox": [401, 276, 734, 292]}, {"image_id": 11, "file_name": "3208_11.png", "page": 6, "dpi": 300, "bbox": [405, 53, 755, 286], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12: Execution times of the algorithm [6].   and   show individual execution times for instances without and with logging active. The red and blue lines denote the corresponding means. ", "caption_bbox": [404, 296, 754, 340]}, {"image_id": 12, "file_name": "3208_12.png", "page": 6, "dpi": 300, "bbox": [404, 850, 756, 978], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 13: One thread (red box) and similarity between four threads within a temporal window (blue box). ", "caption_bbox": [404, 988, 754, 1016]}, {"image_id": 13, "file_name": "3208_13.png", "page": 7, "dpi": 300, "bbox": [27, 29, 393, 837], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14: Four behaviors of the algorithm.", "caption_bbox": [100, 848, 306, 862]}, {"image_id": 14, "file_name": "3208_14.png", "page": 7, "dpi": 300, "bbox": [391, 29, 746, 767], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15: Two signals of an emerging event: (blue box) over 10 users appear in 1 hour and more than half of them are similar; (red box) at least one user performs similarly for 5 hours. ", "caption_bbox": [392, 778, 742, 819]}, {"image_id": 15, "file_name": "3208_15.png", "page": 8, "dpi": 300, "bbox": [405, 440, 755, 602], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 17: Distribution of events in the ground truth.", "caption_bbox": [453, 612, 705, 626]}, {"image_id": 16, "file_name": "3208_16.png", "page": 9, "dpi": 300, "bbox": [27, 29, 394, 514], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 18: Example of events detected by our visualization.", "caption_bbox": [59, 526, 346, 540]}], "3209": [{"image_id": 0, "file_name": "3209_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 395], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Close-up view of an explainer, the main building-block used to construct an iterative XAI pipeline for the understanding, diagnosis, and re\ufb01nement of ML models. Explainers have \ufb01ve properties; they take one or more model states as input, applying an XAI method, to output an explanation or a transition function. Global monitoring and steering mechanisms expand the pipeline to the full XAI framework, supporting the overall work\ufb02ow by guiding, steering, or tracking the explainers during all steps. ", "caption_bbox": [61, 405, 710, 460]}, {"image_id": 1, "file_name": "3209_01.png", "page": 4, "dpi": 300, "bbox": [40, 29, 758, 228], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Unrolled view of an iterative model explanation and re\ufb01nement work\ufb02ow. Different explainers are applied to a model state 1, to generate insights and identify \ufb02aws. From the detected issues, re\ufb01nements are proposed, which induce a transition to a new model state 2, etc. Global monitoring and steering mechanisms are used to document the provenance of the XAI process and keep track of different quality metrics. ", "caption_bbox": [40, 239, 755, 281]}, {"image_id": 2, "file_name": "3209_02.png", "page": 6, "dpi": 300, "bbox": [39, 406, 756, 940], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: System design overview. The TensorFlow model is created        \ufb02ow. For a model developer, this step offers information necessary to in Python. During training, log\ufb01les are written using the explAIner    create a \ufb01tting model, e.g., layer sizes, loss function, used optimizer, summary method, which saves graph de\ufb01nition, tensor contents, and      etc. For a model user and a model novice, it explains a given model the model itself. The explAIner TensorBoard plugin queries data either and its functionality by providing visual representations, descriptions from the native backend implementation or from an external server,                                                                           2 System is publicly available under: http://explainer.ai/ depending on whether the explainer uses tensor data or the model. ", "caption_bbox": [40, 939, 755, 1022]}, {"image_id": 3, "file_name": "3209_03.png", "page": 6, "dpi": 300, "bbox": [391, 29, 758, 312], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: The diagnosis dashboard. Explainers are arranged in a toolbox-like interface, ordered descending, from high-abstraction to low-abstraction. The graph visualization provides an overview of the full model and allows for node selection. Explanations are shown in the upper toolcard, while information about the explainer is displayed beneath. The provenance bar contains cards from interesting \ufb01ndings. ", "caption_bbox": [404, 322, 754, 405]}, {"image_id": 4, "file_name": "3209_04.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 262], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Information cards showing results for different explainers (top) with the corresponding descriptions for the explainer itself (bottom). ", "caption_bbox": [392, 267, 742, 295]}, {"image_id": 5, "file_name": "3209_05.png", "page": 8, "dpi": 300, "bbox": [39, 29, 757, 136], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Use-case showing the provenance bar sequence used for model re\ufb01nement towards the correct prediction of a seven. A misclassi\ufb01ed image is analyzed using (1) Saliency and (2) LRP. As a re\ufb01nement, explAIner suggests using (3) convolutional layers. After applying the re\ufb01nement, (4) LRP and (5) Saliency show a correct focus on the relevant features. ", "caption_bbox": [40, 146, 755, 187]}], "3210": [{"image_id": 0, "file_name": "3210_00.png", "page": 1, "dpi": 300, "bbox": [40, 29, 757, 533], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Reliability attack on spam \ufb01lters. (1) Poisoning instance #40 has the largest impact on the recall value, which is (2) also depicted in the model overview. (3) There is heavy overlap among instances in the two classes as well the poisoning instances. (4) Instance #40 has been successfully attacked causing a number of innocent instances to have their labels \ufb02ipped. (5) The \ufb02ipped instances are very close to the decision boundary. (6) On the feature of words \u201cwill\u201d and \u201cemail\u201d, the variances of poisoning instances are large. (7) A sub-optimal target (instance #80) has less impact on the recall value, but the cost of insertions is 40% lower than that of instance #40. ", "caption_bbox": [73, 549, 724, 616]}, {"image_id": 1, "file_name": "3210_01.png", "page": 3, "dpi": 300, "bbox": [40, 29, 758, 354], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Key features of an adversary. (Left) The general components servers [7]. Furthermore, in the paradigm of proactive defense, it is an adversary must consider when planning an attack. (Right) Speci\ufb01c meaningful to use the worst case attack to explore the upper bounds considerations in a data poisoning attack.                          of model vulnerability [10]. In terms of poisoning operations on the ", "caption_bbox": [40, 353, 755, 395]}, {"image_id": 2, "file_name": "3210_02.png", "page": 4, "dpi": 300, "bbox": [391, 29, 745, 414], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. A visual analytics framework for explaining model vulnerabilities to adversarial machine learning attacks. The framework consists of: vulnerability analysis, attack space analysis, and attack result analysis. ", "caption_bbox": [392, 429, 744, 469]}, {"image_id": 3, "file_name": "3210_03.png", "page": 5, "dpi": 300, "bbox": [40, 29, 758, 243], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. An illustration of data poisoning attacks using the Binary-Search and Stingray algorithms. (a) In a binary classi\ufb01cation problem, the goal of a data-poisoning attack is to prevent the target instance, xt , from being classi\ufb01ed as its original class. (b) The Binary-Search and StingRay attacks consist of three main steps: 1) select the nearest neighbor to the target instance, 2) \ufb01nd a proper poisoning candidate, and 3) retrain the model with the poisoned training data. The procedure repeats until the predicted class label of xt is \ufb02ipped, or the budget is reached. ", "caption_bbox": [40, 259, 755, 314]}, {"image_id": 4, "file_name": "3210_04.png", "page": 5, "dpi": 300, "bbox": [41, 322, 757, 453], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                              the two attack algorithms, and the interface for the corresponding attack Fig. 5. Estimating the decision boundary distance. (Left) Six directional     result will be opened in a new tab page below. vectors are sampled from the unit ball. (Right) For each direction, the original instance is perturbed one step at a time until it is in the opposite 4.3    Visualizing the Attack Results ", "caption_bbox": [40, 452, 755, 507]}, {"image_id": 5, "file_name": "3210_05.png", "page": 6, "dpi": 300, "bbox": [392, 254, 744, 363], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Visual design of the local impact view. (a) The process of building kNN graph structures. (b) The visual encodings for nodes and edges. ", "caption_bbox": [392, 378, 742, 406]}, {"image_id": 6, "file_name": "3210_06.png", "page": 6, "dpi": 300, "bbox": [28, 29, 394, 232], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Design of the virtual decision boundaries in the instance attribute view. The central vertical line acts as the virtual decision boundary. Two circles representing the prediction results of the victim and the poisoned models are placed beside the line. In this example, the data instance far away from the decision boundary was classi\ufb01ed as positive with a relatively high probability. However, in the poisoned model, the instance crosses the boundary, causing the label to \ufb02ip. ", "caption_bbox": [27, 247, 377, 340]}, {"image_id": 7, "file_name": "3210_07.png", "page": 7, "dpi": 300, "bbox": [40, 29, 756, 287], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. A targeted attack on hand-written digits. (1) In the data table view, we identify the target instance #152 (2) as a potential vulnerability. (3) In the model overview, we observe no signi\ufb01cant change of the prediction performance after an attack on #152 occurs. (4) In the projection view, the two classes of instances are clearly separated into two clusters. The poisoning instances (dark blue circles) penetrate class Number 8 and reach the neighboring region of instance #152. (5) The attack can also be explored in the local impact view where poisoning nodes and the target show strong neighboring connections. (6) The detailed prediction results for instance #152 are further inspected in the instance attribute view. ", "caption_bbox": [40, 301, 755, 368]}], "3211": [{"image_id": 0, "file_name": "3211_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 396], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. We propose a design framework to protect individuals and groups from discrimination in algorithm-assisted decision making. A visual analytic system, FairSight, is implemented based on our proposed framework, to help data scientists and practitioners make fair decisions. The decision is made through ranking individuals who are either members of a protected group (orange bars) or a non-protected group (green bars). (a) The system provides a pipeline to help users understand the possible bias in a machine learning task as a mapping from the input space to the output space. (b) Different notions of fairness \u2013 individual fairness and group fairness \u2013 are measured and summarized numerically and visually. For example, the individual fairness is quanti\ufb01ed by how pairwise distances between individuals are preserved through the mapping. The group fairness is quanti\ufb01ed by the extent to which it leads to fair outcome distribution across groups, with (i) a 2D plot, (ii) a color-coded matrix , and (iii) a ranked-list plot capturing the pattern of potential biases. The system provides diagnostic modules to help (iv) identify and (v) mitigate biases through (c) investigating features before running a model, and (d) leveraging fairness-aware algorithms during and after the training step. ", "caption_bbox": [60, 398, 711, 528]}, {"image_id": 1, "file_name": "3211_01.png", "page": 6, "dpi": 300, "bbox": [39, 29, 756, 433], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. The work\ufb02ow of fair decision making in FairSight. (a) It starts with setting up inputs including the sensitive attribute and protected group. (b) After running a model, the ranking outcome and measures are represented in Ranking View. (c) Global Inspection View visualizes the two spaces and the mapping process of Individual and Group fairness provided in the separate tap. (d) When an individual is hovered, Local Inspection View provides the instance- and group-level exploration. (e) In Feature Inspection View, users can investigate the feature distortion and feature perturbation to identify features as the possible source of bias. (f) All generated ranking outcomes are listed and plotted in the Ranking List View. ", "caption_bbox": [39, 436, 756, 503]}, {"image_id": 2, "file_name": "3211_02.png", "page": 6, "dpi": 300, "bbox": [404, 520, 758, 677], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. The visualization of ranking outcome supports the exploration of cumulative ranking quality in terms of fairness and utility. The individuals are encoded as vertical bars with the group indicator at the bottom, with a blue rectangle (fairness) and a red circle (utility). It facilitates the selection of different k values, by helping users recognize the score change as k increases, as the trade-off between the fairness and utility. ", "caption_bbox": [404, 682, 754, 760]}, {"image_id": 3, "file_name": "3211_03.png", "page": 8, "dpi": 300, "bbox": [39, 29, 758, 345], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. Summary of case study in loan approval (g) with the initial features and method. Users can understand (a) the distribution of groups and individuals in Data phase. With the feature auditing modules to detect (b) feature correlation and (c) feature distortion, the visual components of (d) Matrix View in Mapping and (d) Ranking View in Outcome phase represent that the fairness in each phase improves. (f) Within-ranking module helps adjusting better top-k threshold. (h) Ranking List View displays the trade-off between fairness and utility to help users comparing the rankings. found that two groups were not clearly separated by their cluster any-       machine learning to ensure they are familiar with the typical machine more (Fig. 6a-i3). She also found that Group Skew score in Model             learning work\ufb02ow and terminologies. We conducted a within-subject phase improved by 0.05. But GFDCG score in Outcome phase was                 study where each participant was asked to use both tools in a random still severely biased towards men, which still left much room to improve     order. We gave participants the tutorial (25 mins) and let them explore (score = 0.53). At this time, she observed in Feature Inspection View        the two systems (15 mins). ", "caption_bbox": [39, 353, 755, 479]}, {"image_id": 4, "file_name": "3211_04.png", "page": 9, "dpi": 300, "bbox": [430, 102, 698, 310], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Visualization of three stages from (a) 9 features (Iteration 1 in Case study) with the in-processing method (b) 6 features (Iteration 5 in Case study) with Logistic regression. ", "caption_bbox": [392, 311, 742, 351]}], "3212": [{"image_id": 0, "file_name": "3212_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 470], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. With Summit, users can scalably summarize and interactively interpret deep neural networks by visualizing what features a network detects and how they are related. In this example, I NCEPTION V1 accurately classi\ufb01es images of tench (yellow-brown \ufb01sh). However, S UMMIT reveals surprising associations in the network (e.g., using parts of people) that contribute to its \ufb01nal outcome: the \u201ctench\u201d prediction is dependent on an intermediate \u201chands holding \ufb01sh\u201d feature (right callout), which is in\ufb02uenced by lower-level features like \u201cscales,\u201d \u201cperson,\u201d and \u201c\ufb01sh\u201d. (A) Embedding View summarizes all classes\u2019 aggregated activations using dimensionality reduction. (B) Class Sidebar enables users to search, sort, and compare all classes within a model. (C) Attribution Graph View visualizes highly activated neurons as vertices (\u201cscales,\u201d \u201c\ufb01sh\u201d) and their most in\ufb02uential connections as edges (dashed purple edges). ", "caption_bbox": [59, 482, 711, 576]}, {"image_id": 1, "file_name": "3212_01.png", "page": 2, "dpi": 300, "bbox": [433, 53, 726, 174], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. A high-level illustration of how we take thousands of images for a given class, e.g., images from white wolf class, compute their top activations and attributions, and combine them to form an attribution graph that shows how lower-level features (\u201clegs\u201d) contribute to higher- level ones (\u201cwhite fur\u201d), and ultimately the \ufb01nal outcome. ", "caption_bbox": [404, 177, 756, 244]}, {"image_id": 2, "file_name": "3212_02.png", "page": 3, "dpi": 300, "bbox": [54, 741, 746, 926], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                               data could then be viewed as a feature of vector for each class Fig. 3. A common, widely shared example illustrating how neural net-                                                                                where the features are the counts of images that had a speci\ufb01c works learn hierarchical feature representations. Our work crystallizes                                                                                channel as a top channel (Sect. 6.1). these illustrations by systematically building a graph representation that describe what features a model has learned and how they are related.       G2. Aggregating in\ufb02uences by counting previous top in\ufb02uential We visualize features learned at individual neurons and connect them           channels. We aim to identify the most in\ufb02uential paths data takes to understand how high-level feature representations are formed from           throughout a network. If aggregated for every image, we could use lower-level features. Ex. taken from Yann LeCun, 2015.                         intermediate outputs of the fundamental convolutional operation ", "caption_bbox": [27, 925, 742, 1018]}, {"image_id": 3, "file_name": "3212_03.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 354], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. Our approach for aggregating activations and in\ufb02uences for a layer l. Aggregating Activations: (A1) given activations at layer l, (A2) compute the max of each 2D channel, and (A3) record the top activated channels into an (A4) aggregated activation matrix, which tells us which channels in a layer most activate and represent every class in the model. Aggregating In\ufb02uences: (I1) given activations at layer l \u2212 1, (I2) convolve them with a convolutional kernel from layer l, (I3) compute the max of each resulting 2D activation map, and (I4) record the top most in\ufb02uential channels from layer l \u2212 1 that impact channels in layer l into an (I5) aggregated in\ufb02uence matrix, which tells us which channels in the previous layer most in\ufb02uence a particular channel in the next layer. ", "caption_bbox": [28, 358, 743, 438]}, {"image_id": 4, "file_name": "3212_04.png", "page": 6, "dpi": 300, "bbox": [573, 412, 757, 446], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. Selectable network minimap animates the Embedding View. ", "caption_bbox": [574, 449, 754, 476]}, {"image_id": 5, "file_name": "3212_05.png", "page": 8, "dpi": 300, "bbox": [415, 54, 752, 293], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Using S UMMIT we can \ufb01nd classes with mixed semantics that shift their primary associations throughout the network layers. For example, early in the network, horsecart is most similar to mechanical classes (e.g., harvester, thresher, snowplow), towards the middle it shifts to be nearer to animal classes (e.g., bison, wild boar, ox), but ultimately returns to have a stronger mechanical association at the network output. ", "caption_bbox": [404, 299, 756, 379]}, {"image_id": 6, "file_name": "3212_06.png", "page": 8, "dpi": 300, "bbox": [40, 55, 384, 239], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. An example substructure from the lion\ufb01sh attribution graph that shows unexpected texture features, like \u201cquills\u201d and \u201cstripes,\u201d in\ufb02uencing top activated channels for a \ufb01nal layer\u2019s \u201corange \ufb01sh\u201d feature (some lion \ufb01sh are reddish-orange, and have white \ufb01n rays). ", "caption_bbox": [40, 240, 390, 296]}, {"image_id": 7, "file_name": "3212_07.png", "page": 9, "dpi": 300, "bbox": [28, 29, 745, 365], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9. With attribution graphs, we can compare classes throughout layers of a network. Here we compare two similar classes: black bear and brown bear. From the intersection of their attribution graphs, we see both classes share features related to bear-ness, but diverge towards the end of the network using fur color and face color as discriminable features. This feature discrimination aligns with how humans might classify bears. ", "caption_bbox": [28, 370, 743, 411]}, {"image_id": 8, "file_name": "3212_08.png", "page": 9, "dpi": 300, "bbox": [41, 419, 744, 535], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 10. Using S UMMIT on I NCEPTION V1 we found non-semantic chan-              Hyperparameter selections. Our approach has a few hyperparam- nels that detect irrelevant features, regardless of the input image, e.g., in eters choices, including determining how many channels to record per layer mixed3a, channel 67 is activated by the frame of an image.              image when aggregating activations and computing attribution graph ", "caption_bbox": [28, 535, 743, 575]}], "3213": [{"image_id": 0, "file_name": "3213_00.png", "page": 1, "dpi": 300, "bbox": [40, 29, 758, 514], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. CloudDet facilitates the exploration of anomalous cloud computing performances through three levels of analysis: (a) anomaly ranking, (b) anomaly inspection, and (c) anomaly clustering. The \ufb01gure showcases some exploration results with Bitbrains Datacenter traces data. Node (b1) contains both short and long term spikes. Node (b2) shows a 12-hour periodic pattern for the performance metrics by observing the calendar chart in (a2), but encounters a spike in the process. Node (b3) shows many short-term and near-periodic spikes at the beginning and an abnormal long-term spike near the end. After collapsing the long-term one into a visual aggregation glyph in (b5), (b3) is updated and the latter temporal data \u201cpop out\u201d, which shows a similar pattern as the beginning. Node (b4) shows a general periodic trend by using the PCA analysis in (b6). Most of the nodes are clustered into three groups in (c). ", "caption_bbox": [73, 519, 722, 609]}, {"image_id": 1, "file_name": "3213_01.png", "page": 3, "dpi": 300, "bbox": [391, 29, 758, 191], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. CloudDet system overview and data processing pipeline.", "caption_bbox": [405, 201, 719, 215]}, {"image_id": 2, "file_name": "3213_02.png", "page": 4, "dpi": 300, "bbox": [28, 29, 745, 183], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Algorithm pipeline: (1) pattern matching for time series, (2) anomaly scoring, and (3) anomaly score summary.", "caption_bbox": [28, 188, 602, 202]}, {"image_id": 3, "file_name": "3213_03.png", "page": 5, "dpi": 300, "bbox": [39, 29, 757, 403], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4. The CloudDet system contains \ufb01ve interactive modules: (1) Spatial Overview, (2) Temporal Overview, (3) Rank View, (4) Performance View in the horizon chart mode, and (5) Cluster View. The performance view contains two other views/modes: (6) the multi-line mode and (7) the PCA mode. Users can select and \ufb01lter data in (a), switch to different visualization modes in different views by buttons (b) and (d), and change the layout by the slider bar in (c). (f) is the explanation of the glyph in (5). (e) shows the color schemes used in different views. ", "caption_bbox": [40, 414, 755, 466]}, {"image_id": 4, "file_name": "3213_04.png", "page": 5, "dpi": 300, "bbox": [405, 475, 756, 616], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5. (1)\u2013(3) show the standard horizon chart design [25]. The horizon chart can be extended by: (a) selecting a speci\ufb01c period by brushing, and then (b) visually collapsing this period into a glyph for data abstraction and time scaling, or (c) stretching this period for detailed inspection. ", "caption_bbox": [405, 622, 755, 674]}, {"image_id": 5, "file_name": "3213_05.png", "page": 9, "dpi": 300, "bbox": [39, 29, 394, 267], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8. Identi\ufb01ed anomalous patterns with live cloud system data.", "caption_bbox": [40, 269, 360, 283]}], "3214": [{"image_id": 0, "file_name": "3214_00.png", "page": 1, "dpi": 300, "bbox": [73, 65, 724, 507], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1. Exploranative code quality document for Lucene 2.0.    A Textual overview in terms of quality attributes, code smells, and bugs, which includes embedded visualizations.   B Overview visualizations: parallel coordinates plot and scatterplot.       C Source code of a class provided in the details view.                                       D Description of a quality attribute alternatively presented in the details view. ", "caption_bbox": [73, 520, 723, 560]}, {"image_id": 1, "file_name": "3214_01.png", "page": 4, "dpi": 300, "bbox": [391, 71, 744, 499], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2. Abstract representation of the interface structure.", "caption_bbox": [392, 513, 668, 527]}, {"image_id": 2, "file_name": "3214_02.png", "page": 5, "dpi": 300, "bbox": [40, 52, 757, 457], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3. Various instances of vis\u2013text interactions. A persistent highlighting (click on TraverseSchema) marks the related elements with bold font in text (text\u2013text), a black line in the parallel coordinates, a black dot in the scatterplot (text\u2013vis), and black background in embedded detail visualizations (text\u2013emvis). Similarly, a non-persistent highlighting (hover on UTF8Reader ) marks the corresponding elements in yellow. The details panel shows the class-speci\ufb01c description and source code of the persistently selected class, TraverseSchema. ", "caption_bbox": [40, 471, 755, 526]}, {"image_id": 3, "file_name": "3214_03.png", "page": 5, "dpi": 300, "bbox": [40, 549, 759, 758], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "                                                                         start to stop node results in a report section. Decision nodes control the Fig. 4. Methodological explanation of classifying complexity in terms of \ufb02ow of the graph based on the values of decision variables. Text nodes low, regular, or good. ", "caption_bbox": [40, 757, 755, 792]}, {"image_id": 4, "file_name": "3214_04.png", "page": 7, "dpi": 300, "bbox": [42, 52, 755, 265], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6. A selection of the classes in Xerces 1.2 that have a Functional Decomposition smell; they are highlighted in the parallel coordinates plot and the scatterplot. The caption of the visualizations adapts to describe the selection. ", "caption_bbox": [40, 273, 754, 300]}, {"image_id": 5, "file_name": "3214_05.png", "page": 8, "dpi": 300, "bbox": [28, 29, 745, 292], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 7. Low quality in HTMLElementImpl found through active exploration of the embedded detail visualizations of coupling, cohesion, and inheritance.", "caption_bbox": [28, 300, 745, 315]}], "3215": [{"image_id": 0, "file_name": "3215_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 490], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Evolution of the t-SNE embedding for the MNIST dataset. The optimization is performed in only a few seconds while running in a web browser and providing progressive updates. Previous implementations require tens of minutes to run in multithreaded C++ programs. CUDA implementations exists, but require NVIDIA GPUs and do not run in the browser. The example can be run at the following link https://nicola17.github.io/tfjs-tsne-demo/ ", "caption_bbox": [61, 500, 711, 555]}, {"image_id": 1, "file_name": "3215_01.png", "page": 4, "dpi": 300, "bbox": [40, 29, 758, 237], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Fields used in our approach. (a) The MNIST dataset contains images of handwritten digits and is embedded in a 2-dimensional space. The minimization of the objective function is computed in linear time by making use of a scalar \ufb01eld S (b) and a 2-dimensional vector \ufb01eld V , where (c-d) show the horizontal and vertical components respectively. The \ufb01elds are computed on the GPU by drawing properly designed mathematical kernels using the additive blending function of the rendering pipeline. The rest of the minimization is treated as a series of tensor computations that are computed on the GPU. ", "caption_bbox": [39, 247, 756, 316]}, {"image_id": 2, "file_name": "3215_02.png", "page": 4, "dpi": 300, "bbox": [404, 345, 757, 480], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Functions drawn over each embedding point to approximate the scalar \ufb01eld S and the 2-dimensional vector \ufb01eld V . ", "caption_bbox": [404, 490, 754, 519]}, {"image_id": 3, "file_name": "3215_03.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 304], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: Computational work\ufb02ow of our approach. On the lower side of the chart, the computation of the repulsive forces is presented. The \ufb01elds texture is generated by the additive texture splatting presented in Section 5.1.2. The values of S and V are obtained through texture interpolation and are used to compute the repulsive forces. The attractive forces are computed in a custom shader that takes as input the similarities P and the embedding. The gradient of the objective function is then computed using both forces and is used to update the embedding. ", "caption_bbox": [28, 304, 743, 359]}, {"image_id": 4, "file_name": "3215_04.png", "page": 6, "dpi": 300, "bbox": [40, 29, 758, 607], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Embeddings of the MNIST, ImageNet Mixed3a, ImageNet Head0, WikiWord and Word2Vec datasets generated by our technique.", "caption_bbox": [52, 625, 741, 639]}, {"image_id": 5, "file_name": "3215_05.png", "page": 8, "dpi": 300, "bbox": [40, 29, 770, 719], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Results of the experiments on the MNIST, WikiWord and Word2Vec datasets for the t-SNE, Barnes-Hut-SNE, t-SNE-CUDA and our approach. The \ufb01rst row shows the evolution of the execution time with increasingly bigger subsets of the dataset. The second row shows how well the objective function is ful\ufb01lled, while the third row shows the Nearest-Neighborhood Preservation (NPP). Our technique is up to two orders of magnitude faster than Barnes-Hut-SNE and provides higher quality embeddings compared to Barnes-Hut-based techniques. ", "caption_bbox": [39, 734, 754, 789]}], "3216": [{"image_id": 0, "file_name": "3216_00.png", "page": 1, "dpi": 300, "bbox": [28, 29, 745, 471], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 1: Interface of Galex consists of three panels: area picker panel (left), main panel (middle), and network panel (right). The main panel includes three layers: discipline, area, and institution layers. To date, all papers (over 86,000) from all 26 areas are shown in the discipline layer, forming an overview (galaxy) of computer science. The area picker controls the areas that are displayed. A contour line indicates the paper distribution of one area (here, visualization). The spotlight reveals the semantics of the regions of interest. The time brush acts as a time \ufb01lter. The toolbox provides useful components, such as pallet, spotlight, and time slice selector. The network panel includes three kinds of networks to uncover the structure of one area. ", "caption_bbox": [61, 473, 710, 556]}, {"image_id": 1, "file_name": "3216_01.png", "page": 3, "dpi": 300, "bbox": [395, 274, 741, 397], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 3: Schematic of terminologies of science hierarchy. Categories and areas are determined by CSRankings. The AI category includes an area also called AI. ", "caption_bbox": [392, 400, 742, 441]}, {"image_id": 2, "file_name": "3216_02.png", "page": 3, "dpi": 300, "bbox": [391, 29, 745, 247], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 2: Pipline of Galex.", "caption_bbox": [506, 249, 629, 263]}, {"image_id": 3, "file_name": "3216_03.png", "page": 4, "dpi": 300, "bbox": [408, 879, 752, 955], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 5: Time slice creator allows analysts to create small snapshots in two steps: selecting a time interval by brushing and then clicking one bottom button to segment the interval into equal-length (here, \ufb01ve years) time slices. ", "caption_bbox": [404, 957, 754, 1012]}, {"image_id": 4, "file_name": "3216_04.png", "page": 4, "dpi": 300, "bbox": [40, 29, 758, 266], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 4: The main overlap zone between the AI (contour line) and CV area (points) located at the upper right side of the latter. Spotlight indicates the top scholars, institutions (arcs), and phrases inside the covered region  .                                                                              a Contour line of CV indicates that most papers are distributed across three groups and Group 2 owns most of the papers, forming two cores  .     b Paper will be re-colored by its predominant topic after clicking    .                                                                                                                                                  c ", "caption_bbox": [40, 269, 755, 312]}, {"image_id": 5, "file_name": "3216_05.png", "page": 5, "dpi": 300, "bbox": [28, 29, 745, 293], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 6: Small snapshots and entity lists of the AI area. In recent years, AI techniques have rapidly applied to other areas of the AI category which signi\ufb01cantly expanded the original knowledge base of the AI area. For example, in 2015-2017, deep-learning-related technologies, such as convolutional neural network, recurrent neural network, and       Fig. 7: Top generative adversarial network, were widely used in the following \ufb01elds:  Using the cu representation learning (word embedding and network embedding),           VA&InfoVis social network (link prediction and dynamic network), and CV (action recognition). Currently, phrases are sorted by G2 statistics. ", "caption_bbox": [27, 292, 457, 416]}, {"image_id": 6, "file_name": "3216_06.png", "page": 5, "dpi": 300, "bbox": [395, 417, 741, 537], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 8: Layout results of the LDA + t-SNE work\ufb02ow            a and the Doc2Vec + t-SNE work\ufb02ow        b on the computer networks area. The color of paper is determined by its predominant topic. The clusters (top- ics) in           a are visually signi\ufb01cant and semantically explicit, as veri\ufb01ed by an expert in computer networks. ", "caption_bbox": [392, 539, 744, 608]}, {"image_id": 7, "file_name": "3216_07.png", "page": 5, "dpi": 300, "bbox": [392, 623, 746, 936], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 9: Enhanced spotlights corresponding to the two dark grey circles in Fig. 7. UC Davis owns the most papers on the topic of \ufb02ow visu- alization. Three scholars at UC Davis published thirteen papers and collaborated with two other institutions on this topic  .                                                         a Benjamin Lok published the most papers on VR&AR topic. He has three main cooperators on this topic  .                           b ", "caption_bbox": [392, 938, 744, 1021]}, {"image_id": 8, "file_name": "3216_08.png", "page": 6, "dpi": 300, "bbox": [392, 29, 758, 178], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 11: Cooperation network of top 100 institutions of AI area       a (all papers are from AAAI and IJCAI conference) and co-occurrence network of top 100 phrases of VIS area      b (all papers are from VIS and VR conference) (colored bubbles are hand-painted). After \ufb01ltering out the link with less than \ufb01ve times of cooperation, we detect three academic groups in the AI area, namely, America group, China group, and UK&Australia group. National University of Singapore (NUS), Nanyang Technological University (NTU), and University of Southern California (USC) act as pivots. After deleting the phrase (\u201cvisualiza- tion\u201d, \u201cdata visualization\u201d, and \u201ccomputer graphics\u201d) that links with almost all the other phrases, we identify three main topics in VIS area: SciVis, InfoVis&VA, and VR&AR. ", "caption_bbox": [404, 180, 756, 346]}, {"image_id": 9, "file_name": "3216_09.png", "page": 7, "dpi": 300, "bbox": [28, 29, 394, 120], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 12: Evolution of computer vision from 1990 to 2018. It shows a noticeable right shift of the hotspot from 3D vision and computational- geometry-related topics to 2D-image-processing-related topics which involve machine learning and deep learning techniques. ", "caption_bbox": [28, 122, 380, 177]}, {"image_id": 10, "file_name": "3216_10.png", "page": 7, "dpi": 300, "bbox": [391, 29, 745, 198], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 14: Human-computer interaction and computer graphics overlap with the upper and lower parts of the visualization, respectively. ", "caption_bbox": [392, 200, 742, 228]}, {"image_id": 11, "file_name": "3216_11.png", "page": 8, "dpi": 300, "bbox": [39, 29, 758, 204], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 15: Growth of topic tree revealing the evolution of visualization. In the early age of VIS (1990-2004), SciVis dominates the research contents. For the next 15 years, VA/InfoVis and VR&AR grew rapidly. These \ufb01ndings are consistent with the VIS history described in vispubdata.org [29]. ", "caption_bbox": [40, 206, 757, 234]}, {"image_id": 12, "file_name": "3216_12.png", "page": 9, "dpi": 300, "bbox": [30, 293, 746, 421], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 18: Evolution of computer science from 2009 to 2017 with a three-  CSRankings for computer science. For other disciplines, a reasonable year interval. HCI, computer security, and the areas in the AI category classi\ufb01cation is required. ", "caption_bbox": [27, 420, 742, 450]}, {"image_id": 13, "file_name": "3216_13.png", "page": 9, "dpi": 300, "bbox": [28, 29, 745, 248], "visualization_bbox": {}, "nums_of_visualizations": {}, "caption_text": "Fig. 17: Evolution of computer science from 1980 to 2017 with a 10-year interval. The core force driving the development of computer science has changed from the traditional fundamental areas to the rising application areas. ", "caption_bbox": [28, 250, 742, 278]}]}